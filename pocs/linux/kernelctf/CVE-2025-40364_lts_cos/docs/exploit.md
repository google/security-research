# Vulnerability
## Overview
Vulnerability is a use-after-free that exists in io_uring subsystem. It occurs when io_uring prepare for async operation for IO_READV. Prepared import provided buffers (`io_buffer_list` object) is not recycle after async preparation, which make it can be freed using `IORING_UNREGISTER_PBUF_RING` and will reused at the point where `IO_READV` executed asynchronously.

## Vuln Details
### io_uring Imported Buffer
IO Uring had a feature where we can register buffers that can used on most of io uring operation. Buffer stored in a `xarray`. We can register/unregister buffer using `IORING_REGISTER_PBUF_RING` or `IORING_UNREGISTER_PBUF_RING`. When issuing io_uring request, we can put the index of the buffer we want to use at `buf_index` and flag `IOSQE_BUFFER_SELECT` of io_uring submission context.
It will handled by `io_buffer_select` function.
```c
void __user *io_buffer_select(struct io_kiocb *req, size_t *len,
			      unsigned int issue_flags)
{
	struct io_ring_ctx *ctx = req->ctx;
	struct io_buffer_list *bl;
	void __user *ret = NULL;

	io_ring_submit_lock(req->ctx, issue_flags);

	bl = io_buffer_get_list(ctx, req->buf_index);
	if (likely(bl)) {
		if (bl->is_mapped)
			ret = io_ring_buffer_select(req, len, bl, issue_flags);
		else
			ret = io_provided_buffer_select(req, len, bl);
	}
	io_ring_submit_unlock(req->ctx, issue_flags);
	return ret;
}
```
There's two type of buffers, classic buffer and ring buffer, in this exploitation we gonna use ring buffer. `io_buffer_select` will call `io_ring_buffer_select` for our chosen buffer ring.
```c
static void __user *io_ring_buffer_select(struct io_kiocb *req, size_t *len,
					  struct io_buffer_list *bl,
					  unsigned int issue_flags)
{
	...
	req->flags |= REQ_F_BUFFER_RING;
	req->buf_list = bl;
	req->buf_index = buf->bid;
	...
}
```
Our buffer ring object will stored at `req->buf_list` here (without increase any refs at all), and will be used across io_uring operation later.

### io_uring Async Preparation
IO Uring had mechanism where it tries to execute operation asynchronously (which will execute in another kernel thread), before doing that io_uring uses `io_req_prep_async` to prepare some data context or object. Each operation can define `prep_async` by its own.
```C
int io_req_prep_async(struct io_kiocb *req)
{
	const struct io_cold_def *cdef = &io_cold_defs[req->opcode];
	const struct io_issue_def *def = &io_issue_defs[req->opcode];

	/* assign early for deferred execution for non-fixed file */
	if (def->needs_file && !(req->flags & REQ_F_FIXED_FILE) && !req->file)
		req->file = io_file_get_normal(req, req->cqe.fd);
	if (!cdef->prep_async)
		return 0;
	if (WARN_ON_ONCE(req_has_async_data(req)))
		return -EFAULT;
	if (!def->manual_alloc) {
		if (io_alloc_async_data(req))
			return -EAGAIN;
	}
	return cdef->prep_async(req);
}
```

### Problem
At IO_READV operation, `prep_async` will point to `io_readv_prep_async`. It then try to prepare our buffer ring via `io_import_iovec` -> `io_buffer_select`, at that point our `io_buffer_list` still stored at `req->buf_list` without any protection at all. After that we will unregister buffer ring using IORING_UNREGISTER_PBUF_RING, it will free the `io_buffer_list` at `io_put_bl`, because previously `req->buf_list` didn't hold any ref, at this code below it can smoothly free the `io_buffer_list`.
```c
void io_put_bl(struct io_ring_ctx *ctx, struct io_buffer_list *bl)
{
	if (atomic_dec_and_test(&bl->refs)) {
		__io_remove_buffers(ctx, bl, -1U);
		kfree_rcu(bl, rcu);
	}
}
```
After io_uring operation finish it will call `io_put_kbuf`, then it will call `__io_put_kbuf_list` for our buffer ring.
```c
static inline unsigned int __io_put_kbuf_list(struct io_kiocb *req,
					      struct list_head *list)
{
	unsigned int ret = IORING_CQE_F_BUFFER | (req->buf_index << IORING_CQE_BUFFER_SHIFT);

	if (req->flags & REQ_F_BUFFER_RING) {
		if (req->buf_list) {
			req->buf_index = req->buf_list->bgid;
			req->buf_list->head++; //[1]
		}
		req->flags &= ~REQ_F_BUFFER_RING;
	} else {
		req->buf_index = req->kbuf->bgid;
		list_add(&req->kbuf->list, list);
		req->flags &= ~REQ_F_BUFFER_SELECTED;
	}

	return ret;
}
```
At this point, `req->buf_list` still hold freed `io_buffer_list` then it will use-after-free at [1] when trying to increase the head. So we have use-after-free increase primitive.

## Exploit detail in LTS

The primitive we obtained from this vulnerability allows us to increase the value at offset 22 within a kmalloc-64 slab. However, due to the limited use-after-free (UAF) nature of the bug, we cannot directly overwrite kernel pointers.

For example, a valid kernel heap address like 0xffff888100063bf0 becomes 0x888100063bf0 after the increase operation—this is an invalid kernel address. Similarly, we cannot overwrite a Page Table Entry (PTE). A valid PTE like 0x8000000100c63163 would turn into 0x8001000100c63163 after an increase, pointing to an invalid physical address and thus not being exploitable.

### Discovery of a New Exploitation Path via BPF
While exploring potential overwrite targets, we took a closer look at the BPF (Berkeley Packet Filter) subsystem, suspecting it might offer useful primitives. Importantly, BPF does not require unprivileged namespaces, making it more viable in constrained environments.

We identified a particularly interesting function: `bpf_map_is_rdonly`. This function is used during verification to determine if a BPF map can be treated as read-only. If so, the verifier assumes the contents of the map will not change, allowing it to relax certain checks and optimize based on that assumption.

Here is the relevant kernel code:

```C
static bool bpf_map_is_rdonly(const struct bpf_map *map)
{
	/* A map is considered read-only if the following conditions are true:
	 *
	 * 1) The BPF program cannot change any of the map's contents.
	 *    This is enforced by the BPF_F_RDONLY_PROG flag, which must
	 *    have been set at map creation time.
	 * 2) The map values have been initialized from user space and then
	 *    "frozen", preventing further update/delete operations from user space.
	 * 3) Any parallel or pending update/delete operations must have completed.
	 *    Only then can the map be considered truly immutable.
	 */
	return (map->map_flags & BPF_F_RDONLY_PROG) &&
	       READ_ONCE(map->frozen) &&
	       !bpf_map_write_active(map);
}
```
### The Exploitation Idea

A new exploitation path emerged from this logic:
What if we can make the BPF verifier believe a map's content is A, but then—using our UAF-based increase primitive—we change it to B? This would break the verifier’s assumptions, and potentially allow us to bypass checks and construct a stable arbitrary read/write primitive inside a BPF program, much like traditional BPF-based kernel exploits [1].

### Exploit Summary
The exploitation path can be summarized as follows:

Create multiple array_map BPF maps with the BPF_F_RDONLY_PROG flag to ensure the kernel cross-cache from kmalloc-64 slab pages to array_map's pages.

Freeze the maps, setting the map->frozen field to true, making them read-only from the syscall bpf with `BPF_MAP_FREEZE`.

Construct BPF programs that uses these maps. Because bpf_map_is_rdonly() returns true, the verifier performs a read-ahead of the map content and treats it as a constant (SCALAR_VALUE):

```C
			if (tnum_is_const(reg->var_off) &&
			    bpf_map_is_rdonly(map) &&
			    map->ops->map_direct_value_addr) {
				int map_off = off + reg->var_off.value;
				u64 val = 0;

				err = bpf_map_direct_read(map, map_off, size,
							  &val);
				if (err)
					return err;

				regs[value_regno].type = SCALAR_VALUE;
				__mark_reg_known(&regs[value_regno], val);
```
Use the UAF-based increase primitive to modify the map contents after verifier checks but before actual execution, violating the SCALAR assumption. This can be exploited via BPF_FUNC_skb_load_bytes_relative, which can lead to a stack overflow, enabling further exploitation like arbitrary kernel memory read/write.

Use the achieved kernel read/write to overwrite core_pattern, escaping the container and gaining root in the init namespace.




## Exploit detail in COS

Due to the kernel version in COS instances being prior to commit c4c84f6fb2c4dc4c0f5fd927b3c3d3fd28b7030e, which introduced the following code:

```c
diff --git a/kernel/bpf/syscall.c b/kernel/bpf/syscall.c
index c7f6807215e605..c9a201e4c4572f 100644
--- a/kernel/bpf/syscall.c
+++ b/kernel/bpf/syscall.c
@@ -1931,6 +1931,11 @@ static int map_freeze(const union bpf_attr *attr)
 		return -ENOTSUPP;
 	}
 
+	if (!(map_get_sys_perms(map, f) & FMODE_CAN_WRITE)) {
+		err = -EPERM;
+		goto err_put;
+	}
+
 	mutex_lock(&map->freeze_mutex);
 	if (bpf_map_write_active(map)) {
 		err = -EBUSY;
@@ -1940,10 +1945,6 @@ static int map_freeze(const union bpf_attr *attr)
 		err = -EBUSY;
 		goto err_put;
 	}
-	if (!bpf_capable()) {
-		err = -EPERM;
-		goto err_put;
-	}
 
 	WRITE_ONCE(map->frozen, true);
 err_put:
```

This means unprivileged users cannot use BPF_MAP_FREEZE, effectively blocking the exploit path used in LTS instances.

### mnt_namespace Refcount Overflow
We target the struct mnt_namespace object, specifically the .ns.count field in its embedded struct ns_common:

```c
struct ns_common {
	atomic_long_t              stashed;   /*     0     8 */
	const struct proc_ns_operations *ops; /*     8     8 */
	unsigned int               inum;      /*    16     4 */
	refcount_t                 count;     /*    20     4 */
};

struct mnt_namespace {
	struct ns_common           ns;                   /*     0    24 */
	struct mount              *root;                 /*    24     8 */
	struct list_head           list;                 /*    32    16 */
	spinlock_t                 ns_lock;              /*    48     4 */
	/* XXX 4-byte hole */
	struct user_namespace     *user_ns;              /*    56     8 */
	struct ucounts            *ucounts;              /*    64     8 */
	u64                        seq;                  /*    72     8 */
	wait_queue_head_t          poll;                 /*    80    24 */
	u64                        event;                /*   104     8 */
	unsigned int               mounts;               /*   112     4 */
	unsigned int               pending_mounts;       /*   116     4 */
};
```

### Refcount Overflow
By continuously incrementing count by 0x10000 (1 << 16) each time, we can eventually overflow the refcount. This results in a use-after-free condition on struct mnt_namespace.

### Step-by-Step Exploit Chain
Trigger UAF by overflowing the .count refcount field.

Reclaim the freed `mnt_namespace` as a `struct msg_msgseg` with the following payload:

```c
(struct mnt_namespace){
    .ns.count = 1,
    .ucounts = init_ucounts,
}
```

Pass to free_mnt_ns and free again, reclaim again as an `io_fixed_file` array under kmalloc-128 slab:

```c
bool io_alloc_file_tables(struct io_file_table *table, unsigned nr_files) {
    table->files = kvcalloc(nr_files, sizeof(table->files[0]), GFP_KERNEL);
}
```
Layout of files array:

```css
Offset  Content
0       0x0
8       fileA
16      fileB
```
Leak pointer to fileA via `msg_msgseg`, then free it using `msgrcv`.

Reclaim the freed chunk as a new `msg_msgseg`, this time faking the layout as:

```css
Offset  Content
0       0x0
8       fileA
16      fileA
```
Call `IORING_REGISTER_FILES_UPDATE`, which triggers an extra fput() on fileA, causing a file UAF.

With the file UAF, leverage Dirty Pagetable technique to overwrite core_pattern and achieve container escape, pivoting to root in the init namespace.


