# Vulnerability

This vulnerability is about TLS receive path (tls_sw_recvmsg) mixed with its lower layer protocol which TCP receive path (tcp_recvmsg). When TLS receive data from TCP side, it will call `tls_strp_read_sock`:

```go
#0  tls_strp_read_sock (strp=0xffff888101f7fb58) at net/tls/tls_strp.c:505
#1  tls_strp_check_rcv (strp=0xffff888101f7fb58) at net/tls/tls_strp.c:543
#2  0xffffffff8257b565 in tls_data_ready (sk=0xffff888020204280) at net/tls/tls_sw.c:2448
#3  0xffffffff824f19d7 in tcp_data_queue (sk=sk@entry=0xffff888020204280, skb=skb@entry=0xffff8880203684e0) at net/ipv4/tcp_input.c:5333
#4  0xffffffff824f2506 in tcp_rcv_established (sk=sk@entry=0xffff888020204280, skb=skb@entry=0xffff8880203684e0) at net/ipv4/tcp_input.c:6287
#5  0xffffffff825014fe in tcp_v4_do_rcv (sk=sk@entry=0xffff888020204280, skb=skb@entry=0xffff8880203684e0) at net/ipv4/tcp_ipv4.c:1916
#6  0xffffffff8250479d in tcp_v4_rcv (skb=0xffff8880203684e0) at net/ipv4/tcp_ipv4.c:2351
#7  0xffffffff824ca1fb in ip_protocol_deliver_rcu (net=net@entry=0xffffffff85293440 <init_net>, skb=0xffff8880203684e0, protocol=<optimized out>) at net/ipv4/ip_input.c:205
#8  0xffffffff824ca3f9 in ip_local_deliver_finish (net=0xffffffff85293440 <init_net>, sk=<optimized out>, skb=<optimized out>) at net/ipv4/ip_input.c:233
#9  0xffffffff823297a9 in __netif_receive_skb_one_core (skb=<optimized out>, pfmemalloc=pfmemalloc@entry=0x0) at net/core/dev.c:5741
#10 0xffffffff82329a49 in __netif_receive_skb (skb=<optimized out>) at net/core/dev.c:5854
#11 process_backlog (napi=0xffff88811c436e08, quota=0x40) at net/core/dev.c:6190
#12 0xffffffff8232a6f8 in __napi_poll (n=0xffff888101f7fb58, n@entry=0xffff88811c436e08, repoll=repoll@entry=0xffffc90000003ebf) at net/core/dev.c:6841
#13 0xffffffff8232acd7 in napi_poll (repoll=0xffffc90000003ed8, n=0xffff88811c436e08) at net/core/dev.c:6910
#14 net_rx_action () at net/core/dev.c:7032
#15 0xffffffff811accd9 in handle_softirqs (ksirqd=<optimized out>) at kernel/softirq.c:579
#16 0xffffffff811ac80b in do_softirq () at kernel/softirq.c:480
```
`tls_strp_read_sock` will call `tls_strp_load_anchor_with_queue` and TLS will set up `strp->anchor` by get the first TCP socket buffer on the TCP receive queue and set it to `skb_shinfo(strp->anchor)->frag_list`:
```C
          tls_strp_load_anchor_with_queue()
            first = tcp_recv_skb(strp->sk, tp->copied_seq, &offset);
            ...
            skb_shinfo(strp->anchor)->frag_list = first;
```
The TCP socket buffer still remains on the tcp receive queue, `strp->anchor` just take the skb pointer for the decryption later at `tls_sw_recvmsg`. Problem is, if we have `tcp_recvmsg` runs when TCP packet comes, it will eat and free the skb while it still hold by `strp->anchor`. We can call `tcp_recvmsg` in seperate thread before install the TLS ULP on the TCP socket. So when `tcp_recvmsg` hang for waiting the packet comes (in seperate thread), then we install TLS to the TCP socket, then when TCP packet receive by the kernel, `tls_strp_read_sock` will get the TCP socket buffer after that `tcp_recvmsg` will eat and free the skb, so we have freed the SKB at `skb_shinfo(strp->anchor)->frag_list`.

# Exploitation
If we call `tls_sw_recvmsg` and finally it will reach `tls_decrypt_sg` there are a few places that it uses our freed skb. For example, `skb_to_sgvec` and `skb_nsg`:
```C
static int tls_decrypt_sg(struct sock *sk, struct iov_iter *out_iov,
			  struct scatterlist *out_sg,
			  struct tls_decrypt_arg *darg)
{

	struct sk_buff *skb = tls_strp_msg(ctx); // strp->anchor
	....
	n_sgin = skb_nsg(skb, rxm->offset + prot->prepend_size,
			 rxm->full_len - prot->prepend_size);
	....
	err = skb_to_sgvec(skb, &sgin[1],
			   rxm->offset + prot->prepend_size,
			   rxm->full_len - prot->prepend_size);
```
But there's no any meaningful operation to use it as exploitation because those function only perform UAF read when it read `skb_shinfo(skb)->frag_list` (our freed skb placed).

Another path that use our freed skb in `skb_shinf(skb)->frag_list` is via this path, we can reach this line when we perform async decryption:

```C
static int tls_decrypt_sg(struct sock *sk, struct iov_iter *out_iov,
			  struct scatterlist *out_sg,
			  struct tls_decrypt_arg *darg)
{
        ....
	if (unlikely(darg->async)) {
		err = tls_strp_msg_hold(&ctx->strp, &ctx->async_hold);
		if (err)
			__skb_queue_tail(&ctx->async_hold, darg->skb);
		return err;
	}
}
```
It will reach `skb_clone` path at `tls_strp_msg_hold`:
```C
int tls_strp_msg_hold(struct tls_strparser *strp, struct sk_buff_head *dst)
{
        struct skb_shared_info *shinfo = skb_shinfo(strp->anchor);
        ....
		struct sk_buff *iter, *clone;
		int chunk, len, offset;

		offset = strp->stm.offset;
		len = strp->stm.full_len;
		iter = shinfo->frag_list; // our freed skb

		while (len > 0) {
			if (iter->len <= offset) {
				offset -= iter->len;
				goto next;
			}

			chunk = iter->len - offset;
			offset = 0;

			clone = skb_clone(iter, strp->sk->sk_allocation);
			if (!clone)
				return -ENOMEM;
			__skb_queue_tail(dst, clone);

			len -= chunk;
next:
			iter = iter->next;
		}
	}
}
```

If we succesfully clone our freed skb, it will queued to the `ctx->async_hold`. After decryption finish, cloned skb that resides at `ctx->async_hold` will free at `__skb_queue_purge`.

```C
int tls_sw_recvmsg(struct sock *sk,
		   struct msghdr *msg,
		   size_t len,
		   int flags,
		   int *addr_len)
{
	...
recv_end:
	if (async) {
		int ret;

		/* Wait for all previously submitted records to be decrypted */
		ret = tls_decrypt_async_wait(ctx);
		__skb_queue_purge(&ctx->async_hold);
}

static inline void __skb_queue_purge_reason(struct sk_buff_head *list,
					    enum skb_drop_reason reason)
{
	struct sk_buff *skb;

	while ((skb = __skb_dequeue(list)) != NULL)
		kfree_skb_reason(skb, reason);
}

static inline void __skb_queue_purge(struct sk_buff_head *list)
{
	__skb_queue_purge_reason(list, SKB_DROP_REASON_QUEUE_PURGE);
}
```

## COS and Mitigation exploit (Linux 6.1.x)
In skb, there's a data structure called `skb_shared_info`. Usually it contain non linear skb data, for example such skb that hold spliced page pointer, linked skb, etc.:
```C
/* This data is invariant across clones and lives at
 * the end of the header data, ie. at skb->end.
 */
struct skb_shared_info {
	__u8		flags;
	__u8		meta_len;
	__u8		nr_frags;
	__u8		tx_flags;
	unsigned short	gso_size;
	/* Warning: this field is not always filled in (UFO)! */
	unsigned short	gso_segs;
	struct sk_buff	*frag_list;
	union {
		struct skb_shared_hwtstamps hwtstamps;
		struct xsk_tx_metadata_compl xsk_meta;
	};
	unsigned int	gso_type;
	u32		tskey;

	/*
	 * Warning : all fields before dataref are cleared in __alloc_skb()
	 */
	atomic_t	dataref;
	unsigned int	xdp_frags_size;

	/* Intermediate layers must ensure that destructor_arg
	 * remains valid until skb destructor */
	void *		destructor_arg;

	/* must be last field, see pskb_expand_head() */
	skb_frag_t	frags[MAX_SKB_FRAGS];
};
```
It placed at the end of skb data specifically at `skb->head + skb->end`. If we call `skb_clone`, `__skb_clone` will copy head pointer and increase its `dataref`.
```C
/*
 * You should not add any new code to this function.  Add it to
 * __copy_skb_header above instead.
 */
static struct sk_buff *__skb_clone(struct sk_buff *n, struct sk_buff *skb)
{
#define C(x) n->x = skb->x

	n->next = n->prev = NULL;
	n->sk = NULL;
	__copy_skb_header(n, skb);
	....
	atomic_inc(&(skb_shinfo(skb)->dataref));
	skb->cloned = 1;

	return n;
#undef C
}
```

At `kfree_skb` -> `skb_release_data` it will decrement the `dataref`, if it zero it continue to free the rest.
```C
static void skb_release_data(struct sk_buff *skb)
{
	struct skb_shared_info *shinfo = skb_shinfo(skb);
	int i;

	if (skb->cloned &&
	    atomic_sub_return(skb->nohdr ? (1 << SKB_DATAREF_SHIFT) + 1 : 1,
			      &shinfo->dataref))
		goto exit;
	....
}
```

If we have freed skb, this skb still contain old data so `skb_shinfo(victim_skb)` also contain old data. Old data means, we have `skb_shinfo(victim_skb)->dataref` is 0. If this freed skb is cloned, the dataref will increase to 1 again. Then, this cloned skb is linked to `ctx->async_hold` and will call `__skb_queue_purge` and  reach `skb_release_data` again. Now kernel tries to free our the cloned_skb again based on old `skb_shared_info`.

If we have old spliced page pointer still live in `shinfo->frags`, we can do put page pointer again.
```C
static void skb_release_data(struct sk_buff *skb)
{
	struct skb_shared_info *shinfo = skb_shinfo(skb);
	int i;

	....
	for (i = 0; i < shinfo->nr_frags; i++)
		__skb_frag_unref(&shinfo->frags[i], skb->pp_recycle);
}
```
`__skb_frag_unref` will put page at `shinfo->frags[i]`. So from our freed skb, it already release this `shinfo` and also already put the page from frags, the cloned skb will do release for second time and the page from frags also will put for second time. So we have double put of page.

So these the steps for exploitation in COS and MITIGATION:
1. create two pair of TCP socket, server and client
2. setup another thread that will hang on tcp_recvmsg on client socket with MSG_WAITALL
3. setup TLS on client side
4. splice pipe data page via server socket, it will alloc its victim_skb and victim_skb->head and send to client
5. kernel will got data_ready notification and prepare `strp->anchor`, also set `strp->anchor->frag_list` victim_skb
6. close server socket, it will make tcp_recvmsg return and release the victim_skb (freeing skb A), at this time spliced page also released at `skb_release_data`
7. send 1 byte to a `dummy_cli` socket to allocate an interleaving skb (B) and avoid fast-path slab double-free detection.
8. call recvmsg on client socket with an invalid user pointer to trigger EFAULT and abort early, but leave the skb state in asynchronous processing.
9. read 1 byte from the `dummy_serv` socket to free the interleaving skb (B). By freeing skb B in between, we achieve a free A -> B -> A pattern that completely bypasses the slab allocator's adjacent double-free crash detection.
10. read from the pipe to extract the reclaimed memory and free the page to `pipe->tmp_page`.
11. complete the recvmsg on the client socket to trigger the page put from the Use-After-Free (UAF).
12. write to the pipe to get the page from `pipe->tmp_page` and allocate it.
13. touch all the `mmap` addresses to force new page table allocations over the overlapping UAF object.
14. read from the pipe; this reads the page (which now should be a pagetable) and frees it back into `pipe->tmp_page`, extracting the memory to user space so we can find the physical page holding the pagetable natively.

After this step we get a page freed that still hold by pipe. At this time we reclaim the page with pagetable and can write the pagetable by writing to the pipe.
We choose to target `core_pattern` to perform privilege escalation, we can deduce `core_pattern` physical address after we spray pagetable and read every memory that we allocate, it will install `empty_zero_page` PTE to us, then we can calculate `core_pattern` from that PTE.

# LTS exploit (Linux 6.12.x)

The exploit behavior is different between COS (Linux 6.1.x) and LTS (Linux 6.12.x) due to how `shinfo->dataref` is handled.

In Linux 6.1, during the first `kfree` in the UAF race, `skb_release_data` will subtract `shinfo->dataref` to 0, which successfully frees the page. Later, `skb_clone` in TLS will increase the freed skb's `shinfo->dataref` back to 1. As a result, the next `skb_release_data` will drop it to 0 again, successfully freeing the page a second time to achieve the double-put.

However, in newer LTS versions (Linux 6.12.x), the first `kfree` reaches `skb_release_data` but it does not decrease `shinfo->dataref` to 0 (it remains at 1). Because of this, the subsequent `skb_clone` in TLS will increase the freed skb's `shinfo->dataref` to 2 (from 1). When the double free happens, `shinfo->dataref` will only decrease to 1. This means it is **unable to free `shinfo`** and cannot trigger the page double-put.

Because we cannot double-put the page, the exploit strategy for 6.12.x must take advantage of the fact that the `sk_buff` struct itself is freed. The updated exploitation path utilizes `sk_buff` spray to overwrite the memory layout directly through fake `skb` structures.

The steps are roughly:
1. Bypass KASLR using the `flushandreload` timing side-channel on the kernel text.
2. Initialize and groom the slab cache using `socketpair` allocations for later spraying.
3. Trigger the UAF using the race condition between `tls_sw_recvmsg` and asynchronous socket processing, which frees an `skb`.
4. Construct a fake `skb` payload locally, setting controlled `head` and `data` pointers targeting `core_pattern` and the BSS section.
5. Spray the fake `skb` payload using `send()` to strategically reclaim the freed memory.
6. Call `recv()` on the connection to process the overwritten `skb`, translating the UAF into an arbitrary memory free of the `core_pattern` page.
7. Reclaim the freed `core_pattern` page by mapping user-space payloads (fake structures and the `|/proc/%P/fd/666` string), then wait until the `core_pattern` overwrite is confirmed.
8. Trigger a crash via null pointer dereference to invoke the core dump handler, executing our root payload from a `memfd_create` file descriptor.

### Arbitrary Free Primitive

Because the kernel's linear mapping translates physical addresses directly into virtual addresses, we could theoretically free any page backing a target object if we knew its page address. However, the `page` structure address itself is located at `*vmemmap_base`, which is subject to KASLR, meaning we don't necessarily know which page we are targeting. Because of this, using functions like `unpin_user_page` directly might not be viable for this exploit.

In the kernelCTF environment, the most we can typically obtain is a kernel text leak through a prefetch side-channel. Fortunately, we observed that `kfree` is capable of freeing **any** kernel virtual address, not just addresses allocated within the slab heap.

```C
void kfree(const void *object)
{
	struct folio *folio;
	struct slab *slab;
	struct kmem_cache *s;

	trace_kfree(_RET_IP_, object);

	if (unlikely(ZERO_OR_NULL_PTR(object)))
		return;

	if (unlikely(!is_slab_addr(object))) { // [1]
		folio = virt_to_folio(object);
		if (slab_virtual_enabled() &&
			CHECK_DATA_CORRUPTION(folio_test_slab(folio),
			"unexpected slab page mapped outside slab range"))
			return;
		free_large_kmalloc(folio, (void *)object);
		return;
	}

	slab = virt_to_slab(object);
	s = slab->slab_cache;
	__kmem_cache_free(s, (void *)object, _RET_IP_);
}
EXPORT_SYMBOL(kfree);

void free_large_kmalloc(struct folio *folio, void *object)
{
	unsigned int order = folio_order(folio);

	if (WARN_ON_ONCE(order == 0))
		pr_warn_once("object pointer: 0x%p\n", object);

	kmemleak_free(object);
	kasan_kfree_large(object);
	kmsan_kfree_large(object);

	mod_lruvec_page_state(folio_page(folio, 0), NR_SLAB_UNRECLAIMABLE_B,
			      -(PAGE_SIZE << order));
	__free_pages(folio_page(folio, 0), order);
}
```

As highlighted at `[1]`, if the object address does not belong to a slab, it will fall back to `free_large_kmalloc`, which happily calls `__free_pages` on the passed folio. This means we only need a valid kernel virtual address to trigger the free; we don't need its exact page address, because `virt_to_folio` will handle the conversion for us internally.

### Overwriting `core_pattern` and Root Shell Setup

Because we only possess a leaked kernel text address (and by extension, the data and BSS sections, since their offsets are relative), we investigated what would happen if we executed `kfree` on a kernel data address directly. Surprisingly, it works perfectly! 

When we `kfree` the address of `core_pattern`, its backing page is immediately inserted into the buddy allocator's free list. Inspecting this with the `buddy-dump` command in `bata-gef` confirms that our `core_pattern` page gets moved into the zone DMA32 PCP free list:

```text
core_pattern: 0xffffffff83808a60
----------------------------------------------------------- zone[1] @ 0xffff88811fffa140 (DMA32) -----------------------------------------------------------
---------------------------------------------------------------------- per_cpu_pageset ----------------------------------------------------------------------
cpu: 0
  pcp_index: 0, order: 0 (0x001000 bytes), mtype: 0 (=Unmovable)
  pcp_index: 1, order: 0 (0x001000 bytes), mtype: 1 (=Movable)
    page:0xffffea00000e0200  size:0x001000  virt:0xffff888003808000-0xffff888003809000  phys:0x0000000003808000-0x0000000003809000 (pcp)
```

Attempting to selectively reclaim this specific page might seem difficult at first glance, but it becomes trivial if we induce high memory pressure. By `mmap`ing a massive chunk of memory prior to the exploit and then continuing to `mmap` and write memory after freeing `core_pattern`, we can force the kernel to eventually reallocate the `core_pattern` page back to our user-space process.

Once we successfully map the page into user space, we can overwrite it. This technique requires us to fake the contents of the entire `core_pattern` page. This is straightforward, as the original page only contains kernel text pointers, which we can trivially calculate using our initial side-channel leak.

Finally, after `core_pattern` has been overwritten with our malicious path (e.g., pointing to an executable `memfd` payload like `|/proc/%P/fd/666`), we simply intentionally crash our own user-space process. The kernel invokes the core dump handler, reading our injected `core_pattern`, and executes our binary as `root` to read the flag.

### Generating the TLS Payload

Throughout the exploit source code representations, you will notice static `unsigned char tls_record[]` arrays containing the Application Data payload required to reach the vulnerable asynchronous processing path in `tls_sw_recvmsg`. 

These payloads correspond to a purely zeroed-out, AES-CCM encrypted TLS 1.2 record. To calculate valid frame offsets (including Explicit IVs and MAC tags), this structure can be dynamically generated using Python's `cryptography` library.

The script below demonstrates how these arrays were formulated:

```python
import struct
from cryptography.hazmat.primitives.ciphers.aead import AESCCM

def generate_tls_record():
    # The exploit configures TLS via setsockopt:
    # struct tls12_crypto_info_aes_ccm_128 crypto = {0};
    # This zeroes out the key, salt, and IV implicitly.
    
    key = b'\x00' * 16    # 128-bit key
    salt = b'\x00' * 4    # 4-byte implicit salt
    iv = b'\x00' * 8      # 8-byte explicit IV sent on the wire

    # The payload (arbitrary data that fits the math length needed)
    # In this case, exactly 11 bytes of application payload
    payload = b'\x00' * 11

    # AAD (Additional Authenticated Data) construction for TLS 1.2 AEAD
    # sequence_num (8) + type (1) + version (2) + length (2)
    seq_num = b'\x00' * 8     # Assuming it's the first record
    rec_type = b'\x17'        # Application Data (23)
    version = b'\x03\x03'     # TLS 1.2
    
    # Notice that the AAD length is the length of the *plaintext* payload, not the fully encrypted frame.
    length = struct.pack('>H', len(payload))
    aad = seq_num + rec_type + version + length

    # Initialize AES-CCM with our zeroed key and a tag length of 16 (128-bits)
    # The IV for AES-CCM in TLS 1.2 is salt (4 bytes) + explicit IV (8 bytes)
    nonce = salt + iv
    ccm = AESCCM(key, tag_length=16)

    # Encrypt the payload and generate the MAC tag automatically
    encrypted_payload_and_tag = ccm.encrypt(nonce, payload, aad)

    # Build the final TLS 1.2 Record
    # The length in the header covers the explicit IV + Ciphertext + Tag
    record_header = rec_type + version + struct.pack('>H', len(iv) + len(encrypted_payload_and_tag))

    final_record = record_header + iv + encrypted_payload_and_tag

    print("unsigned char tls_record[] = {")
    hex_bytes = [f"0x{b:02x}" for b in final_record]
    
    # Format nicely like the C string counterpart
    print("    " + ", ".join(hex_bytes[:12]) + ",")
    print("    " + ", ".join(hex_bytes[12:24]) + ",")
    print("    " + ", ".join(hex_bytes[24:36]) + ",")
    print("    " + ", ".join(hex_bytes[36:]) + "};")

if __name__ == "__main__":
    generate_tls_record()
```
