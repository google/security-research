# Exploit detail about CVE-2024-50164
If you want to get some base information about CVE-2024-50164, please read [vulnerability.md](./vulnerability.md) first.

## Background
eBPF, which stands for extended Berkeley Packet Filter, is a revolutionary technology developed for Linux that enables a variety of tracing and performance analysis tasks directly in the operating system's kernel. Originally designed for network packet filtering, eBPF has evolved into a versatile tool with applications far beyond its initial purpose.

One of the key features of eBPF is its ability to run sandboxed programs within the Linux kernel without having to change the kernel source code or load custom kernel modules. This allows developers to safely and efficiently extend kernel functionality for purposes such as performance monitoring, networking, and security enhancements. Programs written for eBPF are first compiled into an intermediate representation and then verified by the kernel before execution to ensure they do not hang the system or perform unsafe operations.


## Cause anaylysis
In the eBPF verifier, when the eBPF program calls the helper function provided by the kernel, to check the helper function call, the verifier calls the `check_helper_call` function. The `check_helper_call` function checks whether the call parameters meet the expectations through the `check_func_arg` function. Among them, for the expected parameters of `ARG_CONST_SIZE` and `ARG_CONST_SIZE_OR_ZERO`, the function `check_mem_size_reg` is used to check. When the eBPF program calls the helper function provided by the kernel, if the passed parameter contains a memory pointer, the function definition corresponding to the helper will define whether to read or write the memory pointer. Here are two examples of helper functions:

```
const struct bpf_func_proto bpf_get_current_comm_proto = {
	.func		= bpf_get_current_comm,
	.gpl_only	= false,
	.ret_type	= RET_INTEGER,
	.arg1_type	= ARG_PTR_TO_UNINIT_MEM,
	.arg2_type	= ARG_CONST_SIZE,
};

static const struct bpf_func_proto bpf_strncmp_proto = {
	.func		= bpf_strncmp,
	.gpl_only	= false,
	.ret_type	= RET_INTEGER,
	.arg1_type	= ARG_PTR_TO_MEM | MEM_RDONLY,
	.arg2_type	= ARG_CONST_SIZE,
	.arg3_type	= ARG_PTR_TO_CONST_STR,
};
```

For parameters of type `ARG_CONST_SIZE` and `ARG_CONST_SIZE_OR_ZERO`, the verifier calls `check_mem_size_reg` to check whether the read and write operations and length of the memory parameter are allowed.

```
static int check_mem_size_reg(struct bpf_verifier_env *env,
			      struct bpf_reg_state *reg, u32 regno,
			      bool zero_size_allowed,
			      struct bpf_call_arg_meta *meta)
{
	int err;

	...
	meta->msize_max_value = reg->umax_value;

	/* The register is SCALAR_VALUE; the access check
	 * happens using its boundaries.
	 */
	if (!tnum_is_const(reg->var_off))
		/* For unprivileged variable accesses, disable raw
		 * mode so that the program is required to
		 * initialize all the memory that the helper could
		 * just partially fill up.
		 */
		meta = NULL; (1)

	...
	err = check_helper_mem_access(env, regno - 1,
				      reg->umax_value,
				      zero_size_allowed, meta);
	if (!err)
		err = mark_chain_precision(env, regno);
	return err;
}
```

However, at position (1), due to some considerations, the function `check_mem_size_reg` sets `meta` to Null. The `meta` is used to determine whether the function will read or write memory in the `check_helper_mem_access` function:
```
static int check_helper_mem_access(struct bpf_verifier_env *env, int regno,
				   int access_size, bool zero_size_allowed,
				   struct bpf_call_arg_meta *meta)
{
	struct bpf_reg_state *regs = cur_regs(env), *reg = &regs[regno];
	u32 *max_access;

	switch (base_type(reg->type)) {
	...
	case PTR_TO_MAP_VALUE:
		if (check_map_access_type(env, regno, reg->off, access_size,
					  meta && meta->raw_mode ? BPF_WRITE :
					  BPF_READ)) //Meta is used here to determine whether the function will read or write memory
			return -EACCES;
		return check_map_access(env, regno, reg->off, access_size,
					zero_size_allowed, ACCESS_HELPER);
	...
	}
}

```
This allows us to write a map with flag BPF_F_RDONLY_PROG.

## Exploit

Exploiting this vulnerability is similar to exploiting CVE-2024-49861, and both use a key piece of code in the verifier:

```
static bool bpf_map_is_rdonly(const struct bpf_map *map)
{
	/* A map is considered read-only if the following condition are true:
	 *
	 * 1) BPF program side cannot change any of the map content. The
	 *    BPF_F_RDONLY_PROG flag is throughout the lifetime of a map
	 *    and was set at map creation time.
	 * 2) The map value(s) have been initialized from user space by a
	 *    loader and then "frozen", such that no new map update/delete
	 *    operations from syscall side are possible for the rest of
	 *    the map's lifetime from that point onwards.
	 * 3) Any parallel/pending map update/delete operations from syscall
	 *    side have been completed. Only after that point, it's safe to
	 *    assume that map value(s) are immutable.
	 */
	return (map->map_flags & BPF_F_RDONLY_PROG) &&
	       READ_ONCE(map->frozen) &&
	       !bpf_map_write_active(map);
}

static int check_mem_access(struct bpf_verifier_env *env, int insn_idx, u32 regno,
			    int off, int bpf_size, enum bpf_access_type t,
			    int value_regno, bool strict_alignment_once, bool is_ldsx)
{
	...
	if (reg->type == PTR_TO_MAP_KEY) {
		...
	} else if (reg->type == PTR_TO_MAP_VALUE) {
		...
		err = check_map_access_type(env, regno, off, size, t);
		if (err)
			return err;
		err = check_map_access(env, regno, off, size, false, ACCESS_DIRECT);
		...
		if (kptr_field) {
			...
		} else if (t == BPF_READ && value_regno >= 0) {
			struct bpf_map *map = reg->map_ptr;

			/* if map is read-only, track its contents as scalars */
			if (tnum_is_const(reg->var_off) &&
			    bpf_map_is_rdonly(map) &&
			    map->ops->map_direct_value_addr) {
				int map_off = off + reg->var_off.value;
				u64 val = 0;

				err = bpf_map_direct_read(map, map_off, size,
							  &val, is_ldsx);
				if (err)
					return err;

				regs[value_regno].type = SCALAR_VALUE;
				__mark_reg_known(&regs[value_regno], val);
			} else {
				mark_reg_unknown(env, regs, value_regno);
			}
		}
```
When a map is frozen with the flag `BPF_F_RDONLY_PROG`, the verifier assumes its contents are unchanged. The verifier will directly read the map value based on the offset for subsequent checks. This feature can be exploited to modify the value of a frozen map with the flag `BPF_F_RDONLY_PROG`, thereby bypassing the verifier's checks and performing out-of-bounds reads and writes.


I used the following steps to exploit it:

1. Create an array map with value_size `0x10000` and max_entries `1`. Let's assume this is `map A`.
2. Create a ringbuf map with max_entries of `0x4000`. Let's assume this is `map B`.
3. Create an array map with value_size `0x10000` and max_entries `1`. Let's assume this is `map C`.
4. Malloc a memory of size `0x10000` and make the following modifications: 
   1.  Copy the integer `0x40000` to the memory offset 0.
Use `update_map_element` to modify element 0 of `map A` using this memory.
   2. Copy the integer `8` to the memory offset 0.
Use `update_map_element` to modify element 0 of `map C` using this memory.
5. Freeze `map A`.
6. Run the following ebpf programï¼š

	1. Use the helper function `ringbuf_reserve` to request a memory of length 0x2800 from `map B`. We assume this is `memory A`
	2. Get the memory of element 0 from `map C` through the helper function `map_lookup_elem`. We assume this is `memory B`.  (`memory B` was initialized in `step 4`.)
	3. Load `memory B [0]` into `BPF_REG_8`. Here will finally set `BPF_REG_8 = 8` because our initialization in `step 4`.
	4. Set the `umax`, `umin`,`smin` of `BPF_REG_8` by code `BPF_JSGT`, `BPF_JLT`, `BPF_JGT`. We need to bypass some check in `check_mem_size_reg`:
```
   static int check_mem_size_reg(struct bpf_verifier_env *env,
			      struct bpf_reg_state *reg, u32 regno,
			      bool zero_size_allowed,
			      struct bpf_call_arg_meta *meta)
{
	...
	if (reg->smin_value < 0) {
		verbose(env, "R%d min value is negative, either use unsigned or 'var &= const'\n",
			regno);
		return -EACCES;
	}

	if (reg->umin_value == 0) {
		...
	}

	if (reg->umax_value >= BPF_MAX_VAR_SIZ) {
		verbose(env, "R%d unbounded memory access, use 'var &= const' or 'if (var < const)'\n",
			regno);
		return -EACCES;
	}
	err = check_helper_mem_access(env, regno - 1,
				      reg->umax_value,
				      zero_size_allowed, meta);
	if (!err)
		err = mark_chain_precision(env, regno);
	return err;
}
```

6.  5. Get the memory of element 0 from `map A` through the helper function `map_lookup_elem`. We assume this is `memory C`.  (`memory C` was initialized in `step 4`.)	
    6. Use `PTR_TO_CTX` as the first parameter. Use `memory C` as the third parameter. Use `BPF_REG_8` as the fourth parameter to store the data. Call helper function `skb_load_bytes`. We will modify the memory of `memory C`. Due to the vulnerability, the verifier did not find that we modified the value of a frozen map. From this point on, the actual runtime behavior differs from the verifier's assumptions. How can I control what is written to `memory C`? By controlling what is sent:
	```
	if(10 != write(socks[1], "\x00\x10\x00\x00\x00\x00\x00\x00\x00\x00", 10))
    {
        goto done;
    }
	```
	 
	7. Load `memory C+0` and call the helper function `ringbuf_reserve` with it as the second parameter. Due to the following code, the verifier will think that the value of the second parameter is `0x10000` because we froze `map A` in step 5 :
   ```
   static int check_mem_access(struct bpf_verifier_env *env, int insn_idx, u32 regno,
			    int off, int bpf_size, enum bpf_access_type t,
			    int value_regno, bool strict_alignment_once, bool is_ldsx)
   ...
   } else if (reg->type == PTR_TO_MAP_VALUE) {
		...
		} else if (t == BPF_READ && value_regno >= 0) {
			struct bpf_map *map = reg->map_ptr;

			/* if map is read-only, track its contents as scalars */
			if (tnum_is_const(reg->var_off) &&
			    bpf_map_is_rdonly(map) &&
			    map->ops->map_direct_value_addr) {
				int map_off = off + reg->var_off.value;
				u64 val = 0;

				err = bpf_map_direct_read(map, map_off, size,
							  &val, is_ldsx);
				if (err)
					return err;

				regs[value_regno].type = SCALAR_VALUE;
				__mark_reg_known(&regs[value_regno], val);
			} else {
				mark_reg_unknown(env, regs, value_regno);
			}
		}
		...
   ```
	8. After the previous step, we get a memory, we assume it is `memory D`. Interestingly, because of the vulnerability, when executing this ebpf code, calling the `skb_load_bytes` function will modify the memory of `memory C + 0`. But the verifier does not realize that the memory of `memory C + 0` has been modified. Because the above code, the verifier still thinks that it is `0x40000`. Therefore, the verifier thinks that the legal length of `memory D` is `0x40000`, but in actual execution, due to the modification of `skb_load_bytes`, the actual length of `memory D` is `0x1000`. This means that we can bypass the verifier's check and perform memory out-of-bounds read and write to `memory D`. 
	9. Get `map C`'s ops (`struct bpf_map->ops`)and rcu(`struct bpf_map->rcu`) by using memory out-of-bounds of `memory D`. (`Map B` and `map C` are adjacent in memory) Calculate the kaslr offset through the leaked address by `map C`'s ops. Because `struct bpf_map->rcu` is a two-way pointer pointing to itself, we can get the address of `map C` through this address.
	10. Change `map C -> ops` to `&map C->value`(`struct bpf_array->value`). Change the new `map C -> ops -> map_delete_elem` to the address of the ROP gadget of `pop rbx ; ret`.
	11. Store the obtained kaslr offset into `memory A` for subsequent use.

7. Run another eBPF program:

	1. Store the ROP gadget on the stack
	2. Delete any element in map C through the helper function `map_delete_elem`. This will jmp to the ROP gadget `pop rbx ; ret`. The stack will look like follow.So when it jmp to `pop rbx ; ret`, it will finally jmp to the ROP gadget we stored on the stack.  
    
```
	RSP -> | Return address of map_delete_elem |

           |       ROP gadget we pad           |
```

## Why do I need to run two ebpf programs?
Because even though we have modified `map C -> ops -> map_delete_elem` when executing the first eBPF program, we cannot directly hijack the control flow because of the following code:

```
static int do_misc_fixups(struct bpf_verifier_env *env)
{
	...
	if (prog->jit_requested && BITS_PER_LONG == 64 &&
		    (insn->imm == BPF_FUNC_map_lookup_elem ||
		     insn->imm == BPF_FUNC_map_update_elem ||
		     insn->imm == BPF_FUNC_map_delete_elem ||
		     insn->imm == BPF_FUNC_map_push_elem   ||
		     insn->imm == BPF_FUNC_map_pop_elem    ||
		     insn->imm == BPF_FUNC_map_peek_elem   ||
		     insn->imm == BPF_FUNC_redirect_map    ||
		     insn->imm == BPF_FUNC_for_each_map_elem ||
		     insn->imm == BPF_FUNC_map_lookup_percpu_elem)) {
			aux = &env->insn_aux_data[i + delta];
			if (bpf_map_ptr_poisoned(aux))
				goto patch_call_imm;

			map_ptr = BPF_MAP_PTR(aux->map_ptr_state);
			ops = map_ptr->ops;
			...
			switch (insn->imm) {
			...
			case BPF_FUNC_map_delete_elem:
				insn->imm = BPF_CALL_IMM(ops->map_delete_elem);
				continue;
			...
```
This code means that as long as our eBPF code passes the verifier, the verifier will directly hardcode ops->map_delete_elem into the eBPF code. Therefore, only after modifying ops->map_delete_elem and running a new eBPF code, can the control flow be hijacked.
