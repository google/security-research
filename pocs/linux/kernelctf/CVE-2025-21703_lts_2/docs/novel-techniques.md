# User-Controlled Kernel Variable

`/dev/random` and `/dev/urandom` are two pseudo character devices built into the kernel by default, and they are widely used across nearly all Linux-based operating systems. Users can read data from either device to obtain sufficiently random values with high entropy. However, a closer inspection reveals that these devices are not only readable but also **writable** by normal users!

``` bash
user@lts-6:/$ ls -al /dev/urandom /dev/random
crw-rw-rw- 1 nobody nogroup 1, 8 Jul 12 17:01 /dev/random
crw-rw-rw- 1 nobody nogroup 1, 9 Jul 12 17:01 /dev/urandom
```

Looking into the source code, the `devlist[]` array defines the file operation tables and access permissions for these devices [1, 2]:

``` c
static const struct memdev {
    const char *name;
    const struct file_operations *fops;
    fmode_t fmode;
    umode_t mode;
} devlist[] = {
    // [...]
    [8] = { "random", &random_fops, FMODE_NOWAIT, 0666 }, // [1]
    [9] = { "urandom", &urandom_fops, FMODE_NOWAIT, 0666 }, // [2]
    // [...]
};
```

Both file operation tables use the same write iterator handler: `random_write_iter()` [3, 4].

``` c
const struct file_operations random_fops = {
    // [...]
    .write_iter = random_write_iter, // [3]
    // [...]
};

const struct file_operations urandom_fops = {
    // [...]
    .write_iter = random_write_iter, // [4]
    // [...]
}
```

The function `random_write_iter()` simply passes the iter parameter to `write_pool_user()` [5]. This function reads 0x40 bytes of data from user space [6] and stores it in a local variable `block[]`. The `block[]` is then passed to `mix_pool_bytes()`. This process continues until the input is exhausted [7] or a pending signal is detected in the current task [8], meaning that the termination of the iterations is effectively user-controlled.

``` c
static ssize_t random_write_iter(struct kiocb *kiocb, struct iov_iter *iter)
{
    return write_pool_user(iter); // [5]
}

static ssize_t write_pool_user(struct iov_iter *iter)
{
    u8 block[BLAKE2S_BLOCK_SIZE /* 64 */];
    ssize_t ret = 0;
    size_t copied;

    // [...]
    for (;;) {
        copied = copy_from_iter(block, sizeof(block), iter); // [6]
        ret += copied;
        mix_pool_bytes(block, copied);
        // [...]
        if (!iov_iter_count(iter) || copied != sizeof(block)) // [7]
            break;

        if (ret % PAGE_SIZE == 0) {
            if (signal_pending(current)) // [8]
                break;
            cond_resched();
        }
    }
    // [...]
}
```

Internally, `mix_pool_bytes()` calls `blake2s_update()` with the kernel variable `&input_pool.hash` [9]. The `hash` member of the `input_pool` variable is a `struct blake2s_state`, which tracks the internal state of the BLAKE2s hashing function.

``` c
static void mix_pool_bytes(const void *buf, size_t len)
{
    unsigned long flags;

    // [...]
    _mix_pool_bytes(buf, len /* 0x40 */); // <-----------
    // [...]
}

static void _mix_pool_bytes(const void *buf, size_t len)
{
    blake2s_update(&input_pool.hash, buf, len /* 0x40 */); // [9]
}
```

The `blake2s_update()` function copies the input block into the hash buffer [10]. Once the buffer is filled, it calls `blake2s_compress()` to compute the hash. Since the buffer contents remain intact after compression, `blake2s_update()` behaves like a `memcpy()` that writes 0x40 bytes directly into `&input_pool.hash.buf`.

``` c
void blake2s_update(struct blake2s_state *state, const u8 *in, size_t inlen /* 0x40 */)
{
    const size_t fill = BLAKE2S_BLOCK_SIZE /* 0x40 */ - state->buflen;

    // [...]
    if (inlen > fill) {
        memcpy(state->buf + state->buflen, in, fill);
        blake2s_compress(state, state->buf, 1, BLAKE2S_BLOCK_SIZE); // [10]
        state->buflen = 0;
        in += fill;
        inlen -= fill;
    }
    // [...]
    memcpy(state->buf + state->buflen, in, inlen);
    state->buflen += inlen;
}
```

As a result, we can **fully control the contents of `&input_pool.hash.buf` with 0x40 bytes of arbitrary data**. This enables precise control over a kernel variable, which can be leveraged to construct fake objects or ROP chains, given a kernel base address leak.

```
pwndbg> x/10gx &input_pool.hash.buf
0xffffffff83e85030 <input_pool+48>:     0x4141414141414141      0x4141414141414141
0xffffffff83e85040 <input_pool+64>:     0x4141414141414141      0x4141414141414141
0xffffffff83e85050 <input_pool+80>:     0x4141414141414141      0x4141414141414141
0xffffffff83e85060 <input_pool+96>:     0x4141414141414141      0x4141414141414141
```

However, this technique has two drawbacks:

First, the data is not persistent. Some functions may modify the hash buffer internally, and the kernel periodically updates it via a timer as well. To increase reliability, multiple threads can be created and pinned to a specific CPU. If they continuously call `mix_pool_bytes()` through `write_pool_user()`, there is a high likelihood that the hash buffer will remain under user control.

Second, the value of `state->buflen` determines the starting offset of the input data. Although it appears to be 8-byte aligned and capable of holding valid pointers, I have not found a reliable way to accurately predict its value.

I think the first drawback can be easily mitigated by creating a large number of threads. However, the second drawback makes it unreliable to construct a fake object. But in use cases where only a single ROP gadget is needed for stack pivoting — such as mine — this limitation has no practical impact.