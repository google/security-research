# User-Controlled Kernel Variable

`/dev/random` and `/dev/urandom` are two pseudo character devices built into the kernel by default, and they are widely used across nearly all Linux-based operating systems. Users can read data from either device to obtain sufficiently random values with high entropy. However, a closer inspection reveals that these devices are not only readable but also **writable** by normal users!

``` bash
user@lts-6:/$ ls -al /dev/urandom /dev/random
crw-rw-rw- 1 nobody nogroup 1, 8 Jul 12 17:01 /dev/random
crw-rw-rw- 1 nobody nogroup 1, 9 Jul 12 17:01 /dev/urandom
```

Looking into the source code, the `devlist[]` array defines the file operation tables and access permissions for these devices [1, 2]:

``` c
static const struct memdev {
    const char *name;
    const struct file_operations *fops;
    fmode_t fmode;
    umode_t mode;
} devlist[] = {
    // [...]
    [8] = { "random", &random_fops, FMODE_NOWAIT, 0666 }, // [1]
    [9] = { "urandom", &urandom_fops, FMODE_NOWAIT, 0666 }, // [2]
    // [...]
};
```

Both file operation tables use the same write iterator handler: `random_write_iter()` [3, 4].

``` c
const struct file_operations random_fops = {
    // [...]
    .write_iter = random_write_iter, // [3]
    // [...]
};

const struct file_operations urandom_fops = {
    // [...]
    .write_iter = random_write_iter, // [4]
    // [...]
}
```

The function `random_write_iter()` simply passes the iter parameter to `write_pool_user()` [5]. This function reads 0x40 bytes of data from user space [6] and stores it in a local variable `block[]`. The `block[]` is then passed to `mix_pool_bytes()`. This process continues until the input is exhausted [7] or a pending signal is detected in the current task [8], meaning that the termination of the iterations is effectively user-controlled.

``` c
static ssize_t random_write_iter(struct kiocb *kiocb, struct iov_iter *iter)
{
    return write_pool_user(iter); // [5]
}

static ssize_t write_pool_user(struct iov_iter *iter)
{
    u8 block[BLAKE2S_BLOCK_SIZE /* 64 */];
    ssize_t ret = 0;
    size_t copied;

    // [...]
    for (;;) {
        copied = copy_from_iter(block, sizeof(block), iter); // [6]
        ret += copied;
        mix_pool_bytes(block, copied);
        // [...]
        if (!iov_iter_count(iter) || copied != sizeof(block)) // [7]
            break;

        if (ret % PAGE_SIZE == 0) {
            if (signal_pending(current)) // [8]
                break;
            cond_resched();
        }
    }
    // [...]
}
```

Internally, `mix_pool_bytes()` calls `blake2s_update()` with the kernel variable `&input_pool.hash` [9]. The `hash` member of the `input_pool` variable is a `struct blake2s_state`, which tracks the internal state of the BLAKE2s hashing function.

``` c
static void mix_pool_bytes(const void *buf, size_t len)
{
    unsigned long flags;

    // [...]
    _mix_pool_bytes(buf, len /* 0x40 */); // <-----------
    // [...]
}

static void _mix_pool_bytes(const void *buf, size_t len)
{
    blake2s_update(&input_pool.hash, buf, len /* 0x40 */); // [9]
}
```

The `blake2s_update()` function copies the input block into the hash buffer [10]. Once the buffer is filled, it calls `blake2s_compress()` to compute the hash. Since the buffer contents remain intact after compression, `blake2s_update()` behaves like a `memcpy()` that writes 0x40 bytes directly into `&input_pool.hash.buf`.

``` c
void blake2s_update(struct blake2s_state *state, const u8 *in, size_t inlen /* 0x40 */)
{
    const size_t fill = BLAKE2S_BLOCK_SIZE /* 0x40 */ - state->buflen;

    // [...]
    if (inlen > fill) {
        memcpy(state->buf + state->buflen, in, fill);
        blake2s_compress(state, state->buf, 1, BLAKE2S_BLOCK_SIZE); // [10]
        state->buflen = 0;
        in += fill;
        inlen -= fill;
    }
    // [...]
    memcpy(state->buf + state->buflen, in, inlen);
    state->buflen += inlen;
}
```

As a result, we can **fully control the contents of `&input_pool.hash.buf` with 0x40 bytes of arbitrary data**. This enables precise control over a kernel variable, which can be leveraged to construct fake objects or ROP chains, given a kernel base address leak.

```
pwndbg> x/10gx &input_pool.hash.buf
0xffffffff83e85030 <input_pool+48>:     0x4141414141414141      0x4141414141414141
0xffffffff83e85040 <input_pool+64>:     0x4141414141414141      0x4141414141414141
0xffffffff83e85050 <input_pool+80>:     0x4141414141414141      0x4141414141414141
0xffffffff83e85060 <input_pool+96>:     0x4141414141414141      0x4141414141414141
```

However, this technique has two drawbacks, which we will analyze separately in the following two sections.

## 1. Non-Persistent Hash Buffer

First, the data is not persistent. Some functions may modify the hash buffer internally, and the kernel periodically updates it via a timer as well. To increase reliability, multiple threads can be created and pinned to a specific CPU. If they continuously call `mix_pool_bytes()` through `write_pool_user()`, there is a high likelihood that the hash buffer will remain under user control.

I believe this drawback can be mitigated by creating a large number of threads.

## 2. Non-deterministic Offset

Second, the value of `state->buflen` determines the starting offset of the input data, but its initial value is not reliably predictable.

To address this problem, we can create a new process to flush the hash buffer.

When a process is created, the canary is initialized by the `get_random_canary()` function.

``` c
static struct task_struct *dup_task_struct(struct task_struct *orig, int node)
{
    // [...]
    tsk->stack_canary = get_random_canary();
    // [...]
}
```

This function internally calls `_get_random_bytes()`. The simplified execution flow is as follows:

```
copy_process()
=> dup_task_struct()
  => get_random_canary()
    => get_random_long()
      => get_random_u64()
        => _get_random_bytes()
```

In `_get_random_bytes()`, the parameter len is 8 because it generates a `u64`. The function then calls `crng_make_state()` with `first_block_len` set to 8 [1].

``` c
static void _get_random_bytes(void *buf, size_t len /* 8 */)
{
    u32 chacha_state[CHACHA_STATE_WORDS];
    u8 tmp[CHACHA_BLOCK_SIZE];
    size_t first_block_len;

    if (!len)
        return;

    first_block_len = min_t(size_t, 32, len);
    crng_make_state(chacha_state, buf, first_block_len /* 8 */); // [1]
    len -= first_block_len;
    buf += first_block_len;

    while (len) {
        // [...]
    }

    memzero_explicit(chacha_state, sizeof(chacha_state));
}
```

If the kernel CRNG (Cryptographic Random Number Generator) is not yet ready, the `extract_entropy()` function will be invoked [2]. In general, the CRNG becomes ready only after enough entropy has been collected, so it is typically not ready immediately after booting.

``` c
static void crng_make_state(u32 chacha_state[CHACHA_STATE_WORDS],
                u8 *random_data, size_t random_data_len)
{
    unsigned long flags;
    struct crng *crng;

    // [...]
    if (!crng_ready()) {
        bool ready;

        spin_lock_irqsave(&base_crng.lock, flags);
        ready = crng_ready();
        if (!ready) {
            if (crng_init == CRNG_EMPTY)
                extract_entropy(base_crng.key, sizeof(base_crng.key)); // [2]
            // [...]
        }
        spin_unlock_irqrestore(&base_crng.lock, flags);
        if (!ready)
            return;
    }
}
```

While extracting entropy, the function `blake2s_final()` [3] is called as part of the process. This function calls `memzero_explicit()` to reset `&input_pool.hash` at the end [4]. After that, the `blake2s_init_key()` function is called [5], which internally calls `__blake2s_init()` to set `state->buflen` to 64 [6].

``` c
static void extract_entropy(void *buf, size_t len)
{
    // [...]
    blake2s_final(&input_pool.hash, seed); // [3]
    
    // [...]
    blake2s_init_key(&input_pool.hash, BLAKE2S_HASH_SIZE /* 32 */, next_key, sizeof(next_key)); // [5]
}

void blake2s_final(struct blake2s_state *state, u8 *out)
{
    // [...]
    memzero_explicit(state, sizeof(*state)); // [4]
}

static inline void __blake2s_init(struct blake2s_state *state, size_t outlen,
                  const void *key, size_t keylen)
{
    // [...]
    if (keylen) {
        // [...]
        state->buflen = BLAKE2S_BLOCK_SIZE; // [6]
    }
}
```

Finally, when `dup_task_struct()` completes and returns to `copy_process()`, the `add_latent_entropy()` function is invoked, which internally sets `state->buflen` to 8 [7].

``` c
pid_t kernel_clone(struct kernel_clone_args *args)
{
    // [...]
    p = copy_process(NULL, trace, NUMA_NO_NODE, args);
    add_latent_entropy(); // <-----------
    // [...]
}

static inline void add_latent_entropy(void)
{
    // [...]
    add_device_randomness(NULL, 0); // <-----------
    // [...]
}

void add_device_randomness(const void *buf, size_t len)
{
    unsigned long entropy = random_get_entropy();
    unsigned long flags;

    spin_lock_irqsave(&input_pool.lock, flags);
    _mix_pool_bytes(&entropy, sizeof(entropy) /* 8 */); // [7]
    _mix_pool_bytes(buf, len);
    spin_unlock_irqrestore(&input_pool.lock, flags);
}
```

As a result, in theory, after cloning a new process, the offset within the input hash buffer (`state->buf`) will be set to 8, which makes it predictable.

