# Novel Techniques

While exploiting CVE-2023-3390, we found several novel insights and exploit techniques, especially with the detailed research about mitigation kernel configs & custom mitigations. With these comprehensive novel techniques and approaches, we succeed to pwn all three targets of kernelCTF with only one vulnerability.


## Fundamental bypass against `CONFIG_KMALLOC_SPLIT_VARSIZE` for UAF on variable-sized object

On [current UAF mitigations of mitigation instance](https://github.com/thejh/linux/blob/slub-virtual/MITIGATION_README), in-cache UAF exploit is mitigated by variable-sized object isolation, which isolates the slab cache for objects that can have dynamic size. It effectively prevents the reclamation of fixed-sized vulnerable objects with variable-sized objects.

However, the current mitigation approach cannot fundamentally prevent UAF exploitation if the vulnerable object itself is a variable-sized object. This also applies to `CVE-2023-3390` in which the vulnerable object is variable-sized `struct nft_set`.

Some may argue that the mitigation is still effective because it hinders usage of useful fixed-sized target objects for kernel slab leak, KASLR leak, and PC control, from variable-sized slab caches. However, we found that there are numerous variable-sized target objects that can still be exploited. For example, `dyn-kmalloc-cg-1k` slab has `struct pipe_buffer` as a variable-sized target object (refer [8. LTS 6.1 Mitigation Instance section of exploit.md](./exploit.md#8-lts-61-mitigation-instance)) and `kmalloc-1k` slab has `struct nft_set` as a target object (refer [Usage of `struct nft_set` as a novel target object section](#usage-of-struct-nft_set-as-a-novel-target-object)). Moreover, by applying the methodologies used in the paper mentioned below (such as customized fuzzing) or current mitigation (compile-time object size checking), we can automatically identify these variable-sized target objects.

We found that the similar concept of mitigation is also suggested in the paper, [A Systematic Study of Elastic Objects in Kernel Exploitation (Yueqi Chen et al., 2020)](https://dl.acm.org/doi/10.1145/3372297.3423353). However, we believe that issues with the current `CONFIG_KMALLOC_SPLIT_VARSIZE` would also apply in this case.

## `CONFIG_DEBUG_LIST` leading to expand vulnerability's capability

While changing the UAF vulnerability into double free primitive, we noticed that `CONFIG_DEBUG_LIST` can expand the vulnerability's capability contrary to its original intention.

Here is the interesting code flow caused by `CONFIG_DEBUG_LIST` that we have observed.
`struct nft_set` object is freed twice through the following two code paths (refer [From 4. UAF to double free section of exploit.md](./exploit.md#4-from-uaf-to-double-free) for details of each functions).
```
# On cleanup routines of NFT_MSG_NEWRULE
- nf_tables_newrule
└ - nf_tables_rule_release
  └ - nft_rule_expr_deactivate
    └ - nf_tables_deactivate_set
      └	- nf_tables_unbind_set
        └ - list_del_rcu // [1]
    - nf_tables_rule_destroy
    └ - nf_tables_expr_destroy 
      └	- nft_set_destroy // [2]
```
```
# On transaction abort routine
- nf_tables_abort
└ - __nf_tables_abort 
  └ - nft_rule_expr_deactivate
    └ - nf_tables_deactivate_set
      └	- nf_tables_unbind_set
        └ - list_del_rcu // [3]
  - nf_tables_abort_release 
  └ - nf_tables_rule_destroy
    └ - nf_tables_expr_destroy 
      └	- nft_set_destroy // [4]
```
Notice that `struct nft_set` is not only freed in both paths [2][4], but also unlinked from the same list twice (double unlink) by `list_del_rcu` function [1][3]. 
- [include/linux/rculist.h:list_del_rcu()](https://git.kernel.org/pub/scm/linux/kernel/git/stable/linux.git/tree/include/linux/rculist.h?h=linux-6.1.y&id=d2869ace6eeb8ea8a6e70e6904524c5a6456d3fb#n109)
```c
/**
 * list_del_rcu - deletes entry from list without re-initialization
 * @entry: the element to delete from the list.
 *
 * Note: list_empty() on entry does not return true after this,
 * the entry is in an undefined state. It is useful for RCU based
 * lockfree traversal.
 *
 * In particular, it means that we can not poison the forward
 * pointers that may still be used for walking the list.
 *
 * The caller must take whatever precautions are necessary
 * (such as holding appropriate locks) to avoid racing
 * with another list-mutation primitive, such as list_del_rcu()
 * or list_add_rcu(), running on this same list.
 * However, it is perfectly legal to run concurrently with
 * the _rcu list-traversal primitives, such as
 * list_for_each_entry_rcu().
 *
 * Note that the caller is not permitted to immediately free
 * the newly deleted entry.  Instead, either synchronize_rcu()
 * or call_rcu() must be used to defer freeing until an RCU
 * grace period has elapsed.
 */
static inline void list_del_rcu(struct list_head *entry)
{
	__list_del_entry(entry);
	entry->prev = LIST_POISON2;
}
```
The `list_del_rcu` function performs deletion from the list and poisons the prev pointer of the list entry with the magic values (`LIST_POISON2`). This makes the entry in the undefined status, preventing further usage of list entry. However, as the comment says, the `list_del_rcu` function doesn't corrupt the next pointer, so the list element is still accessible by traversal routines even after deletion.

Then, how can the same list entry of `struct nft_set` object be unlinked twice without triggering GP fault on poison values? We can find the answer from `__list_del_entry` function and `__list_del_entry_valid` function.
- [include/linux/list.h:__list_del_entry()](https://git.kernel.org/pub/scm/linux/kernel/git/stable/linux.git/tree/include/linux/list.h?h=linux-6.1.y&id=d2869ace6eeb8ea8a6e70e6904524c5a6456d3fb#n132)
```c
static inline void __list_del_entry(struct list_head *entry)
{
	if (!__list_del_entry_valid(entry))
		return;

	__list_del(entry->prev, entry->next);
}
```
- [lib/list_debug.c:__list_del_entry_valid()](https://git.kernel.org/pub/scm/linux/kernel/git/stable/linux.git/tree/lib/list_debug.c?h=linux-6.1.y&id=d2869ace6eeb8ea8a6e70e6904524c5a6456d3fb#n42)
```c
bool __list_del_entry_valid(struct list_head *entry)
{
	struct list_head *prev, *next;

	prev = entry->prev;
	next = entry->next;

	if (CHECK_DATA_CORRUPTION(next == NULL,
			"list_del corruption, %px->next is NULL\n", entry) ||
	    CHECK_DATA_CORRUPTION(prev == NULL,
			"list_del corruption, %px->prev is NULL\n", entry) ||
	    CHECK_DATA_CORRUPTION(next == LIST_POISON1,
			"list_del corruption, %px->next is LIST_POISON1 (%px)\n",
			entry, LIST_POISON1) ||
	    CHECK_DATA_CORRUPTION(prev == LIST_POISON2,
			"list_del corruption, %px->prev is LIST_POISON2 (%px)\n",
			entry, LIST_POISON2) ||
	    CHECK_DATA_CORRUPTION(prev->next != entry,
			"list_del corruption. prev->next should be %px, but was %px. (prev=%px)\n",
			entry, prev->next, prev) ||
	    CHECK_DATA_CORRUPTION(next->prev != entry,
			"list_del corruption. next->prev should be %px, but was %px. (next=%px)\n",
			entry, next->prev, next))
		return false;

	return true;

}
```
With `CONFIG_DEBUG_LIST` config enabled, `__list_del_entry` function validates given list entry with `__list_del_entry_valid` function; `__list_del_entry_valid` function performs a basic doubly-linked list sanity check and returns a boolean value that indicates which current list entry is corrupted or not. Thus, if the list entry is already in an invalid state (i.e., the `__list_del_entry_valid` returns `false`), `__list_del_entry` function will return immediately without actually unlinking the list entry. 
Thanks to this behavior, our exploit could successfully perform double unlink (and double free) of `struct nft_set`, with harmless kernel warnings.
- Kernel warning via double list unlink
```
[    6.078010] ------------[ cut here ]------------
[    6.078158] list_del corruption, ffff88800506e400->prev is LIST_POISON2 (dead000000000122)
[    6.078743] WARNING: CPU: 0 PID: 145 at lib/list_debug.c:56 __list_del_entry_valid+0x9a/0xd0
[    6.079275] Modules linked in:
[    6.079573] CPU: 0 PID: 145 Comm: poc Not tainted 6.1.31+ #1
[    6.079867] Hardware name: QEMU Standard PC (i440FX + PIIX, 1996), BIOS 1.15.0-1 04/01/2014
[    6.080178] RIP: 0010:__list_del_entry_valid+0x9a/0xd0
// omitted
[    6.083046] Call Trace:
[    6.083836]  <TASK>
[    6.084239]  ? __warn+0x7d/0xd0
[    6.084391]  ? __list_del_entry_valid+0x9a/0xd0
[    6.084514]  ? report_bug+0xe6/0x170
[    6.084622]  ? console_unlock+0x148/0x1d0
[    6.084823]  ? handle_bug+0x41/0x70
[    6.084936]  ? exc_invalid_op+0x13/0x60
[    6.085041]  ? asm_exc_invalid_op+0x16/0x20
[    6.085195]  ? __list_del_entry_valid+0x9a/0xd0
[    6.085331]  nf_tables_deactivate_set+0x7f/0x110
[    6.085511]  __nf_tables_abort+0x1f2/0xad0
```
With kernel which `CONFIG_DEBUG_LIST` disabled, we can observe that GP fault is triggered on the list deletion routine, and further exploit is hindered by a kernel panic.
- General protection fault via double list unlink
```
[    5.581627] general protection fault, probably for non-canonical address 0xdead000000000122: 0000 [#1] PREEMPT SMP PTI
[    5.582058] CPU: 0 PID: 144 Comm: poc Not tainted 6.1.34 #5
[    5.582325] Hardware name: QEMU Standard PC (i440FX + PIIX, 1996), BIOS 1.15.0-1 04/01/2014
[    5.582665] RIP: 0010:nf_tables_deactivate_set+0x59/0xc0
// omitted
[    5.585499] Call Trace:
[    5.586200]  <TASK>
[    5.586561]  ? __die_body.cold+0x1a/0x1f
[    5.586749]  ? die_addr+0x39/0x60
[    5.586854]  ? exc_general_protection+0x1a7/0x440
[    5.587004]  ? asm_exc_general_protection+0x22/0x30
[    5.587156]  ? nf_tables_deactivate_set+0x59/0xc0
[    5.587300]  ? nft_lookup_destroy+0x10/0x10
[    5.587410]  nft_rule_expr_deactivate+0x4c/0x80
[    5.587607]  __nf_tables_abort+0x33b/0x990
```

This cases gave us inspiring insight:`CONFIG_DEBUG_LIST` is not simply preventing exploits, but might expand a vulnerability's capability in more severe ways. It definitely prevents many exploit primitives such as info leak or AAR/W that can be gained through corrupted list link/unlink, but it might reveal the vulnerability's new and much more severe capability for exploit (AAR/W through UAFed object fields, double free) by avoiding invalid memory access on corrupted list operation and allow continued use of corrupted objects just like our exploit.
Thus, if kernel config like `CONFIG_DEBUG_LIST` or `CONFIG_SLAB_FREELIST_HARDENED` (refer [Abusing `CONFIG_SLAB_FREELIST_HARDENED` mitigation to aid exploitation](#abusing-config_slab_freelist_hardened-mitigation-to-aid-exploitation)) are used for mitigation purposes, it is necessary to enable with other canary-like kernel mitigations also.
- [include/linux/bug.h:CHECK_DATA_CORRUPTION](https://git.kernel.org/pub/scm/linux/kernel/git/stable/linux.git/tree/include/linux/bug.h?h=linux-6.1.y&id=d2869ace6eeb8ea8a6e70e6904524c5a6456d3fb#n81)
```c
#define CHECK_DATA_CORRUPTION(condition, fmt, ...)			 \
	check_data_corruption(({					 \
		bool corruption = unlikely(condition);			 \
		if (corruption) {					 \
			if (IS_ENABLED(CONFIG_BUG_ON_DATA_CORRUPTION)) { \
				pr_err(fmt, ##__VA_ARGS__);		 \
				BUG();					 \
			} else						 \
				WARN(1, fmt, ##__VA_ARGS__);		 \
		}							 \
		corruption;						 \
	}))

#endif	/* _LINUX_BUG_H */
```
For example, `CONFIG_BUG_ON_DATA_CORRUPTION` kernel config trigger kernel BUG to panic kernel on data corruption and prevent further exploit attempt. 
- [kernel/panic.c:check_panic_on_warn()](https://git.kernel.org/pub/scm/linux/kernel/git/stable/linux.git/tree/kernel/panic.c?h=linux-6.1.y&id=d2869ace6eeb8ea8a6e70e6904524c5a6456d3fb#n230)
```c
void check_panic_on_warn(const char *origin)
{
	unsigned int limit;

	if (panic_on_warn)
		panic("%s: panic_on_warn set ...\n", origin);

	limit = READ_ONCE(warn_limit);
	if (atomic_inc_return(&warn_count) >= limit && limit)
		panic("%s: system warned too often (kernel.warn_limit is %d)",
		      origin, limit);
}
```
Samely, `panic_on_warn` flag on the boot parameter makes the kernel panic when it faced a kernel WARNING. 

These mitigations are considered seriously harmful to service availability since they always panic the kernel even with benign kernel bugs. However, adopting these mitigations should be positively considered if the target system has security-oriented purposes (ex. mitigation instance of kernelCTF). The usefulness of premature kernel halt to prevent security-relevant primitives is already mentioned by Google Project Zero's blogpost, ["Exploiting null-dereferences in the Linux kernel"](https://googleprojectzero.blogspot.com/2023/01/exploiting-null-dereferences-in-linux.html).

## Bypassing `CONFIG_SLAB_FREELIST_HARDENED` freelist pointer validation mitigation via unaligned head

- [mm/slub.c:freelist_ptr_decode()](https://github.com/thejh/linux/blob/c02401c87a2d84efb47c4354400a9ad17d7b6436/mm/slub.c#L596)
```c
	slab_base = decoded ? slab_base : 0;
	if (CHECK_DATA_CORRUPTION(
			((unsigned long)decoded & slab->align_mask) != slab_base,
			"bad freeptr (encoded %lx, ptr %px, base %lx, mask %lx",
			ptr.v, decoded, slab_base, slab->align_mask))
		return NULL;
	return decoded;
```

From the `freelist_ptr_decode` function, we note that the above slab freelist pointer validation logic is enabled when `CONFIG_SLAB_FREELIST_HARDENED` is on.

This is a coarse-grained address alignment & range validation approach designed to prevent an attacker from poisoning the freelist to make the allocator return unaligned pointers. The coarseness of alignment validation is already a [well known problem](https://github.com/thejh/linux/blob/c02401c87a2d84efb47c4354400a9ad17d7b6436/mm/slub.c#L621) and thus not dealt with in this report - we instead focus on how this mitigation is ineffective against unaligned free.

Even with the mitigation in effect, an attacker can easily free and reclaim an unaligned chunk by keeping it in freelist head, i.e. free an unaligned chunk, and free nothing more into the same freelist until reclamation. In this case, the unaligned chunk is only set on freelist head (per-cpu freelist `c->freelist`) but the pointer itself is never written inside a free chunk as an encoded freelist pointer. Thus, it never passes through `freelist_ptr_{encode,decode}()` and the mitigation is never reached. Fundamentally, this is because the mitigation is designed to prevent the allocator from returning unaligned pointers through **freelist corruption**, but not through **unaligned free**.

We can verify this in code, in both free and allocation phase. Note how the mitigation is completely oblivious of the unaligned chunk. Only the relevant codes are shown, and others are omitted for brevity.

- [mm/slub.c:do_slab_free()](https://github.com/thejh/linux/blob/c02401c87a2d84efb47c4354400a9ad17d7b6436/mm/slub.c#L4016)

```c
static __always_inline void do_slab_free(struct kmem_cache *s,
				struct slab *slab, void *head, void *tail,
				int cnt, unsigned long addr)
{
	void *tail_obj = tail ? : head;							// [1] tail_obj = head = unaligned chunk to free
	struct kmem_cache_cpu *c;
// ...

	if (likely(slab == c->slab)) {
#ifndef CONFIG_PREEMPT_RT
		void **freelist = READ_ONCE(c->freelist);			// [2] freelist = current per-cpu freelist head

		set_freepointer(s, tail_obj, freelist);				// [3] set tail_obj next freepointer to freelist (via freelist_ptr_encode())

		if (unlikely(!this_cpu_cmpxchg_double(				// [4] set cpu_slab->freelist = unaligned chunk
				s->cpu_slab->freelist, s->cpu_slab->tid,
				freelist, tid,
				head, next_tid(tid)))) {

			note_cmpxchg_failure("slab_free", s, tid);
			goto redo;
		}
#else /* CONFIG_PREEMPT_RT */
// ...
#endif
		stat(s, FREE_FASTPATH);
	} else
		__slab_free(s, slab, head, tail_obj, cnt, addr);

}
```

- [mm/slub.c:slab_alloc_node()](https://github.com/thejh/linux/blob/c02401c87a2d84efb47c4354400a9ad17d7b6436/mm/slub.c#L3694)

```c
static __always_inline void *slab_alloc_node(struct kmem_cache *s, struct list_lru *lru,
		gfp_t gfpflags, int node, unsigned long addr, size_t orig_size)
{
	// ...
	object = c->freelist;														// [1] get object = per-cpu freelist head (unaligned chunk)
	slab = c->slab;

	// ...
	} else {
		void *next_object;
		freeptr_t next_encoded = get_freepointer_safe(s, object, slab);			// [2] next_encoded = encoded next freepointer inside `object`

		if (unlikely(READ_ONCE(c->tid) != tid))
			goto redo;

		next_object = freelist_ptr_decode(s, next_encoded,
				(unsigned long)kasan_reset_tag(object) + s->offset, slab);		// [3] decode next_encoded via freelist_ptr_decode()
																				//     `next_object` is properly aligned, so no problem

		// ...
		if (unlikely(!this_cpu_cmpxchg_double(									// [4] set per-cpu freelist head to `next_object`
				s->cpu_slab->freelist, s->cpu_slab->tid,
				object, tid,
				next_object, next_tid(tid)))) {

			note_cmpxchg_failure("slab_alloc", s, tid);
			goto redo;
		}
		prefetch_freepointer(s, next_object);
		stat(s, ALLOC_FASTPATH);
	}

	// ...
	return object;																// [5] return the unaligned chunk `object`, bypassing mitigation
}
```

In the exploit for `lts-6.1.31`, we pivot double free into unaligned free by overwriting `next` field `struct msg_msg` to an unaligned slub chunk address (`kmalloc-cg-1k_chunk_addr` - `sizeof(struct msg_msg)`), which is freed with the corresponding `struct msg_msg` by `msgrcv` syscall. This unaligned free gives full control over `kmalloc-cg-1k` chunks if we reclaim unaligned chunks with `struct msg_msg`(refer [6.5 KASLR leak for LTS 6.1.31 Instance section of exploit.md](./exploit.md#65-kaslr-leak) for more details). In `mitigation-6.1`, we additionally employ this bypass technique to enable unaligned free, allowing us to use the exact same exploitation steps as that of `lts-6.1.31`.

## Abusing `CONFIG_SLAB_FREELIST_HARDENED` mitigation to aid exploitation

Recall again:
- `CHECK_DATA_CORRUPTION()` is used to throw warnings on debug checks ([`CONFIG_DEBUG_LIST` leading to expand vulnerability's capability](#config_debug_list-leading-to-expand-vulnerabilitys-capability))
- Freelist pointer is validated in `freelist_ptr_decode()` via `CHECK_DATA_CORRUPTION()` ([Bypassing `CONFIG_SLAB_FREELIST_HARDENED` freelist pointer validation mitigation via unaligned head](#bypassing-config_slab_freelist_hardened-freelist-pointer-validation-mitigation-via-unaligned-head))

If the freelist pointer validation mitigation detects an invalid freelist pointer, it simply returns a NULL pointer from `freelist_ptr_decode()` with a kernel warning. **This seemingly benign approach of returning a NULL pointer without panicking the kernel critically undermines the security benefits implicitly provided by `CONFIG_SLAB_FREELIST_HARDENED`** as it allows attackers to simply ignore unintended freelist corruption without crashing the kernel.

In many cases, especially in the initial steps of exploitation, the attacker does not have enough infoleak (kernel heap address + slab `random` value) or precise control over encoded freelist pointer field for use-after-free or double free bugs. In such cases the victim freelist is permanently corrupted - the attacker must exert precise control over kernel heap allocations such that the corrupted chunk is never touched forever. This may prove difficult due to several reasons, with some inexhaustive list of examples:

1. Triggering the vulnerability may result in unavoidable allocation patterns that fetches the corrupted pointer from freelist. (`exploit` case)
2. Noise from other processes or inherent non-determinism of the kernel may interfere with the allocation pattern of the exploit. (`failed exploit` case)
3. Even after a successful exploitation the attacker must either fix up the freelist or use various other mechanisms to permanently freeze away the corrupted chunk from being allocated, or else face an unstable kernel that might crash in any time. (`post-exploit` case)

All the above causes the corrupted pointer to be fetched from freelist, where any immediately following memory operations upon it will trigger a general protection fault panicking the kernel.

When the mitigation is in effect, **all the aformentioned cases do not panic the kernel** as the mitigation returns NULL for the invalid pointer, simply ignoring the invalid pointer and resetting the freelist. The mitigation does mitigate attacker-controlled freelist corruption to some degree, but also regresses on uncontrolled freelist corruption by "mitigating" kernel panic which is the implicit but profound security benefits provided by `CONFIG_SLAB_FREELIST_HARDENED` - mitigation is aiding exploitation, not hindering it.

An attacker may use this to:
- Stabilize exploitation and enable allocation patterns that would have been impossible or difficult to achieve with corrupted freelist (`exploit` case)
  - ex: Re-enable using a slab which has its freelist corrupted
- Avoid crashing on failed exploitation attempts due to unexpected allocation patterns, allowing continuous exploit attempts until success (`failed exploit` case)
- Enjoy a stable kernel after exploitation as the corrupted freelist will automatically fix itself on allocation (`post-exploit` case)

For comparison, below is a kernel panic log in `lts-6.1.31` **without** the freelist pointer validation mitigation in effect as an example. (Intentional) failure to handle the corrupted freelist after a succesful exploit leads to a general protection fault while spawning a new `sh` binary, panicking the kernel (i.e. `post-exploit` case).

```
[    9.209136] general protection fault, probably for non-canonical address 0x5bcc265b074e761f: 0000 [#1] PREEMPT SMP PTI
[    9.209913] CPU: 0 PID: 149 Comm: sh Tainted: G        W          6.1.31+ #1
[    9.210307] Hardware name: QEMU Standard PC (i440FX + PIIX, 1996), BIOS 1.15.0-1 04/01/2014
[    9.210772] RIP: 0010:__kmem_cache_alloc_node+0x2fd/0x450
// omitted
[    9.215387] Call Trace:
[    9.215560]  <TASK>
[    9.215724]  ? __die_body.cold+0x1a/0x1f
[    9.215999]  ? die_addr+0x38/0x60
[    9.216242]  ? exc_general_protection+0x1ae/0x450
[    9.216551]  ? __handle_mm_fault+0xb8a/0x10d0
[    9.216887]  ? asm_exc_general_protection+0x22/0x30
[    9.217217]  ? security_prepare_creds+0xd1/0xf0
[    9.217526]  ? __kmem_cache_alloc_node+0x2fd/0x450
[    9.217877]  ? __kmem_cache_alloc_node+0x38d/0x450
[    9.218188]  ? security_prepare_creds+0xd1/0xf0
[    9.218504]  ? security_prepare_creds+0xd1/0xf0
[    9.218794]  __kmalloc+0x45/0x150
[    9.219031]  security_prepare_creds+0xd1/0xf0
[    9.219315]  prepare_creds+0x197/0x2b0
[    9.219546]  prepare_exec_creds+0xb/0x50
[    9.219792]  bprm_execve+0x57/0x650
[    9.220061]  do_execveat_common.isra.0+0x1ad/0x220
```

Below is a kernel panic log in `mitigation-6.1` **with** the mitigation in effect as an example. Due to an `... -> A -> B -> A` pattern double free, freelist is now a circular linked list of `A -> B -> A -> ...`. The exploit then allocates three `msg_msg` from the same slab:
1. First allocation returns `A` but corrupts encoded freelist pointer pointing to `B`
2. Second allocation returns `B`
3. Third allocation returns `A` again but mitigation detects corrupted freepointer
   - Only a kernel warning is emitted and freelist is reset back to NULL (empty)

Our exploit is free to continue on with the overlapping chunks, with an additional benefit that the corrupted freelist is reset to a safe state (i.e. `exploit` case).

```
[    5.384563] bad freeptr (encoded 0, ptr 6bab38c2708e598c, base fffffe8602dca000, mask ffffffffffffe1ff
[    5.384608] WARNING: CPU: 0 PID: 145 at mm/slub.c:660 __kmem_cache_alloc_node+0x3ee/0x420
[    5.385029] Modules linked in:
[    5.385195] CPU: 0 PID: 145 Comm: poc Tainted: G        W          6.1.0+ #1
[    5.385502] Hardware name: QEMU Standard PC (i440FX + PIIX, 1996), BIOS 1.15.0-1 04/01/2014
[    5.385713] RIP: 0010:__kmem_cache_alloc_node+0x3ee/0x420
// omitted
[    5.388000] Call Trace:
[    5.388072]  <TASK>
[    5.388151]  ? load_msg+0x35/0x1c0
[    5.388315]  ? load_msg+0x35/0x1c0
[    5.388416]  __kmalloc+0x45/0x150
[    5.388522]  load_msg+0x35/0x1c0
[    5.388621]  do_msgsnd+0x8e/0x590
```

This is not only true for our exploit, but also for other mitigation submission. [exp47](https://github.com/google/security-research/pull/31) uses CVE-2023-0461 to achieve use-after-free on `tls_context`. To pivot this into a use-after-free on `fqdir`, the exploit runs as follows:
1. Trigger the vulnerability to create two references to the same `tls_context` ([src](https://github.com/0xdevil/security-research/blob/c96963f95e1e5cdc8c137c7837afabb5c76142e0/pocs/linux/kernelctf/CVE-2023-0461_mitigation/exploit/mitigation-6.1/exploit.c#L771))
2. Free one of the `tls_context` references into `kmalloc-512` by closing a socket ([src](https://github.com/0xdevil/security-research/blob/c96963f95e1e5cdc8c137c7837afabb5c76142e0/pocs/linux/kernelctf/CVE-2023-0461_mitigation/exploit/mitigation-6.1/exploit.c#L778))
3. Populate 4 chunks into `kmalloc-512` ([src](https://github.com/0xdevil/security-research/blob/c96963f95e1e5cdc8c137c7837afabb5c76142e0/pocs/linux/kernelctf/CVE-2023-0461_mitigation/exploit/mitigation-6.1/exploit.c#L787))
4. Free the other `tls_context` reference by closing a socket, RCU-freed via [`kfree_rcu()`](https://elixir.bootlin.com/linux/v6.1/source/net/tls/tls_main.c#L286) ([src](https://github.com/0xdevil/security-research/blob/c96963f95e1e5cdc8c137c7837afabb5c76142e0/pocs/linux/kernelctf/CVE-2023-0461_mitigation/exploit/mitigation-6.1/exploit.c#L793))
5. Attempt to reclaim `tls_context` freed at Step #2 as `fqdir` via `unshare(CLONE_NEWNET)` ([src](https://github.com/0xdevil/security-research/blob/c96963f95e1e5cdc8c137c7837afabb5c76142e0/pocs/linux/kernelctf/CVE-2023-0461_mitigation/exploit/mitigation-6.1/exploit.c#L795))
6. RCU-free on dangling `tls_context` at Step #4, now pointing to reclaimed `fqdir`, is `kfree()`d after a grace period, successfully pivoting to `fdqir` UAF

Step #4 corrupts the encoded freelist pointer of freed `tls_context` chunk in `kmalloc-512` cache (freed at Step #2) since [`tls_ctx_free()`](https://elixir.bootlin.com/linux/v6.1/source/net/tls/tls_main.c#L276) calls [`memzero_explicit()`](https://elixir.bootlin.com/linux/v6.1/source/net/tls/tls_main.c#L282) on the encoded freelist pointer offset, zeroing it out. In Step #3 4 chunks are populated into `kmalloc-512` freelist which is reclaimed at Step #5. This is followed by 1 `fqdir` allocation which reclaims `tls_context` but fetches the corrupted pointer into freelist head. However, 2 more `fqdir` allocations are to be served, not to mention that these allocations are done once more as `N_SPRAY_1` is defined as 2 ([src](https://github.com/0xdevil/security-research/blob/c96963f95e1e5cdc8c137c7837afabb5c76142e0/pocs/linux/kernelctf/CVE-2023-0461_mitigation/exploit/mitigation-6.1/exploit.c#L720)).

Without the mitigation, the additional allocations would have resulted in a kernel panic due to general protection fault while trying to fetch the corrupted freelist pointer as a free chunk. However, the mitigation silently "mitigates" this and resets `kmalloc-512` freelist to NULL with just a kernel warning log, which allows subsequent `kmalloc-512` allocations to succeed normally (i.e. yet another `exploit` case). This can also be verified by [exploitation video uploaded on Twitter](https://twitter.com/cor_ctf/status/1671601528594116609).

To the best of our knowledge, this is the first publication posing the regression issue of freelist pointer validation mitigation implemented in kCTF/kernelCTF mitigation instances. As also mentioned in [`CONFIG_DEBUG_LIST` leading to expand vulnerability's capability section](#config_debug_list-leading-to-expand-vulnerabilitys-capability), we strongly suggest enabling canary-like flags (`panic_on_warn`, `CONFIG_BUG_ON_DATA_CORRUPTION`) whenever debug options are enabled, or at least for mitigation instances.

## Usage of `struct nft_set` as a novel target object

While writing exploits for CVE-2023-3390, especially when targeting `cos-105-17412-101.17` instance, we found that `struct nft_set` is an extremely useful object for the kernel exploit, no matter if it is the vulnerable object of the vulnerability. We actively utilized the `struct nft_set` object after we get stable double free, even though double free gives us the capability to overlap arbitrary types of objects, not only `struct nft_set`. 

Generally, we have three reasons to use `struct nft_set` for a target object.
First, the `struct nft_set` contains useful fields in itself for exploit.

- [include/net/netfilter/nf_tables.h:struct nft_set](https://git.kernel.org/pub/scm/linux/kernel/git/stable/linux.git/tree/include/net/netfilter/nf_tables.h?h=linux-5.15.y&id=f48aeeaaa64c628519273f6007a745cf55b68d95#n504)
```c
struct nft_set {
	struct list_head		list;
	struct list_head		bindings;
	struct nft_table		*table;
	possible_net_t			net;
	char				*name;
	u64				handle;
	u32				ktype;
	u32				dtype;
	u32				objtype;
	u32				size;
	u8				field_len[NFT_REG32_COUNT];
	u8				field_count;
	u32				use;
	atomic_t			nelems;
	u32				ndeact;
	u64				timeout;
	u32				gc_int;
	u16				policy;
	u16				udlen;
	unsigned char			*udata;
	struct list_head		pending_update;
	/* runtime data below here */
	const struct nft_set_ops	*ops ____cacheline_aligned;
	u16				flags:14,
					genmask:2;
	u8				klen;
	u8				dlen;
	u8				num_exprs;
	struct nft_expr			*exprs[NFT_SET_EXPR_MAX];
	struct list_head		catchall_list;
	unsigned char			data[]
		__attribute__((aligned(__alignof__(u64))));
};
```
Above is the struct layout of the `struct nft_set` object. We can identify many useful fields that exist in `struct nft_set`. List entries like `list`, `bindings`, `pending_updates`, and `catchall_list` contain current or adjacent `struct nft_set` object's slab chunk addresses. `ops` fields point to vtables of `struct nft_set`'s type, which can be used for KASLR bypass when its value is leaked and get RIP control when it is overwritten. Thus, read/write toward the `struct nft_set` objects (through slab OOB or UAF) almost guarantees the existence of a working exploit strategy. 

Second, the size of `struct nft_set` is controllable. Controllable object size is a powerful characteristic because it makes the object placed into desired slab caches. Below is the routine of `struct nft_set`'s allocation and initialization.
- [net/netfilter/nf_tables_api.c:nf_tables_newset()](https://git.kernel.org/pub/scm/linux/kernel/git/stable/linux.git/tree/net/netfilter/nf_tables_api.c?h=linux-6.1.y&id=d2869ace6eeb8ea8a6e70e6904524c5a6456d3fb#n4494)
```c
// ...
	udlen = 0;
	if (nla[NFTA_SET_USERDATA])
		udlen = nla_len(nla[NFTA_SET_USERDATA]);

	size = 0;
	if (ops->privsize != NULL)
		size = ops->privsize(nla, &desc);
	alloc_size = sizeof(*set) + size + udlen;
	if (alloc_size < size || alloc_size > INT_MAX)
		return -ENOMEM;
	set = kvzalloc(alloc_size, GFP_KERNEL_ACCOUNT);
// ...
```
As the above code path shows, the chunk size of `struct nft_set` is determined by the sum of three size components: the `sizeof(struct nft_set)`, type-specific private size, and user-supplied data length. the `sizeof(struct nft_set)` is fixed (around 256 bytes), and the private size is controlled by types of `struct nft_set` and `NFTA_SET_KEY_LEN`, which is determined by set flags and parameters of `NFT_MSG_NEWSET` requests and current system environments.
- [net/netfilter/nf_tables_api.c:nft_set_types](https://git.kernel.org/pub/scm/linux/kernel/git/stable/linux.git/tree/net/netfilter/nf_tables_api.c?h=linux-6.1.y&id=d2869ace6eeb8ea8a6e70e6904524c5a6456d3fb#n3774)
```c
static const struct nft_set_type *nft_set_types[] = {
	&nft_set_hash_fast_type,
	&nft_set_hash_type,
	&nft_set_rhash_type,
	&nft_set_bitmap_type,
	&nft_set_rbtree_type,
#if defined(CONFIG_X86_64) && !defined(CONFIG_UML)
	&nft_set_pipapo_avx2_type,
#endif
	&nft_set_pipapo_type,
};
```
- [include/uapi/linux/netfilter/nf_tables.h:nft_set_flags](https://git.kernel.org/pub/scm/linux/kernel/git/stable/linux.git/tree/include/uapi/linux/netfilter/nf_tables.h?h=linux-6.1.y&id=d2869ace6eeb8ea8a6e70e6904524c5a6456d3fb#n303)
```c
enum nft_set_flags {
	NFT_SET_ANONYMOUS		= 0x1,
	NFT_SET_CONSTANT		= 0x2,
	NFT_SET_INTERVAL		= 0x4,
	NFT_SET_MAP				= 0x8,
	NFT_SET_TIMEOUT			= 0x10,
	NFT_SET_EVAL			= 0x20,
	NFT_SET_OBJECT			= 0x40,
	NFT_SET_CONCAT			= 0x80,
	NFT_SET_EXPR			= 0x100,
};
```
The user-supplied data is freely passed with arbitrary length through `NFTA_SET_USERDATA` unless it does not exceed the max packet length limit.
We experimentally proved that we can get the allocation size of `struct nft_set` from around 512 bytes to over 0x40000 bytes (vmalloc region) with these components, although we practically used it for `(dyn-)kmalloc-cg-512` slab and `kmalloc-1k` slab only by controlled user-supplied data.

Finally, `struct nft_set` is almost freely sprayable and can be utilized for both `kmalloc` and `kmalloc-cg` slab caches, depending on which kernel version exploits targets since the Netfilter nf_tables objects were moved from normal `kmalloc` cache to `kmalloc-cg` caches after v5.19 with commit [33758c891479ea1c736abfee64b5225925875557](https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/commit/?id=33758c891479ea1c736abfee64b5225925875557). 

To sum up, with the characteristic of `struct nft_set` listed above, it can be a novel object for kernel exploits generally. It has valuable fields for various info leaks, and their size can be adjusted in various ways. There were numerous kernel vulnerabilities and exploits, whether netfilter or non-netfilter vulnerabilities. However, to the best of our knowledge, this is the first exploit that employs `struct nft_set` as target objects for a kernel slab leak, KASLR leak, and PC control.
