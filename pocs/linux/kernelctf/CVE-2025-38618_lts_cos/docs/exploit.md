# CVE-2025-38618

## Overview
The vulnerability allows us to bind a vsock to the illegal port `VMADDR_PORT_ANY`. Opening a connection to this socket can create a peer vsock which is vulnerable to a refcount underflow, but succesfully creating the connection involves a race condition. Most of exploit consists of winning this race. Once the vulnerable vsock is created, it is freed and a cross-cache spray replaces it with a `msgmsg_seg` containing a ROP chain which we execute by closing the vsock.

## Binding to VMADDR_PORT_ANY

We want to increment the static variable `port` in `__vsock_bind_connectible()` until it reaches `VMADDR_PORT_ANY`. We first obtain the current value of `port` by using `getsockname()` on a newly autobound vsock. If this the first time a vsock has been autobound, `port` will have been randomly initialized.

There is a time limit on how long the exploit can run, and chances are we cannot set `port` to `VMADDR_PORT_ANY` within it. Therefore we only proceed if the number of increments needed is less than `MAX_PORTS` and retry on a freshly booted instance otherwise. Eventually `port` will be initialized to a high enough value for the vulnerability to be triggered. 

We set `port` to `VMADDR_PORT_ANY` using the `inc_port()` helper function. The process is sped up by calling `inc_port()` from multiple threads. We then create a new vsock and bind it to `VMADDR_PORT_ANY`.

## Creating vulnerable socket

As explained in [vulnerability.md](./vulnerability.md), we need to call `bind()` and `listen()` on the buggy socket in the window between when it is retrieved from the bound list and when its state is checked in the `virtio_transport_recv_pkt()` call scheduled by `connect()`. Binding the socket undoes the vulnerability, so we only have one try at winning the race. 

The socket is locked before its state is checked, giving us an opportunity to lengthen the race window by holding its lock from another thread. Since `bind()` and `listen()` also take this lock, we will have to hope they run first once it is released. To increase the chances of this happening, we perform these syscalls from multiple threads.

We first take the lock in `setsockopt()`, which will attempt to read from file-mapped memory that is in the process of being deallocated by `fallocate()` while holding the lock. This read faults and is only allowed to proceed once the entire file has been deallocated, giving us plenty of time to call `connect()` and `bind()`.

The race involves four types of thread:

- `falloc_pthread` calls `fallocate()` on a temporary file to deallocate it.
- `setsockopt_pthread` calls `setsockopt()` to take the buggy socket's lock.
- `connect_pthread` calls `connect()` to schedule `virtio_transport_recv_pkt()` with `VMADDR_PORT_ANY` as the target.
- The `bind_pthreads` call `bind()` and `listen()` on the buggy socket.

Here is the order in which events should happen:

1. `falloc_pthread` begins deallocating the temporary file on CPU 0. All other threads run on CPU 1 and immediately `usleep()` to allow the deallocation to start.
2. `setsockopt_pthread` wakes up first and calls `setsockopt()` with `optval` pointing to file-mapped memory that is currently being deallocated. When `setsockopt()` tries to read `optval` it will fault and go to sleep while holding the buggy socket's lock.
3. `connect_pthread` and the `bind_pthreads` wake up next. The `bind_pthread`s will wait on the buggy socket's lock in the `bind()` call. Meanwhile `connect_pthread` calls `connect()` and exits having scheduled `virtio_transport_rcv_pkt()`. This function will run on the vsock loopback workqueue and find the buggy socket on the list of bound sockets then wait on its lock.
4. `falloc_pthread` finishes deallocating the file.
5. `setsockopt_thread` resumes execution and exits, releasing the lock and waking up the waiting threads.
6. Hopefully at least one of the `bind_pthreads` runs, binding the socket and setting its state to `TCP_LISTEN`. If the work queue thread runs before any of the `bind_pthreads`, the exploit fails.
7. `virtio_transport_rcv_pkt()` resumes execution on the work queue and sees the buggy socket in state `TCP_LISTEN`. It calls `virtio_transport_recv_listen()` and creates the vulnerable peer vsock.

Once all threads have finished executing, we get the vulnerable vsock's fd by calling `accept()` on the buggy socket. If `accept()` times out, the race was lost and we cannot proceed. 

## ROP

After obtaining the vulnerable vsock we close the connection by calling `shutdown()` on its peer, bringing the vulnerable vsock's refcount down to 1. Now the refcount decrement in `bind()` will free the vulnerable `vsock_sock`, leaving a dangling pointer from the corresponding `socket`.

We will replace the `vsock_sock` with a `msgmsg_seg` containing a ROP chain which we can execute by closing the vulnerable vsock. This will cause `vsock_release()` to call `sk->sk_prot->close(sk, 0)` on an attacker-controlled `sk`. We make `sk_prot` overlap `drr_qdisc_ops` in the kernel image so that `sk_prot->close` is set to `qdisc_peek_dequeued()`:

```
static inline struct sk_buff *qdisc_peek_dequeued(struct Qdisc *sch)
{
	struct sk_buff *skb = skb_peek(&sch->gso_skb);

	if (!skb) {
		skb = sch->dequeue(sch);
    /* ... */
```

We only need to ensure that `sch->gso_skb` is `NULL` and `sch->dequeue` will be called on `sch`. Since `&sch->gso_skb` happens to be stored in `rbp`, we can set `sch->dequeue` to the stack pivot

```
mov rsp, rbp ; pop rbp ; ret
```
and then execute a ROP chain at `&sch->gso_skb + 8` which overwrites `core_pattern` and teleforks.

Thus we need to prepare the `msgmsg_seg` with:

- 8 bytes for `sk_prot` at `offsetof(struct sock, sk_prot)`.
- 8 bytes for `dequeue` at `offsetof(struct Qdisc, dequeue)`.
- 72 bytes for the ROP chain at `offsetof(struct Qdisc, gso_skb)`

A cross-cache spray is necessary to replace the vulnerable `vsock_sock` in cache `AF_VSOCK` with a `msgmsg_seg` in `kmalloc-cg-1k`. Since the size of each slot in `AF_VSOCK` is 1280 bytes, there are four possible locations for the dangling pointer inside of the 1024-byte `msgmsg_seg`.  Fortunately there is enough space to place a copy  of the necessary pointers and ROP chain at all four offsets simultaneously.

Only a kernel base leak is required, which we can request during reproduction on github by forgoing the reliability bonus (the steps required to trigger the bug make that unobtainable in any case). Against the live instance a prefetch side-channel was used.