# Exploit detail about CVE-2024-41010

## Primitive
When UAF triggered at `uaf_chunk`, we can modify `uaf_chunk[0][0x20, 0x28)` to `start_poll_synchronize_rcu()` return value.

## Leak kBase and Controlable kHeap
### Trigger Cross-Cache (kmalloc-2k -> per-cpu buddy)
From [Trigger Vulnerability](#trigger-vulnerability), we know that UAF can be triggered at `kmalloc-2k`. There arn't exist appropriate controlable structure in `kmalloc-2k`, so we need to trigger cross-cache to control `uaf` content.

In our exploit, we spray `kmalloc-2k` by using `struct super_block`. We can `mount` and `unmount` tmpfs, and it alloc / free only 1 `kmalloc-2k` chunk for each call.
```c++
struct super_block {
	struct list_head           s_list;               /*     0    16 */
	dev_t                      s_dev;                /*    16     4 */
	unsigned char              s_blocksize_bits;     /*    20     1 */

	/* XXX 3 bytes hole, try to pack */

	long unsigned int          s_blocksize;          /*    24     8 */
	loff_t                     s_maxbytes;           /*    32     8 */
	struct file_system_type *  s_type;               /*    40     8 */
	const struct super_operations  * s_op;           /*    48     8 */
	const struct dquot_operations  * dq_op;          /*    56     8 */
	/* --- cacheline 1 boundary (64 bytes) --- */
	const struct quotactl_ops  * s_qcop;             /*    64     8 */
	const struct export_operations  * s_export_op;   /*    72     8 */
	long unsigned int          s_flags;              /*    80     8 */
	long unsigned int          s_iflags;             /*    88     8 */
	long unsigned int          s_magic;              /*    96     8 */
	struct dentry *            s_root;               /*   104     8 */
	struct rw_semaphore        s_umount;             /*   112    40 */

	[...]

	/* XXX 4 bytes hole, try to pack */

	struct list_head           s_inodes_wb;          /*  1376    16 */

	/* size: 1408, cachelines: 22, members: 58 */
	/* sum members: 1321, holes: 8, sum holes: 71 */
	/* padding: 16 */
	/* forced alignments: 3, forced holes: 1, sum forced holes: 44 */
} __attribute__((__aligned__(64)));
```

### Trigger Cross-Cache (per-cpu buddy -> kmalloc-cg-2k)
---
When we use Cross-Cache, for stability, we must cross-cache same order with `kmalloc-2k`. Therefore, we need to trigger cross-cache from `per-cpu buddy` to `kmalloc-cg-2k`.

In our exploit, we use `struct simple_xattr` as cross-cache's uaf victim. The shape of `struct simple_xattr` is as follows:
```c++
struct simple_xattr {
	struct rb_node             rb_node __attribute__((__aligned__(8))); /*     0    24 */
	char *                     name;                 /*    24     8 */
	size_t                     size;                 /*    32     8 */
	char                       value[];              /*    40     0 */

	/* size: 40, cachelines: 1, members: 4 */
	/* forced alignments: 1 */
	/* last cacheline: 40 bytes */
} __attribute__((__aligned__(8)));
```

So, by using Cross-Cache, we overlap `struct simple_xattr` with `uaf_chunk`

### Modify `struct simple_xattr`'s `size`
At [Trigger Vulnerability](#trigger-vulnerability), we know that can modify `uaf_chunk[0][0x20, 0x28)` to `start_poll_synchronize_rcu()` return value. Therefore, by overlapping, we can control `struct simple_xattr`'s `size` as `start_poll_synchronize_rcu()` return value.

The import things is that the first value of `struct simple_xattr` is `rb_node`. The `struct rb_node` is as follows:
```c++
struct rb_node {
	long unsigned int          __rb_parent_color;    /*     0     8 */
	struct rb_node *           rb_right;             /*     8     8 */
	struct rb_node *           rb_left;              /*    16     8 */

	/* size: 24, cachelines: 1, members: 3 */
	/* last cacheline: 24 bytes */
} __attribute__((__aligned__(8)));
```

Therefore, `uaf_chunk[0]` is `(struct rb_node *)uaf_chunk -> __rb_parent_color`.  `__rb_parent_color` is constructed as follows:
```c++
#define RB_RED          0
#define RB_BLACK        1

__rb_parent_color = (unsigned long)parent | color;
```

Therefore, we must set the `struct simple_xattr`'s parent color as `RB_RED` which overlapped with `uaf_chunk`. By doing this, we can control `struct simple_xattr`'s `size` as `start_poll_synchronize_rcu()` return value.

So, We needs to construct structure's allocation as follows:
```
                             ------------------
                       â•­--> |      parent      | 
                       |     ------------------
                       |                         
              ------------------           ------------------
uaf_chunk -> |      rchild      |         |      lchild      |
              ------------------           ------------------
```

When we modify `uaf_chunk[0][0x20, 0x28)` to `start_poll_synchronize_rcu()` return value, we can change `parent->size` as `start_poll_synchronize_rcu()` return value.

From [Cross-Cache (per-cpu buddy -> kmalloc-cg-2k)](#cross-cache-per-cpu-buddy---kmalloc-cg-2k), the `lchild`'s size must be use `kmalloc-cg-2k` and we can freely choose `parent`'s slab cache on `kmalloc-cg-64 ~ kmalloc-cg-4k` and `8k, 16k, 32k` managed by buddy allocator.

### Bypass CONFIG_HARDENED_USERCOPY
The kenrel have config CONFIG_HARDENED_USERCOPY, which makes impossible to copy out of bounded slab kernel memory triggered by using `copy_to_user()`. However, the `simple_xattr_alloc()` alloc kernel memory and copy to there first([1]). After that, `copy_to_user()` executed([2]). For this reason, if we can overwrite `struct simple_xattr` size to any size below `XATTR_SIZE_MAX (0x10000)`, leaking out of bound slab kernel memory is available.
```cpp
ssize_t
do_getxattr(struct mnt_idmap *idmap, struct dentry *d,
	struct xattr_ctx *ctx)
{
	ssize_t error;
	char *kname = ctx->kname->name;

	if (ctx->size) {
		if (ctx->size > XATTR_SIZE_MAX)
			ctx->size = XATTR_SIZE_MAX;
		ctx->kvalue = kvzalloc(ctx->size, GFP_KERNEL); // [1]
		if (!ctx->kvalue)
			return -ENOMEM;
	}

	if (is_posix_acl_xattr(ctx->kname->name))
		error = do_get_acl(idmap, d, kname, ctx->kvalue, ctx->size);
	else
		error = vfs_getxattr(idmap, d, kname, ctx->kvalue, ctx->size);
	if (error > 0) {
		if (ctx->size && copy_to_user(ctx->value, ctx->kvalue, error)) // [2]
			error = -EFAULT;
	} else if (error == -ERANGE && ctx->size >= XATTR_SIZE_MAX) {
		/* The file system tried to returned a value bigger
		   than XATTR_SIZE_MAX bytes. Not possible. */
		error = -E2BIG;
	}

	return error;
}
```

### OOB Read at `parent` Chunk
#### Leak Controlable kHeap
From [bypass CONFIG_HARDENED_USERCOPY](#bypass-config_hardened_usercopy), we can read out-of-bounds information of slab cache eventhought `CONFIG_HARDENED_USERCOPY` is enabled.

WLOG, consider that `parent` is `kmalloc-cg-256` and we spray many `struct simple_xattr`. Then, we can read out-of-bounds information of `kmalloc-cg-256` slab cache. It is shown as follows:
```
 --------------------------------------
|            parent.rb_node            |
|            parent.name               |
|            parent.size               |
|            parent.value              |
 --------------------------------------
|            xattr<1>.rb_node          |
|            xattr<1>.name             |
|            xattr<1>.size             |
|            xattr<1>.value            |
 --------------------------------------
```

Therefore, by using OOB read, we can leak `kHeap` address by read `xattr1.rb_node`'s address. The important thing is we can give value at `xattr<*>.value`. Therefore, by reading `xattr<*>.value` and `xattr<*>.rb_node`'s value, we can identify `xattr<*>`'s `lchild`, `rchild`, and `parent`'s kernel address clearly. It dramatically increase the stability of the exploit.

We can free known kernel address whenever we want. 

#### Leak kbase
When we read OOB data of `parent`, we can read `kbase` address which remained as a uninitialized value.

## Obtain Arbitrary Address Free Primitive
### Cross-Cache (kmalloc-2k -> kmalloc-32k)
By proceeding process at [Trigger Cross-Cache (kmalloc-2k -> per-cpu buddy)](#trigger-cross-cache-kmalloc-2k---per-cpu-buddy), and reallocate to `kmalloc-32k`(not slab cache, but buddy. For convenience, we will call like that). `kmalloc-2k` cache and `kmalloc-32k` cache is same order(order 3), so we can reallocate to `kmalloc-32k` cache.

Then the status of memory is shown as follows:
```
 -------- kmalloc-32k ---------
|             ...              |
|          uaf_chunk           |
|             ...              |
 ------------------------------
```

uaf_chunk was in `kmalloc-2k` slab cache, so if cross-cached, it will be in somewhere in `kmalloc-32k` slab cache. Then, If we overlap `uaf_chunk` with `struct simple_xattr` which is the size of `kmalloc-32k`, we can modify `uaf_chunk[0]`'s address freely. The reason is that `uaf_chunk[0]` is will be placed on somewhere in `struct simple_xattr.value[]`. 

### Modify `struct simple_xattr.rb_node->parent.rb_right`
From [Cross-Cache (kmalloc-2k -> kmalloc-32k)](#cross-cache-kmalloc-2k---kmalloc-32k), we can modify `uaf_chunk[0]`'s address freely and from [OOB Read at `parent` Chunk](#oob-read-at-parent-chunk), we know the `xattr<*>`'s `lchild`, `rchild`, and `parent`'s kernel address.

Let assume that `uaf_chunk[0]`'s address is `xattr<1>.parent - 0x1f` which set at [Cross-Cache (kmalloc-2k -> kmalloc-32k)](#cross-cache-kmalloc-2k---kmalloc-32k). 

Then the status of memory is shown as follows:
```
 -------- uaf_chunk --------                                                offset
|   xattr<1>.parent - 0x1f  | -----> |              ....               |    -0x1f
|            ...            |        |              ....               |    -0x17
 ---------------------------         |              ....               |    -0xf
                                     |              ....               |    -0x7
                                      ------ xattr<1>.parent + 1 ------ 
                                     |              ....               |    +0x1: xattr<1>.parent + 1
                                     |              ....               | 
                                      ---------------------------------
```

Then, when we trigger [Trigger Vulnerability](#trigger-vulnerability), we can modify `xattr<1>.parent[1, 9)` to `start_poll_synchronize_rcu()` return value. And we know that `start_poll_synchronize_rcu()`'s 7th byte is `0x00`.

After trigger UAF write and observe memory from the perspective of `struct xattr<1>.parent`. It is shown as follows:
```
 ------------- xattr<1>.parent ------------- 
|  start_poll_synchronize_rcu() value  | ?? |
|       xattr<1> address               | 00 |
|                   ...                     |
 -------------------------------------------
```

So, we trigger off-by-one at `xattr<1>.parent`. 

### Trigger (Weak) Arbitrary Address Free
Let `xattr<1>.rb_node.parent->rb_right` is allocated in `kmalloc-cg-192`.

#### Why `kmalloc-cg-192`? It was `kmalloc-cg-256`!
We can make `Let xattr<1>.rb_node.parent->rb_child` by proceeding like below.

1. allocate `kmalloc-cg-256`            : `xattr<1>.parent`
2. allocate `kmalloc-cg-2k`             : `xattr<1>`
3. leak `xattr<1>.parent`'s address     : from [OOB Read at `parent` Chunk](#oob-read-at-parent-chunk)
4. free `xattr<1>`         
5. allocate `kmalloc-cg-192`            : `xattr<1>.rb_node.parent->rb_child`

Then, we change `kmalloc-cg-256` to `kmalloc-cg-192`.

For convinience, let's call `rb_child` by `rchild`.

Then, when kernel tried to access to `xattr<1>`(`kmalloc-cg-192`), it accessed to `xattr<1> & ~(0xff)` because of [Modify `struct simple_xattr.rb_node->parent.rb_right`](#modify-struct-simple_xattrrb_node-parentrb_right). 

The `kmalloc-cg-192` has 4 cases when off-by-one triggered. It is shown as follows:
1. `xattr<1> & 0x3ff` is `0x000`
    - `xattr<1> & ~(0xff)` is same with `xattr<1>`, so nothing changed.
2. `xattr<1> & 0x3ff` is `0x0c0`
    - `xattr<1> & ~(0xff)` is `xattr<1> - 0xc0`, so point other sprayed `&struct simple_xattr.rb_node`.
3. `xattr<1> & 0x3ff` is `0x180`
    - `xattr<1> & ~(0xff)` is `xattr<1> - 0x80`, so point other sprayed `&struct simple_xattr.value[0x18]`.
4. `xattr<1> & 0x3ff` is `0x240`
    - `xattr<1> & ~(0xff)` is `xattr<1> - 0x40`, so point other sprayed `&struct simple_xattr.value[0x58]`.

Therefore, when `xattr<1> & 0x3ff == 0x180` or `xattr<1> & 0x3ff == 0x240`, we can use `simple_xattr.value[]` as fake `struct simple_xattr`.

Therefore, when we set value to `struct simple_xattr.value` as fake chunk, trigger off-by-one, and delete `xattr<1>`, `xattr<1>->name` will be freed. 

Therefore, we can free arbitrary address where we can set `xattr<1>->name`'s prefix as `"security."` (for remove xattr, prefix condition must be satisfied because code find xattr by `xattr<1>->name`).

## Trigger Double Free
Now, we will trigger double-free on `kmalloc-cg-64` slab cache.

### Hmm, how we know `kmalloc-cg-64`'s address?
From [OOB Read at `parent` Chunk](#oob-read-at-parent-chunk), we can read out-of-bounds information of slab cache. Therefore, by setting `rchild` as `kmalloc-cg-256`, and `rchild->name` to `kmalloc-cg-64` chunk, we can leak `kmalloc-cg-64`'s address. Also, we can free `kmalloc-cg-64` anytime we want!

Therefore, when we allocate chunk to satisfy Precondition, we can trigger double free
- Precondition: prefix of `&rchild->name[0x28]` is `"security."`(for satisfy [Trigger (Weak) Arbitrary Address Free](#trigger-weak-arbitrary-address-free)) condition

Let's call each chunk as `double_free_rchild` and `double_free_fake_xattr`.

## Arbitrary Address Write & Arbitrary Address Execute
Before AAW, AAE, abstract our primitive.
- When free `double_free_rchild`, we can free `double_free_chunk`
- When free `double_free_fake_xattr`, we can free `double_free_chunk + 0x28`
- `double_free_chunk` is `kmalloc-64`

as diagram, it is shown as follows:
```
offset   --------- double_free_rchild ---------
+0x00   |                                      |
+0x08   |                                      |
+0x10   |                                      |
+0x18   |                                      |
+0x20   |                                      |
                                                ------- double_free_fake_xattr -------
+0x28   |                                      |                                      |
+0x30   |                                      |                                      |
+0x38   |                                      |                                      |
         -------------------------------------- 									  |
+0x40                                          |                                      |
+0x48                                          |                                      |
+0x50                                          |                                      |
+0x58                                          |                                      |
+0x60                                          |                                      |
                                                --------------------------------------     
```           

### Leak VMEMMAP_BASE
Proceding like below, we can leak `VMEMMAP_BASE`.
1. free `double_free_fake_xattr`
2. allocate `struct pipe_buffer` at freed `double_free_chunk`
3. free `double_free_rchild`
4. allocate `struct msg_msgseg` at freed `double_free_chunk`
5. call `pipe_write()`(it allocate vmemmap address at `struct pipe_buffer->page`)
6. read `struct pipe_buffer->page`'s address by read `struct msg_msg`

each struct are as follows:
```c++
struct msg_msg {
	struct list_head           m_list;               /*     0    16 */
	long int                   m_type;               /*    16     8 */
	size_t                     m_ts;                 /*    24     8 */
	struct msg_msgseg *        next;                 /*    32     8 */
	void *                     security;             /*    40     8 */

	/* size: 48, cachelines: 1, members: 5 */
	/* last cacheline: 48 bytes */
};

struct msg_msgseg {
	struct msg_msgseg *        next;                 /*     0     8 */

	/* size: 8, cachelines: 1, members: 1 */
	/* last cacheline: 8 bytes */
};

struct pipe_buffer {
	struct page *              page;                 /*     0     8 */
	unsigned int               offset;               /*     8     4 */
	unsigned int               len;                  /*    12     4 */
	const struct pipe_buf_operations  * ops;         /*    16     8 */
	unsigned int               flags;                /*    24     4 */

	/* XXX 4 bytes hole, try to pack */

	long unsigned int          private;              /*    32     8 */

	/* size: 40, cachelines: 1, members: 6 */
	/* sum members: 36, holes: 1, sum holes: 4 */
	/* last cacheline: 40 bytes */
};

struct pipe_buf_operations {
	int                        (*confirm)(struct pipe_inode_info *, struct pipe_buffer *); /*     0     8 */
	void                       (*release)(struct pipe_inode_info *, struct pipe_buffer *); /*     8     8 */
	bool                       (*try_steal)(struct pipe_inode_info *, struct pipe_buffer *); /*    16     8 */
	bool                       (*get)(struct pipe_inode_info *, struct pipe_buffer *); /*    24     8 */

	/* size: 32, cachelines: 1, members: 4 */
	/* last cacheline: 32 bytes */
};
```

when step(5), the memory is shown as follows:
```
offset   --------- struct msg_msgseg ---------
+0x00   |                next                  |
+0x08   |        msg_msgseg.data[0, 8)         |
+0x10   |                ...                   |
+0x18   |                                      |
+0x20   |                                      |
                                                --------- struct pipe_buffer ---------
+0x28   |     msg_msgseg.data[0x20, 0x28)      |          pipe_buffer->page           |
+0x30   |                                      |                                      |
+0x38   |                                      |                                      |
         -------------------------------------- 									  |
+0x40                                          |                                      |
+0x48                                          |                                      |
+0x50                                          |                                      |
+0x58                                          |                                      |
+0x60                                          |                                      |
                                                --------------------------------------     
```        

So, we can obtain VMEMMAP_BASE.


### Arbitrary Address Write
Using `struct pipe_buffer`, we can write arbitrary address. From `pipe_write()`, we can write arbitrary address to `pipe_buffer->page`. 
```c++
static ssize_t
pipe_writev(struct file *filp, const struct iovec *_iov,
	    unsigned long nr_segs, loff_t *ppos)
{
	struct inode *inode = filp->f_dentry->d_inode;
	struct pipe_inode_info *info;
	ssize_t ret;
	int do_wakeup;
	struct iovec *iov = (struct iovec *)_iov;
	size_t total_len;
	ssize_t chars;

	[...]

	chars = total_len & (PAGE_SIZE-1); /* size of the last buffer */
	if (info->nrbufs && chars != 0) {
		int lastbuf = (info->curbuf + info->nrbufs - 1) & (PIPE_BUFFERS-1);
		struct pipe_buffer *buf = info->bufs + lastbuf;
		struct pipe_buf_operations *ops = buf->ops;
		int offset = buf->offset + buf->len;
		if (ops->can_merge && offset + chars <= PAGE_SIZE) {
			void *addr = ops->map(filp, info, buf);
			int error = pipe_iov_copy_from_user(offset + addr, iov, chars);
			ops->unmap(info, buf);
			ret = error;
			do_wakeup = 1;
			if (error)
				goto out;
			buf->len += chars;
			total_len -= chars;
			ret = chars;
			if (!total_len)
				goto out;
		}
	}

	for (;;) {
		int bufs;
		if (!PIPE_READERS(*inode)) {
			send_sig(SIGPIPE, current, 0);
			if (!ret) ret = -EPIPE;
			break;
		}
		bufs = info->nrbufs;
		if (bufs < PIPE_BUFFERS) {
			int newbuf = (info->curbuf + bufs) & (PIPE_BUFFERS-1);
			struct pipe_buffer *buf = info->bufs + newbuf;
			struct page *page = info->tmp_page;
			int error;

			if (!page) {
				page = alloc_page(GFP_HIGHUSER);
				if (unlikely(!page)) {
					ret = ret ? : -ENOMEM;
					break;
				}
				info->tmp_page = page;
			}
			/* Always wakeup, even if the copy fails. Otherwise
			 * we lock up (O_NONBLOCK-)readers that sleep due to
			 * syscall merging.
			 * FIXME! Is this really true?
			 */
			do_wakeup = 1;
			chars = PAGE_SIZE;
			if (chars > total_len)
				chars = total_len;

			error = pipe_iov_copy_from_user(kmap(page), iov, chars);

            [...]

out:
	up(PIPE_SEM(*inode));
	if (do_wakeup) {
		wake_up_interruptible(PIPE_WAIT(*inode));
		kill_fasync(PIPE_FASYNC_READERS(*inode), SIGIO, POLL_IN);
	}
	if (ret > 0)
		inode_update_time(inode, 1);	/* mtime and ctime */
	return ret;
}
```

However, the address of `pipe_buffer->page` is on vmemmap_base. Therefore, we should write converted address at `pipe_buffer->page`.

The calculation proceeded as below:
```c++
uint64_t virt_to_page(uint64_t virt, uint64_t kheap_base, uint64_t vmemmap_base)
{
	return (((virt - kheap_base) >> 0xc) << 0x6) + vmemmap_base;
}
```

Now, we can write our input at arbitrary address. Already we leak `kbase`, `kheap`, and `vmemmap_base`  so we can write our input at arbitrary address.

### Arbitrary Address Execute
`struct pipe_buffer` has `struct pipe_operations`, and `struct pipe_operatprions->release()` called when `pipe_buffer` is freed. Therefore, we can execute arbitrary address by setting `struct pipe_buffer->ops` as the address of `struct msg_msgseg` and write address what we want to call at `struct msg_msgseg[0x8:0x10)`.

Then, when `pipe_buffer` is freed, `struct pipe_buffer->ops->release()` is called and we can execute arbitrary address.

## Oneshot Like Arbitrary Code Execution
The detail of AAW and RIP Pivoting is shown at [novel_techniques.md](novel_techniques.md#PIPEShot)