# CVE-2025-38083
## Overview
- Requirements:
    - Capabilites: CAP_NET_ADMIN
    - Kernel configuration: CONFIG_NET_SCHED=y CONFIG_NET_SCH_PRIO=y CONFIG_NET_SCH_SFQ=y
    - User namespaces required: Yes
- Introduced by: https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/commit/?id=7b8e0b6e659983154c8d7e756cdb833d89a3d4d7 
- Fixed by: https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/commit/?id=d35acc1be3480505b5931f17e4ea9b7617fea4d3
- Affected Version: v5.0-rc1 - v6.15.2
- Affected Component: netfilter
- Syscall to disable: unshare
- URL: https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2025-38083
- Cause: Race condition
- Description: A race condition in the Linux Kernel net scheduler subsystem can be exploited to achieve local privilege escalation. The prio_tune routine can race with a sfq child's perturb timer, causing a qlen underflow. We recommend upgrading past commit d35acc1be3480505b5931f17e4ea9b7617fea4d3.


## Analysis
There exists a race condition vulnerability between the net/sched prio and sfq scheduler. When reducing the number of bands in a prio qdisc, if one of the childs is a sfq qdisc, it is possible for duplicate decrease in qlen to be propagated up the tree. This can be abused to obtain a UAF on a parent hfsc scheduler. This UAF can be exploited to achieve LPE.

### Details
When changing a prio qdisc with prio_tune(), the following logic is run when decreasing the number of bands:
```c
	sch_tree_lock(sch);                                     // [1]
	q->bands = qopt->bands;
	memcpy(q->prio2band, qopt->priomap, TC_PRIO_MAX+1);

	for (i = q->bands; i < oldbands; i++)
		qdisc_tree_flush_backlog(q->queues[i]);             // [2]

	for (i = oldbands; i < q->bands; i++) {
		q->queues[i] = queues[i];
		if (q->queues[i] != &noop_qdisc)
			qdisc_hash_add(q->queues[i], true);
	}

	sch_tree_unlock(sch);                                   // [3]

	for (i = q->bands; i < oldbands; i++)
		qdisc_put(q->queues[i]);                            // [4]
```
At [1], the qdisc tree is locked from the root, preventing concurrent access. At [2], qdisc_tree_flush_backlog() 'empties' a qdisc by propagating a reduction of qlen up the tree. Crucially, it does not actually remove the packets. At [3], the root lock is released. At [4], the qdisc_put() call chain is: qdisc_put() -> __qdisc_destroy() -> qdisc_reset(). When this call chain completes, the qdisc is successfully removed.

The race window is between [3] and [4]. During this window, the qlen reduction has been propagated up the tree but the child qdisc still contains its packets. If there exists a function that can still access these packets and drop them, a duplicate qlen reduction can occur. Most functions must be triggered by a netlink message from userspace, which is protected by the global RTNL mutex.

However, a timer-ed function exists in sfq qdisc, sfq_perturbation(). This timer function is setup when the sfq is initialized.
```c
static int sfq_init(struct Qdisc *sch, struct nlattr *opt, struct netlink_ext_ack *extack)
{
	// [...]
	timer_setup(&q->perturb_timer, sfq_perturbation, TIMER_DEFERRABLE);
```

```c
static void sfq_perturbation(struct timer_list *t)
{
	struct sfq_sched_data *q = from_timer(q, t, perturb_timer);
	struct Qdisc *sch = q->sch;
	spinlock_t *root_lock;
	siphash_key_t nkey;
	int period;

	get_random_bytes(&nkey, sizeof(nkey));
	rcu_read_lock();
	root_lock = qdisc_lock(qdisc_root_sleeping(sch));
	spin_lock(root_lock);                              // [5]
	q->perturbation = nkey;
	if (!q->filter_list && q->tail)
		sfq_rehash(sch);                               // [6]
	spin_unlock(root_lock);

	/* q->perturb_period can change under us from
	 * sfq_change() and sfq_destroy().
	 */
	period = READ_ONCE(q->perturb_period);
	if (period)
		mod_timer(&q->perturb_timer, jiffies + period);
	rcu_read_unlock();
}
```

Note that this function does not have to obey the netlink lock that exists for all netlink syscalls as it runs asynchronously. At [5], it acquires the same lock (as at [1]). At [6], it calls sfq_rehash(). This function redistributes packets into new slots. If it fails to do so, it will drop the packet ([7], shown below) and propagate the qlen decrease via qdisc_tree_reduce_backlog(). 
```c
static void sfq_rehash(struct Qdisc *sch)
{
    // [...]
	sch->q.qlen -= dropped;
	qdisc_tree_reduce_backlog(sch, dropped, drop_len);       // [7]
}
```
This gives rise the possibility of a double qlen subtraction. The race is as follows:
```
CPU 0                                 CPU 1
[1]: lock root
[2]: qdisc_tree_flush_backlog()
[3]: unlock root
 |
 |                                    [5]: lock root
 |                                    [6]: rehash
 |                                    [7]: qdisc_tree_reduce_backlog()
 |
[4]: qdisc_put()
```

This can be abused to underflow a parent's qlen.