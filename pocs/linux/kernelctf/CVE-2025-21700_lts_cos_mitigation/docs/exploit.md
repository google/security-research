CVE-2025-21700
--------------

Exploit documentation for CVE-2025-21700 against `mitigation-v3b-6.1.55`,
`lts-6.6.66` and `cos-105-17412.495.73`. The exploit against the mitigation
instance will require a different approach, thus will be documented in a following
section.
Also note that the presented work borrows many concepts from my previous exploit(s)
for CVE-2024-53164.

# LTS / COS

## Triggering the Vulnerability Into Use-After-Free

Similar to many exploits for tc related bugs we need to setup a suitable qdisc
structure in order to trigger a use-after-free. For the bug described in
`vulnerability.md` we will use the following (classes in lower case, qdiscs in
upper case, dummy classes added for stability, see below):

```
        DRR'(root)
       /    |    \
      /     |     \         ...         ...
     /      |      \          \           \
   drr   fifo_B   plug      dummy_0 ...  dummy_N
   DRR            PLUG
    |
  fifo_A
   FIFO
```

As the parent qdisc to abuse the use-after-free on we will choose DRR (i.e. root
in the picture above).
This is a convenient choice because it is a rather simple classful qdisc where so
called "non-workconserving" children (i.e. children that may drop or delay on
enqueue) do not implicate any state changes.
Usually any packet enqueue would result in a subsequent dequeue on this qdisc
right away (except for high load cases, which are hard to control reliably).
Using DRR allows us to create a "blocking" child which prevents any subsequent
dequeue operation on any child if the blocking child happens to be non-workconserving.
The exploit uses a PLUG qdisc as a non-workconserving child for packet stack up.

DRR manages active classes (of type `struct drr_class`) in a simple linked list
(with head `struct drr_sched.active` and node in `struct drr_class.alist`).
During dequeue operation, `drr_dequeue` is called and will essentially (as the
qdisc name suggests) cycle through this list and serve packets from its children.
If a class is empty (or non-workconserving, i.e. `qdisc_peek()` returns `NULL`),
it will stop dequeuing:
```c
static struct sk_buff *drr_dequeue(struct Qdisc *sch)
{
	struct drr_sched *q = qdisc_priv(sch);
	struct drr_class *cl;
	struct sk_buff *skb;
	unsigned int len;

	if (list_empty(&q->active))
		goto out;
	while (1) {
		cl = list_first_entry(&q->active, struct drr_class, alist);
		skb = cl->qdisc->ops->peek(cl->qdisc);                         // [1]
		if (skb == NULL) {
			qdisc_warn_nonwc(__func__, cl->qdisc);
			goto out;
		}

		len = qdisc_pkt_len(skb);
		if (len <= cl->deficit) {
			// ...
			return skb;
		}

		cl->deficit += cl->quantum;
		list_move_tail(&cl->alist, &q->active);
	}
out:
	return NULL;
}
```

As described in the vulnerability document, our goal will be to cause the parent
notification in `qdisc_tree_reduce_backlog()` to be missed because of invalid
`parent` pointers.
During `notify()` an empty class is removed from the list. Thus if we trigger
the vulnerability a dangling pointer is kept in that `active` list:
```c
static void drr_qlen_notify(struct Qdisc *csh, unsigned long arg)
{
	struct drr_class *cl = (struct drr_class *)arg;

	list_del(&cl->alist);
}
```

In order to set this up we will graft FIFO from `fifo_A` to `fifo_B`, thus
FIFO's parent will be invalid after we deleted the original `fifo_A` class:
```
        DRR'(root)
          |    \
          |     \
          |      \
        fifo_B   plug    (dummy classes ommitted)
         FIFO    PLUG
```

Then, we only need to activate the classes:
1) Enqueue a packet to PLUG (`active = [plug]`)
  This is non-workconserving, so `drr_dequeue` returns early.
2) Enqueue a packet to FIFO (`active = [plug, fifo_B]`)
  PLUG is still the first active class and it is still non-workconserving, so
  `drr_dequeue` returns early. FIFO contains a single packet.

Now we can delete the class `fifo_B` and due to the bug the class is freed
without notification.

## From Use-After-Free to RIP control

This leaves us with a use-after-free on the `struct drr_class`.
How can we use it? First have to manipulate the DRR schedule so that `plug` is
not the active class anymore (we will do this by dequeuing a packet from `plug`
which exceeds its quantum, thus sets `fifo_B` as the active class).
This implies list operations on `&cl->alist` which in turn prevents us from
modifying the free class object because list hardening (i.e. `CONFIG_DEBUG_LIST`)
will panic the kernel whenever this list pointer is corrupted.
If we were to modify the class object we would nescessarily have to modify the
list pointer.
To solve this issue we will look at the next level of indirection, specifically
the `qdisc` pointer ([2]) on the class object:
```c
struct drr_class {
	struct Qdisc_class_common	common;

	struct gnet_stats_basic_sync		bstats;
	struct gnet_stats_queue		qstats;
	struct net_rate_estimator __rcu *rate_est;
	struct list_head		alist;
	struct Qdisc			*qdisc;                     // [2]

	u32				quantum;                            // [3.a]
	u32				deficit;                            // [3.b]
};
```
If we keep the object intact `qdisc` will be another dangling pointer, this time
of type `struct Qdisc`. Leaving the class object as-is also implies that list
hardening will be happy, so we just need a way to utilize the `qdisc` object.
This is easy enough, because our dequeue operation provides a basic RIP primitive
right away via the `qdisc->ops->peek` function pointer ([1]).

In order to leave the class object as untouched as possible we use the dummy
classes sketched above. By allocating a series of dummy classes, we bury the
victim object in a non-active slab on another CPU. This should prevent it
from being re-allocated by any of our spraying primitives or other random
allocations during the exploit.

## Heap Spray and RIP Primitive

In order to control the `peek` member, we have to reclaim the FIFO qdisc object
with a suitable object having a sane, controllable `ops` member.
This requires us to forge a heap pointer which is usually non-trivial.
Thus the first trick will be to use `AF_PACKET` sockets with an attached
`PACKET_TX_RING` which creates pointer arrays in kernel. Each pointer in the array
points to fully user controllable memory, thus presents itself as the perfect
spraying primitive for our scenario.
See [here](https://www.willsroot.io/2022/08/reviving-exploits-against-cred-struct.html)
for more details on the primitive.

Secondly, instead of constructing a typical ROP chain, we will abuse the eBPF JIT
compiler to craft a payload which is much simpler while working without a KASLR bypass.
The payload is constructed from `jit.s` and overwrites the `core_pattern`.
Note that JIT hardening is not enabled in this LTS / COS release.
The technique is documented in the background section in `novel-techniques.md`
so we will not go into detail here.

## Stability

The exploit is close to 100% stable on LTS (and COS-109 as a matter of fact),
though see stability notes on the eBPF JIT spray.
The COS-105 variant is unfortunately much less stable because of a different eBPF
JIT image allocator strategy (again see eBPF JIT notes). On optimized variants
we expect stability of ~50% but for simplicity sake this COS version was not
tuned to its specific kernel version.

# Mitigation Instance

Because of `CONFIG_RANDOM_KMALLOC_CACHES` we cannot easily reclaim the `struct Qdisc`
object as we did in the LTS versions.
For now, we will instead focus on the `struct drr_class` object. It turns out
that we can indeed deterministically reclaim one of the class objects with objects
of different types because of a similar issue that I pointed out [previously](https://github.com/google/security-research/blob/master/pocs/linux/kernelctf/CVE-2023-6817_mitigation/docs/exploit.md#mitigation-notes):
Consider the following assembly generated for the class allocation in
`drr_change_class()`:
```objdump
ffffffff81ead050 <drr_change_class>:
ffffffff81ead050:       e8 8b a0 26 ff          call   ffffffff811170e0 <__fentry__>
ffffffff81ead055:       41 57                   push   %r15
ffffffff81ead057:       41 56                   push   %r14
ffffffff81ead059:       41 55                   push   %r13
ffffffff81ead05b:       41 54                   push   %r12
ffffffff81ead05d:       4d 89 cc                mov    %r9,%r12
ffffffff81ead060:       55                      push   %rbp
ffffffff81ead061:       53                      push   %rbx
ffffffff81ead062:       48 83 ec 28             sub    $0x28,%rsp
ffffffff81ead066:       48 8b 51 10             mov    0x10(%rcx),%rdx
ffffffff81ead06a:       4d 8b 30                mov    (%r8),%r14
ffffffff81ead06d:       89 74 24 04             mov    %esi,0x4(%rsp)
ffffffff81ead071:       65 48 8b 04 25 28 00    mov    %gs:0x28,%rax
...
ffffffff81ead1a6:       48 8b 44 24 58          mov    0x58(%rsp),%rax          # [5]
ffffffff81ead1ab:       48 33 05 c6 ef 32 01    xor    0x132efc6(%rip),%rax     # ffffffff831dc178 <random_kmalloc_seed>
ffffffff81ead1b2:       be c0 0d 00 00          mov    $0xdc0,%esi
ffffffff81ead1b7:       48 ba eb 83 b5 80 46    movabs $0x61c8864680b583eb,%rdx
ffffffff81ead1be:       86 c8 61
ffffffff81ead1c1:       48 0f af c2             imul   %rdx,%rax
ffffffff81ead1c5:       48 c1 e8 3c             shr    $0x3c,%rax
ffffffff81ead1c9:       48 8d 14 c5 00 00 00    lea    0x0(,%rax,8),%rdx
ffffffff81ead1d0:       00
ffffffff81ead1d1:       48 29 c2                sub    %rax,%rdx
ffffffff81ead1d4:       48 89 d0                mov    %rdx,%rax
ffffffff81ead1d7:       ba 70 00 00 00          mov    $0x70,%edx
ffffffff81ead1dc:       48 c1 e0 04             shl    $0x4,%rax
ffffffff81ead1e0:       48 8b b8 b8 c1 1d 83    mov    -0x7ce23e48(%rax),%rdi
ffffffff81ead1e7:       e8 e4 39 4d ff          call   ffffffff81380bd0 <kmalloc_trace>
...
```
We can see, that the kmalloc call was _inlined_. Additionally, note that the
return address to compute the random seed is directly fetched from the setup
function frame ([5]). This is problematic because this function is being called
by a function pointer, i.e. indirect call, via `struct Qdisc_class_ops`. This 
means we have access to all `cl_ops->change` functions for potential victim 
allocations as they are all called with the same return address in `tc_ctl_tclass`.
They are guaranteed to fall into the same cache as long as the same inlining
behaviour can be observed (and the sizes fall into the same cache).
And indeed there is one candidate which is usable, `qfq_change_class()`:
```objdump
ffffffff81eb1740 <qfq_change_class>:
ffffffff81eb1740:       e8 9b 59 26 ff          call   ffffffff811170e0 <__fentry__>
ffffffff81eb1745:       41 57                   push   %r15
ffffffff81eb1747:       41 56                   push   %r14
ffffffff81eb1749:       41 55                   push   %r13
ffffffff81eb174b:       41 54                   push   %r12
ffffffff81eb174d:       55                      push   %rbp
ffffffff81eb174e:       53                      push   %rbx
ffffffff81eb174f:       48 83 ec 30             sub    $0x30,%rsp
ffffffff81eb1753:       48 8b 51 10             mov    0x10(%rcx),%rdx
ffffffff81eb1757:       4d 8b 18                mov    (%r8),%r11
ffffffff81eb175a:       89 74 24 08             mov    %esi,0x8(%rsp)
ffffffff81eb175e:       65 48 8b 04 25 28 00    mov    %gs:0x28,%rax
...
ffffffff81eb198a:       48 8b 44 24 60          mov    0x60(%rsp),%rax
ffffffff81eb198f:       48 33 05 e2 a7 32 01    xor    0x132a7e2(%rip),%rax        # ffffffff831dc178 <random_kmalloc_seed>
ffffffff81eb1996:       be c0 0d 00 00          mov    $0xdc0,%esi
ffffffff81eb199b:       48 ba eb 83 b5 80 46    movabs $0x61c8864680b583eb,%rdx
ffffffff81eb19a2:       86 c8 61
ffffffff81eb19a5:       48 0f af c2             imul   %rdx,%rax
ffffffff81eb19a9:       48 c1 e8 3c             shr    $0x3c,%rax
ffffffff81eb19ad:       48 8d 14 c5 00 00 00    lea    0x0(,%rax,8),%rdx
ffffffff81eb19b4:       00
ffffffff81eb19b5:       48 29 c2                sub    %rax,%rdx
ffffffff81eb19b8:       48 89 d0                mov    %rdx,%rax
ffffffff81eb19bb:       ba 80 00 00 00          mov    $0x80,%edx
ffffffff81eb19c0:       48 c1 e0 04             shl    $0x4,%rax
ffffffff81eb19c4:       48 8b b8 b8 c1 1d 83    mov    -0x7ce23e48(%rax),%rdi
ffffffff81eb19cb:       e8 00 f2 4c ff          call   ffffffff81380bd0 <kmalloc_trace>
...
```

Consider the `struct qfq_class`:

```c
struct qfq_class {
	struct Qdisc_class_common common;

	struct gnet_stats_basic_sync bstats;
	struct gnet_stats_queue qstats;
	struct net_rate_estimator __rcu *rate_est;
	struct Qdisc *qdisc;
	struct list_head alist;		/* Link for active-classes list. */
	struct qfq_aggregate *agg;	/* Parent aggregate. */
	int deficit;			/* DRR deficit counter. */
};
```

Notice the slightly different member ordering compared to `struct drr_class`,
specifically `alist` and `qdisc`.
We will be causing a type confusion, interpreting a `struct drr_class` as a
`struct qfq_class` in `qfq_peek_skb()`:
```c
static inline struct sk_buff *qfq_peek_skb(struct qfq_aggregate *agg,
					   struct qfq_class **cl,
					   unsigned int *len)
{
	struct sk_buff *skb;

	*cl = list_first_entry(&agg->active, struct qfq_class, alist);   // [6.1]
	skb = (*cl)->qdisc->ops->peek((*cl)->qdisc);                     // [6.2]

	// ...
}
```

Because of the type confusion the `cl->qdisc` pointer will actually be the
`drr_class.alist.next` pointer, thus `(*cl)->qdisc->ops` will reference into
the next `drr_class` in the list and interpret it as a `struct Qdisc`.
Therefor we will want to craft a valid `struct Qdisc_ops` pointer at
`offsetof(struct drr_class, alist) + offsetof(struct Qdisc, ops)` which overlaps
perfectly with `offsetof(struct drr_class, quantum)` (and following `deficit`
member, see ([3.a]) and ([3.b])).
We have full control over those members, thus allowing us to forge a RIP gadget
rather easily.

The attentive reader may have noticed that this requires a somewhat "inverted"
scenario of what we setup previously:
So far we used DRR as the root qdisc where we trigger the class use-after-free.
The sketch above assumes that we have this use-after-free primitive on a
`struct qfq_class`.
This is no problem at all, since we can simply use the QFQ qdisc as a root
qdisc instead of DRR. (Actually QFQ is just a "fancy" DRR with more classification
options. Those should not matter to us, and we can treat their behaviour equivalent
for our purposes).

By spraying a sequence of `struct drr_class` we 1) hopefully reclaim the
free `struct qfq_class` and turn the use-after-free into the outlined
type-confusion and 2) have the `struct drr_class.alist.next` pointer point
to another drr class. This basically requires that the class that hit the victim
spot not being the last drr class in the active list.

There is one small issue remaining though:
As stated above we need to fake a `struct Qdisc_ops` pointer. To fake the pointer
we used the `AF_PACKET` trick on the LTS / COS variants.
This one is obviously impossible because we used a different object.
Therefor we will resort to the well known deterministically known location of the
exception stacks in the CPU entry area. This issue has been documented several
times (CVE-2023-0597).

## RIP Target

### ROP Chain

The original exploit used to capture the flag leveraged a ROP chain which
overwrites the `core_pattern` and continues to sleep the task in the kernel.
This version requires a separate KASLR leak. Side channels were utilized
for simplicity sake. As demonstrated previously there are side channels which
work on the CI runner as well, so no additional work required.
However, I will present a novel approach using eBPF JIT spraying techniques
which shall be outlined in the following section.

Recall that our potential ROP chain will be called from the `struct Qdisc_ops.peek`
member ([6.2]) and we have 120 bytes of payload space available in the
cpu exception stacks.
This scenario is again non-trivial, so we will use a similar trick as we did
before:
By pivoting the initial call into `qdisc_leaf` we get an additional pointer to
our payload into `rbp` (compare ([7.1a]) and ([7.1b])):
```c
static struct Qdisc *qdisc_leaf(struct Qdisc *p, u32 classid)
{
	unsigned long cl;
	const struct Qdisc_class_ops *cops = p->ops->cl_ops;      // [7.1a]

	if (cops == NULL)
		return NULL;
	cl = cops->find(p, classid);                              // [7.2]

	// ...
}
```
```objdump
ffffffff81e85fa0 <qdisc_leaf>:
ffffffff81e85fa0:       e8 3b 11 29 ff          call   ffffffff811170e0 <__fentry__>
ffffffff81e85fa5:       55                      push   %rbp
ffffffff81e85fa6:       53                      push   %rbx
ffffffff81e85fa7:       48 8b 47 18             mov    0x18(%rdi),%rax
ffffffff81e85fab:       48 8b 68 08             mov    0x8(%rax),%rbp   # [7.1b]
...
```

Then, we can use a `leave` instruction as the `cops->find` member ([7.2]) to
perform a stack pivot onto our payload (we can just make `cops` point into the
exception stacks too).
Because of size restrictions the intitial part of the ROP chain will interleave
with most of the previously used pointers but the chain itself is rather
self-explanitory and performs a simple `core_pattern` overwrite which we can
trigger to escalate privileges.

### eBPF JIT RIP Gadget

As hinted above, another approach is demonstrated utilizing eBPF JIT spraying
techniques. Instead of requiring the ROP chain above, it will utilize a
"one gadget" created using eBPF JIT code, thus does not require KASLR leak nor
exhausting ROP chain crafting.
Again, this approach is described in details in the `novel-techniques.md` document
and will directly replace the entry point to the ROP chain crafted in the previous
section.

## Additional Setup Notes for the Mitigation Instance

There are a few minor derivations to the LTS / COS variant in terms of general
exploit structure.
Specifically we will have two processes where `main` will setup the vulnerable
qdisc tree as described earlier. The other `child` process will be running in a
dedicated user namespace in order to perfom the `struct drr_class` heap spray
without interfering with the vulnerable qdisc structure. Additionally it will
perform the cpu entry area write and `core_pattern` monitoring while `main`
triggers the payload.

## Stability

The exploit is close to 100% stable, though see stability notes on the eBPF
JIT spray which decreases success rates to roughly 97% on the mitigation instance.
