# Disclaimer

This document describes high level ideas / design decisions and is accompanied by
the actual implementation in `rip.c` which strives to be well-documented to
highlight some implementation details. I would suggest reading this document first
to get the overview with all the detours, to eventually skim over the code which
implements the final result if you are interested in the encoding details.
(Also note that the code is more or less a PoC so expect BUGS)

# 1: Kernel One Gadget

We present a "kernel one gadget", i.e. a single callable address which leads to graceful
privilege escalation without panicking the kernel.
This is based on previous works abusing the eBPF JIT compiler but will present
novel techniques specifically bypassing `sysctl.net.core.bpf_jit_harden={1,2}` which
mitigates previous solutions. Additionally we will show that the per-image random
header size added can be bypassed reliably when choosing addresses to jump into
prepared images.

The gadget will have the following general properties:
- works on x86_64
- bypasses `sysctl.net.core.bpf_jit_harden={1,2}`
- is 100% stable given a (module) KASLR bypass
- works in environments with restricted `sysctl.net.core.optmem_max`, specifically requiring
  only `0x5000` bytes
- works in unprivileged contexts without access to userns
- perform an arbitrary kernel memory write of 108 bytes with 38 bytes being uncontrollable junk
  (this will be used for a `core_pattern` overwrite to achieve privilege escalation,
   though is not strictly constrained to this)

The specific variant presented will have the following restrictions:
- requires JIT compilation to be enabled by default on all filters
- requires being called from memory region where want to write memory.
  This is required because the kernel write address is derived from the return address.
- potentially blocks a single core in kernel, thus may require at least 2 CPU
  cores to be useful

The presented gadget will run an infinite loop in kernel, which leads to the latter
restriction. This is not a hard constraint and performing a graceful return from the
shellcode is possible to a limited degree with controlled `eax` register.
All callee-preserved registers can be restored except for `r13` if this should
be needed.

Finally note that the analysis is based on 6.x kernels.

## Background

A user may attach cBPF programs to sockets using the `SO_ATTACH_FILTER` socket option.
These programs can use a limited set of instructions which are eventually translated
to a certain subset of eBPF instructions. (see `net/core/filter.c:bpf_convert_filter()`)
These instructions are later JIT-compiled by the kernel (see `arch/x86/net/bpf_jit_comp.c:do_jit()`)
and placed into a RX-memory region (image) inside the module memory range (see `kernel/bpf/core.c:bpf_jit_binary_pack_alloc()`)
By jumping into a specifically crafted image the processor may missinterpret certain
instructions to achieve some desired task.
Previously this was achieved using the `BPF_LD | BPF_K` instruction which is supposed
to load a given constant into a register.
With the JIT compiler translating the given eBPF instruction with f.e. `K = 0x11223344` into:
```
mov eax, 0x11223344     b8 44 33 22 11
```
This gives control over four bytes of arbitrary data. Previously, 3 bytes of this
could be used to encode some operation, followed by another padding byte which would
serve the purpose of "continuation", consuming the following `mov` prefix `0xb8` so that
the next custom 3 bytes will be interpreted etc:
```
mov eax, 0x3cf20148       b8 48 01 f2 3c
mov eax, 0x3c90c031       b8 31 c0 90 3c
```
.. interpreted with miss-alignment of 1 byte:
```
0000: add rdx, rsi        48 01 f2
0003: cmp al, 0xb8        3c b8     # consumes the next mov prefix, serves as a "nop"
0005: xor eax, eax        31 c0
0007: nop                 90
0008: ...
```

This works perfectly fine unless `sysctl.net.core.bpf_jit_harden=1` is enabled.
In this case the JIT compiler will perform "constant blinding" (see `kernel/bpf/core.c:bpf_jit_blind_insn()`).
This will essentially encode all constants using something similar to the following operation:
```
mov eax, <random value>
xor eax, <random value XOR constant k>
```

Our goal will therefor be to work around this constant blinding.

## Technical Details

In order to get known predictable values into the JIT memory we need another primitive
except for `BPF_LD | BPF_K` and friends. Specifically we generally need to avoid
any BPF operation which includes a constant. There are some exceptions to the blinding
of constants, but usually only for single byte values in a known low range (f.e.
`shr reg, K` with some low `K in [0, 31]`).
Luckily there is another exception which "misses" this blinding: The jmp instruction
`BPF_JMP | BPF_JA`. This instruction will have "link time" constants which
the blinding cannot predict, thus ignores.
Again, this primitive gives us 4 bytes of control over JIT memory:
```
jmp <>                  e9 XX XX XX XX
```
For us it is possible to predict those offsets thus we can use it to encode
some chosen constants with some clever eBPF instruction packing.
There is one major problem though: This offset depends on the image size and
there are severe limitation to this size. Firstly there is a limit on the number
of instructions for unprivileged eBPF programs (4096 by default). Secondly there
is a kernel parameter limiting the space each socket may allocate thus limiting
the total size of the image (`sysctl.net.core.optmem_max`).
On newer kernel this parameter is namespaced, thus could be increased in a userns,
but we will assume a restricted default value of `0x5000`.
This means we cannot encode instructions which would require a MSB (`Bi`) of `0x50` or
greater. This leaves us with a very restricted set of instructions, especially
because we need to make sure that the "continuation" property holds, otherwise
we would be executing a single instruction and then return to the normal
eBPF code:
```
# long jmps
e9 A0 B0 00 00          jmp 0xB0A0 (*)
e9 A1 B1 00 00          jmp 0xB1A1 (*)

# short jmps
eb C0                   jmp 0xC0 (*)
eb C1                   jmp 0xC1 (*)
```
( (*) offsetted by instruction length )

The JIT compiler will emit "short" jump instructions for offsets `< 124`, "long"
jumps for longer ranges. For now we will ignore the short jumps because they
are not needed for the main payload.

Starting with the continuation step, we have required zero-bytes as the 2 most
significant bytes. This leaves us with the following possible continuation patterns:
```
# A0 B0 interpreted as single instruction, does not work, breaks the chain:
00 00                   add byte ptr [rax], al

# A0 B0 00 interpreted as single instruction (I)
# - OR -
# A0    single instruction
# B0 00 single instruction
00 e9                   add cl, ch

# A0 B0 00 00 e9 interpreted as a single instruction (II)
A1 B1 00 00 ..
```

Looking through the instruction encodings which satisfy the given pre-conditions
we opted for the following minimal set of instructions for encoding a payload:
```
# (II)
b8 XX 00 00 e9 ..       mov eax, 0xe90000XX
35 XX 00 00 e9 ..       xor eax, 0xe90000XX
05 XX 00 00 e9 ..       add eax, 0xe90000XX
2d XX 00 00 e9 ..       sub eax, 0xe90000XX

# "nop" to encode Bi 00 allowing encoding of single byte instructions for (I)
04 00                   add al, 0x00
# (I):
54                      push rsp
55                      push rbp
50                      push rax
52                      push rdx
5d                      pop rbp
58                      pop rax
5a                      pop rdx

# (I)
89 45 00                mov [rbp], eax
88 45 00                mov [rbp], al

01 45 00                add [rbp], eax
29 45 00                sub [rbp], eax
```

With these primitives at hand, we can do basically any computation we like.
The following building blocks will be used for our shellcode:
```javascript
// basically a movzx eax, byte
func encode_byte_into_eax(byte: u8):
  // for the sake of brevity, something like this, while ignoring the constraints:
  mov eax, 0xe9000001
  xor eax, (0xe9000001 ^ byte)

func get_stack_pointer_into_rbp(offset: u8):
  push rsp

  if offset > 0:
    // get a pointer to the pointer
    push rsp
    pop rbp

    encode_byte_into_eax(offset)

    // note that this may overflow the byte arithmetic. This is desired as we
    // emulate the full length addition byte-wise and as a bonus without losing
    // carry bits.
    add [rbp], eax

  pop rbp

// add the given offset to a pointer on top of the stack
func offset_pointer(ptr: [top of stack], offset: T):
  for i in (0..sizeof(T)):
    offset_byte : u8 = ith byte of offset

    get_stack_pointer_into_rbp(i)
    encode_byte_into_eax(offset_byte)
    add [rbp], eax

func write_payload(dest: rbp, value: [u8]):
  for i in (0..len(value)):
    encode_byte_into_eax(value[i])
    mov [rbp], al

    push rbp
    offset_pointer(1u8)
    pop rbp

```

The only thing missing is the encoding of the destination address, which needs
to be adjusted for KASLR offset. For this we will use the already present
return address on the stack. Assuming we know where our call gadget executes and
the difference between the return address and the destination address is constant
under KASLR, we can simply adjust the pointer by the known offset using the
`offset_pointer()` primitive.
Now we can encode a payload which overwrites the `core_pattern` for a simple
yet effective privilege escalation.

Eventually we only need to terminate the chain gracefully. At this point a simple
return is the easiest solution, utilizing the existing eBPF function epilogue
and continuing with the return address which can be put in place wherever need
be. This may require additional register fixup though, because we clobber quite
a few callee-preserved registers with our shellcode. Thus we will quickly sketch
out how to put the shellcode into an infinite loop in case a graceful return
is not possible. It turns out that this is not much harder because we can simply
subtract a few bytes from the return address so that the call is issued again.
This way we execute our payload over and over again essentially being in an
infinite loop.

Finally, there is a little problem. Even though the approach above would work
perfectly fine on your default desktop linux distribution, it does not work
with the given limited size constraints of `optmem_max = 0x5000`. So we will throw
in another little trick to compress the `write_payload()` primitive into a
single instruction:
```
dd 30               fnsave [rax]
```
This works, because the kernel generally does not touch the x87 FPU state and
we can encode a payload into it in userland which will be preserved all the
way into the kernel until it hits the shellcode.
This gives us the promised `108` bytes write at the cost of `38` bytes of it
being junk with limited control. This is not a problem for the `core_pattern`
overwrite, but may be a problem if some other target would need to be chosen.
In tested kernels these junk bytes would usually hit the `core` `sysctl` table,
rendering any attempt to access them impossible without fixup. This can be trivially
done after privilege escalation if desired.

The attentive reader may notice that this is not an instruction which satisfies
the pre-conditions mentioned above:
```
.. e9               ...
dd 30               fnsave [rax]
00 00               add byte ptr [rax], al
...                 < real eBPF jit code follows >
```
Because we have a valid pointer in `rax`, the junk `add` instruction does not
impose a problem. Since the initial bytes of the payload are clobbered anyways,
this additional byte does not hurt.
Since our chain termination relies on getting back to the eBPF code anyways,
we might as well just use this instruction to get out of the shellcode.

## Stability Notes

eBPF JIT images are allocated using the `module_alloc`ator
(again see `kernel/bpf/core.c:bpf_jit_binary_pack_alloc()` and `:bpf_prog_pack_alloc()`).
These allocations require a different de-randomization leak than the usual
kernel _text offset.
Additionally each image is offset by a random hole of bad instructions:
```c
// from bpf_jit_binary_pack_alloc():
	hole = min_t(unsigned int, size - (proglen + sizeof(*ro_header)),
		     BPF_PROG_CHUNK_SIZE - sizeof(*ro_header));
	start = get_random_u32_below(hole) & ~(alignment - 1);

	*image_ptr = &ro_header->image[start];
```

This code does have a major weakness though because the `start` is always _aligned_.
In our case `hole = 56` and `alignment = 4`, so as long as we have some "nop slide"
with instruction lengths being a multiple of 4 we can reliably bypass this random hole
if our slide is at least 56 bytes long.
We will choose the `neg eax` instruction (`BPF_ALU | BPF_NEG`) for that purpose:
- it is 2 bytes in lengths, thus can satisfy the alignment property
- performs a sane "nop" operation when hit unaligned while satisfying the contiunation condition
```
f7 d8               neg eax
f7 d8               neg eax
e9 A0 B0 00 00      jmp 0xB0A0(*)

# interpreted as:
d8 f7               fdiv st(7)
d8 f7               fdiv st(7)
d8 e9               fsubr st(1)
A0 B0 .. ..         ...
```
One downside is that this clobbers additional space of the FPU state, namely
`st(0)`. But for our purposes we have more than enough space and the additional
kernel clobbering does not hurt.

Given we know the image base, we can now reliably hit our payload bypassing
the random per-image hole.
This is obviously a major limitation thus, the next technique will introduce
a general KASLR bypass.

# 2: Module KASLR bypass

To be a true "one gadget" it needs invariants accross different KASLR offsets.
Previous work already hinted that the `module_alloc`ator has weak randomization.
Thus, the following section claims novelty in the following two aspects:
- Analyse the weak randomization and provide a well documented strategy how it can be bypassed to almost 100% certainty
- Adapt the general strategy to work under the constraints introduced in the first chapter / provide a true one-shot one gadget without any additional leaks required.

The de-randomization is only limited by number of open file descriptors. For the
naive solution this requires ~1500 open file descriptors which may be too limiting
in some scenarios.
(Note that another issue might be vmalloc noise, though we will assume this to be
low which is true for the short period required in many cases).

## Background

As stated above eBPF JIT images are allocated from the `module_alloc`ator. Specifically
for our case (i.e. kernel version > 6.x) they are allocated in "packs", each pack
representing `2MB` of usable memory by the compiler. Those packs are managed using
a simple bitmap which marks individual chunks of length 64 bytes as free or allocated.
An allocation would then search for a large enough free region or allocate a new
pack to serve from otherwise. Those packs are allocated using `alloc_new_pack()` in
`kernel/bpf/core.c`.
We can see that each pack is allocated using `bpf_jit_alloc_exec()` which essentially
aliases to `module_alloc()` in our case. This function has a specialized implementation
on x86 in `arch/x86/kernel/module.c` and boils down to this:
```c
// in our case MODULE_VADDR = 0xffffffffc0000000
return __vmalloc_node_range(size, MODULE_ALIGN,
			 MODULES_VADDR + get_module_load_offset(),
			 MODULES_END, gfp_mask, PAGE_KERNEL,
			 VM_FLUSH_RESET_PERMS | VM_DEFER_KMEMLEAK,
			 NUMA_NO_NODE, __builtin_return_address(0));
```

with `get_module_load_offset()`:
```c
static unsigned long int get_module_load_offset(void)
{
	if (kaslr_enabled()) {
		mutex_lock(&module_kaslr_mutex);
		/*
		 * Calculate the module_load_offset the first time this
		 * code is called. Once calculated it stays the same until
		 * reboot.
		 */
		if (module_load_offset == 0)
			module_load_offset = get_random_u32_inclusive(1, 1024) * PAGE_SIZE;
		mutex_unlock(&module_kaslr_mutex);
	}
	return module_load_offset;
}
```

A few things to note here:
- `__vmalloc_node_range()` is a rather simple allocator which will serve allocations
  one after another, thus making future allocations very predictable
- `__vmalloc_node_range()` will by default insert a "guard page" after each allocation
- `get_module_load_offset()` will initialize a random offset once, being in the range of `[1, 1024] * PAGE_SIZE`

This last point is in nature the same thing as pointed out above about the random
hole at the beginning of each image. Given a "slide" of 1024 `PAGE_SIZE` aligned
allocations we will have an offset which is known to be on our slide for every
possible random offset.
The question now is how good of a slide we can achieve. Given the pack size of
`2MB` we will need to fill two packs with page sized images to get the entire
range. Because of the guard page there will be 1 guaranteed bad page in the slide
thus our chances of hitting the correct page are `1023/1024 = 99.9%`:
```
MODULES_VADDR     real base after module_load_offset applied
|                 |
|                 |
|< R * PAGE_SIZE >|< PACK 0 >G< PACK 1>G< ... >G< PACK n>G< PACK n+1 >G< PACK n+2>
                                                     |
                   start of attacker controlled data |
```
With `R` = `module_load_offset`, `G` representing a guard page and `PACK i` representing
a JIT pack of 512 pages.
Assuming `PACK N` is the last pre-allocated pack with some memory taken, we
can fill it so that our slide will be served from `PACK n+1` and `PACK n+2` for
a total of 1024 images with size `PAGE_SIZE` plus the guard page. Since new
packs are always page aligned we do not need to worry about image base alignment.
We can now predict an almost guaranteed address of our desired image payload at
`MODULES_VADDR + n * 512 + n * G + 1024 + X` with `X` being the offset required
to bypass the per-image random hole.

This assumes that `module_alloc` is only used by the eBPF JIT compiler. This is
not nescessarily true, but the properties still hold, requiring only to apply an
additional offset.

This shows how the `module_load_offset` can be bypassed by using images which
are exactly page sized. Previous payloads were small enough to fit into a single
page thus no special care was needed. This does not hold for the payload presented
in Part I. Thus the following will outline how it can be adapted for it.

## Adaption to Larger Payloads

When the payload size exceeds one page, above properties will not longer hold
and the success chances decline quickly. In order to overcome this a new strategy
is proposed: Instead of serving the payload from each image in the slide directly,
the slide will be constructed as a "real slide" of `PAGE_SIZE`ed images where each
performs inter-image jumps to the following image. This way each image serves as
an elaborated "nop" in a nop sled of images, which will eventually terminate in
the real payload.

To achieve this, a few new gadgets are required. Additionally, some gadgets
which were usable in the larger payload are no longer usable. This stems from
the even stronger constraint of `Bi < 0x10` (see Part I).

Our first goal will be constructing a useful call gadget into a following image.
This call has to satisfy a few constraints:
- needs to be encodable (obviously)
- needs to hit a reasonable offset into a _page aligned_ image
Especially the second point is non-trivial to achieve and it requires another
tool:
Generating calls via the `shr eax, 1` (`BPF_ALU | BPF_RSH` with `K = 1`) instruction:
```
neg eax               f7 d8
shr eax, 1            d1 e8
jmp (0x80-*)          e9 80 00 00 00
```
Assuming we come from our nop-sled of `neg eax` instructions the `BPF_RSH` instruction
will provide a `call` instruction prefix. In combination with a `jmp` encoded
right afterwards we can encode a call offset which is reasonably page aligned:

```
.. ..
d8 f7                 fdiv st(7)
d8 d1                 fcom st(1)
e8 e9 80 00 00        call +0x80ee
```

Now we can call into a following payload again and achieved an inter-image
jump. The problem is that this jump is not the best because it essentially is
a recursion which can go as deep as 1024 jumps. This might cause troubles
exhausting the stack space.
In order work around that we will implement a short payload that mimics a tail
call by modifying the return address of the initial call on the stack, specifically
adding `0x1000` to it and doing a return so that we will slide through our
"image nop sled" one image at a time.

Adding this offset to the return address turns out non-trival as well because of
the `add [rbp], eax` gadget not being encodable in a payload of size `0x1000`.
To work around that we add three more gadgets to the pool:
```
# (I)
44 0b 00              or r8d, dword ptr [rax]
44 03 00              add r8d, dword ptr [rax]
44 01 00              add dword ptr [rax], r8d
```
We can first get a predictable value into `r8d` by or-ing it with `-1` (all bits
set). After that we can combine `add r8d, dword ptr [rax]` and `add dword ptr [rax], r8d`
instructions with a prepared `[rax]` to load any value we want into `r8d` (in our
case `0x1000`)

Eventually we can issue a final `add dword ptr [rax], r8d` on the return address
to increment it by the desired page size.
Luckily a `ret` is easily encoded and we do not have to worry about ever returning.

This concludes our inter image jump chain. It has a minor issue though:
Because of the random hole at the start of each image, the return address on
the stack may point before the call instruction for a few images and after the
call for other images (Where the latter would be desired because then we would
be hitting the real payload deterministically).
To solve this issue, the image which is hit by the call will perform another one
time step by adding a smaller offset to the return address. This way we ensure
that we always only hit the 1-page increment in the following images.
One downside of this is the fact that it takes away much needed space from the
real payload image. We will solve this issue by changing the size of the final
nop sled image slightly so that it cancels out this one time addition made earlier.

To summarize, this is how the inter image nop sled will work:
```
[ IMAGE 0x0000 ] (size = 0x1000)
 : call IMAGE +0x8000 + A
  ..
  A  : add B to [rsp] (i.e. return address)
       push [rsp]
       ret
  ...
  A+B: add 0x1000 to [rsp] (i.e. return address)
       push [rsp]
       ret

[ IMAGE 0x1000 ] (size = 0x1000)
  0  : call IMAGE +0x8000 + A
  ..
  A  : add B to [rsp] (i.e. return address)
       push [rsp]
       ret
  ...
  A+B: add 0x1000 to [rsp] (i.e. return address)
       push [rsp]
       ret

...

[ IMAGE 0x8000 ] (size = 0x1000)
  0  : call IMAGE +0x8000 + A
  ..
  A  : add B to [rsp] (i.e. return address)
       push [rsp]
       ret
  ...
  A+B: add 0x1000 to [rsp] (i.e. return address)
       push [rsp]
       ret

...

```

Now suppose our initial guess hits `IMAGE 0x0000`. This will call into payload
`IMAGE 0x8000 + A`. At this point the return address is basically `IMAGE 0x0000 + 0`.
Now the second payload will adjust this return address to `IMAGE 0x0000 + B`.
Then we _return_ and hit the final part of the _previous_ `IMAGE 0x0000 + A+B`,
which then continues to slide through `IMAGE 0x?000 + A+B`.
Because of the random hole at the start of each image, `0`, `A`, `A+B` are all
padded with nop slides themselves.

The implementation will perform another optimization by merging `A` with `A+B` so
that `A+B` finds known register values and only has to do the addition and
return address duplication.

Note that this is a combination of 3 jump chains encoded into a single image.
To satisfy the space constraints the jump offsets are encoded in a rudimentary
"interleaved" style to make it as small as possible.

With this new style nop sled we can again guess the image address as described
above. Because we do have some additional constraints our success chances decrease
slightly. Since our slide will fail on the guard page between the packs
`n+1` and `n+2`, we need to include a payload at the end of each pack. Additionally
the initial call requires to find a valid nop-slide target within the 8 page range, thus
is not allowed to hit the real payload.
With the payload + final nop slide termination occupying ~7 pages, our success
chance decreases to `2 * (512 - 7 - 8) / 1024 ~ 97%`.

To conclude, this gives us an unprivileged kernel "one gadget" which is independent of KASLR
and has a close to 100% success rate.

# Mitigations

## Dead-Code-Elimination in cBPF programs

My initial proposal to the kernel team boiled down to removing dead code in
the filter code. This prevents "jump chains" altogether because every following
jump would be dead code and would thus be removed.
This mitigation was disliked due to increased complexity being exposed to cBPF
programs.

## Improved Randomization

### Better per-image random prefix size

For x86 alignment requirements on the image start seem to be unescessary. If external
code has assumptions about eBPF JIT image start addresses, additional random nops
could be inserted after the determined image start.
This is non-invasive, relatively easy to implement and would decrease success chances
significantly.

### Disable the JIT pack allocator

Older kernels do not have the JIT pack allocator (which would allocate a whole
2MB pack at once and then re-use it for JIT images). They rather allocate
the images directly from the `vmalloc` allocator. This actually improves
randomization because more guard pages would be generated thus decreasing the
success chances of hitting the payload. (Note that the presented de-randomization
approach, without any additional adaption, is fully prevented by this)

### Allocator Restrictions to improve Module Base Randomization

This one seems tricky to mitigate because of the allocator being deterministic in
nature. One could think about adding the desired non-determinism to the eBPF
JIT pack allocator.
A rather simple approach would be limiting packs to non-page sized allocations.
This seems to have negligible negative effects because the packs are managed
in a large chunk anyways.

## Disable cBPF Filters

On a high level taking away the `SO_ATTACH_FILTER` functionality for unprivileged
users seems to be the easiest and most effective to implement (via LSM or similar).
If this is not an option, further reducing `optmem_max` can be mitigating as well.
Though this seems risky as small images (as sub 1 page as shown) can put out great
damage too.

## Jump Constant Blinding

Finally, it would be possible to improve the existing constant blinding facility
to include blinding of jump offsets. This would likely add a lot of complexity
due to use of indirect jumps or alternatively complex control flow modifications
in the JIT compiler.
