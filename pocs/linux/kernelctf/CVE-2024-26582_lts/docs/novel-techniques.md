## Determining heap and page allocator state by parsing /proc/zoneinfo

Linux kernel exposes a lot of information in a world-readable /proc/zoneinfo including:

- per-node free/low/high page counters for the buddy allocator
- per-cpu cache count/high/batch counters

This can be useful in multiple ways during exploitation.

### Predicting when a new heap slab is going to be allocated

When performing a cross-cache attack or any other technique involving reuse of physical pages by SLUB allocator we would like to be able to allocate our victim object from a newly allocated slab.

This is not trivial because we don't know the existing state of a given kmalloc cache - it probably already has some partial slabs and a new kmalloc will use them before allocating a new slab page.

The usual solution to this problem is to just allocate a lot of objects and hope some will eventually be allocated from the new page.
The downside is that we won't know which allocated object is the one we are interested in (the one from a new page).

There are also often limits on the number of the victim object we can create.
In an extreme case, the victim object can be a single-instance item and we only have one chance to get it allocated from the page we want.

Lastly, when exploiting a use-after-free caused by a race condition we need to perform the reallocation in the shortest time possible and performing hundred allocation syscalls in the tight race condition window just won't work.

Even when there are no such limitations, using this technique tends to increase exploit reliability.

Parsing /proc/zoneinfo solves these problems by giving us a count of the currently available pages on our CPU, for example:
```
    cpu: 0
              count: 293
              high:  378
              batch: 63
```

Before performing our attack we need to prepare by allocating objects from the chosen cache (e.g. kmalloc-256) and reading /proc/zoneinfo after each allocation.
When count is decreased by the number of pages per slab (e.g. kmalloc-256 uses 1 page per slab and kmalloc-512 2 pages, but this is version and config dependent).

When we notice the decrease in page it means our last allocation triggered a new slab.

Now we have to allocate (objects_per_slab-1) objects and we can be sure that the current slab is full and next allocation (the important one) will use a newly allocated physical page.


### Predicting how much we have to allocate free to trigger PCP flush

Sometimes we want to reuse a physical page for allocation that needs a page of a different order (e.g. we have a use-after-free object from kmalloc-512 that uses order 1 page and we want to reallocate it from kmalloc-256 cache that uses order 0 pages).

To be able to do this we have to flush our page from the PCP to return it to the buddy allocation. To do this we need to free enough physical pages to exceed the 'high' mark of the PCP. 
Parsing /proc/zoneinfo allows us to know exactly how many pages have to be freed instead of doing it blindly.



