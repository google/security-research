# Background

As `__rtnl_unlock()` commented, RTNL sequences are usually protected by `rtnl_lock()` and `rtnl_unlock()` to avoid race issues.

```c
void __rtnl_unlock(void)
{
	...
	/* Ensure that we didn't actually add any TODO item when __rtnl_unlock()
	 * is used. In some places, e.g. in cfg80211, we have code that will do
	 * something like
	 *   rtnl_lock()
	 *   wiphy_lock()
	 *   ...
	 *   rtnl_unlock()
	 *
	 * and because netdev_run_todo() acquires the RTNL for items on the list
	 * we could cause a situation such as this:
	 * Thread 1			Thread 2
	 *				  rtnl_lock()
	 *				  unregister_netdevice()
	 *				  __rtnl_unlock()
	 * rtnl_lock()
	 * wiphy_lock()
	 * rtnl_unlock()
	 *   netdev_run_todo()
	 *     __rtnl_unlock()
	 *
	 *     // list not empty now
	 *     // because of thread 2
	 *				  rtnl_lock()
	 *     while (!list_empty(...))
	 *       rtnl_lock()
	 *				  wiphy_lock()
	 * **** DEADLOCK ****
	 *
	 * However, usage of __rtnl_unlock() is rare, and so we can ensure that
	 * it's not used in cases where something is added to do the list.
	 */
	WARN_ON(!list_empty(&net_todo_list));

	mutex_unlock(&rtnl_mutex);
	...
}
```

And `rtnl_unlock()` is actually a wrapper for `netdev_run_todo()`, which internally calls `__rtnl_unlock()` to unlock `rtnl_mutex`. After unlocking `rtnl_mutex` via `netdev_run_todo()`, `netdev->reg_state` might be `NETREG_UNREGISTERING` or `NETREG_UNREGISTERED`. Moreover, the network device object will be freed when `netdev refcnt` is `0`, otherwise `netdev_wait_allrefs_any()` will keep waiting.

```c
/* ...
 * 2) Since we run with the RTNL semaphore not held, we can sleep
 *    safely in order to wait for the netdev refcnt to drop to zero.
 * ...
 */
void netdev_run_todo(void)
{
	...
	__rtnl_unlock();
	...

	list_for_each_entry_safe(dev, tmp, &list, todo_list) {
		if (unlikely(dev->reg_state != NETREG_UNREGISTERING)) {
			netdev_WARN(dev, "run_todo but not unregistering\n");
			list_del(&dev->todo_list);
			continue;
		}

		write_lock(&dev_base_lock);
		dev->reg_state = NETREG_UNREGISTERED;
		write_unlock(&dev_base_lock);
		linkwatch_forget_dev(dev);
	}

	while (!list_empty(&list)) {
		dev = netdev_wait_allrefs_any(&list);
		list_del(&dev->todo_list);
		...
		/* Free network device */
		kobject_put(&dev->dev.kobj);
	}
}
```

`ethnl_ops_begin()` is generally protected by `rtnl_lock()` and `rtnl_unlock()`, such as `ethnl_default_doit()`. `ethnl_default_parse()` places the searched target network device into `req_info->dev`, `ethnl_init_reply_data()` then sets `reply_data->dev = req_info->dev`, followed by a call to the `prepare_data()` function, such as `linkinfo_prepare_data()`, and subsequently calls `ethnl_ops_begin()` and `__ethtool_get_link_ksettings()`.

```c
static int ethnl_default_doit(struct sk_buff *skb, struct genl_info *info)
{
	...
	ret = ethnl_default_parse(req_info, info, ops, !ops->allow_nodev_do);
	if (ret < 0)
		goto err_dev;
	ethnl_init_reply_data(reply_data, ops, req_info->dev);

	rtnl_lock();
	ret = ops->prepare_data(req_info, reply_data, info);
	rtnl_unlock();
	...
}

static int linkinfo_prepare_data(const struct ethnl_req_info *req_base,
				 struct ethnl_reply_data *reply_base,
				 const struct genl_info *info)
{
	struct linkinfo_reply_data *data = LINKINFO_REPDATA(reply_base);
	struct net_device *dev = reply_base->dev;
	...
	ret = ethnl_ops_begin(dev);
	if (ret < 0)
		return ret;
	ret = __ethtool_get_link_ksettings(dev, &data->ksettings);
	...
}
```

However, there is a race condition between `ethnl_ops_begin()` and `netdev_run_todo()`. If `dev->reg_state` is `NETREG_UNREGISTERED`, the check in `ethnl_ops_begin()` is passed, and subsequent operations access an unregistered network device object.

```c
int ethnl_ops_begin(struct net_device *dev)
{
	...
	if (!netif_device_present(dev) ||
	    dev->reg_state == NETREG_UNREGISTERING) {
		ret = -ENODEV;
		goto err;
	}
	...
}
```

# Exploit

To exploit the race condition between `ethnl_ops_begin()` and `netdev_run_todo()`, the `refcnt` of a network device object can be increased, widening the time window between `__rtnl_unlock()` and `kobject_put(&dev->dev.kobj)`, and ensuring that `dev->reg_state` is in the `NETREG_UNREGISTERED` state as much as possible.

Exploitation scenario: Assume there are two netdev objects, `dev1` and `dev2`. Where `dev1` has a large refcnt and `dev2` has a normal refcnt, then `dev2` will be freed first. If `dev2` is accessed through `dev1`, a Use-After-Free will occur.

To meet the above scenario, the `ipvlan` network device is a good target object. Where `ipvlan->phy_dev` can point to other netdev objects, such as `veth`.

```c
int ipvlan_link_new(struct net *src_net, struct net_device *dev,
		    struct nlattr *tb[], struct nlattr *data[],
		    struct netlink_ext_ack *extack)
{
	struct ipvl_dev *ipvlan = netdev_priv(dev);
	struct ipvl_port *port;
	struct net_device *phy_dev;
	...
	phy_dev = __dev_get_by_index(src_net, nla_get_u32(tb[IFLA_LINK]));
	if (!phy_dev)
		return -ENODEV;
	...
	ipvlan->phy_dev = phy_dev;
	...
}
```

Control flow hijacking can be achieved by heap spraying to overwrite the `ethtool_ops` pointer of `ipvlan->phy_dev` and forging the `get_link_ksettings()` function pointer.

```c
int __ethtool_get_link_ksettings(struct net_device *dev,
				 struct ethtool_link_ksettings *link_ksettings)
{
	ASSERT_RTNL();

	if (!dev->ethtool_ops->get_link_ksettings)
		return -EOPNOTSUPP;

	if (!netif_device_present(dev))
		return -ENODEV;

	memset(link_ksettings, 0, sizeof(*link_ksettings));
	return dev->ethtool_ops->get_link_ksettings(dev, link_ksettings);
}

static const struct ethtool_ops ipvlan_ethtool_ops = {
	.get_link	= ethtool_op_get_link,
	.get_link_ksettings	= ipvlan_ethtool_get_link_ksettings,
	.get_drvinfo	= ipvlan_ethtool_get_drvinfo,
	.get_msglevel	= ipvlan_ethtool_get_msglevel,
	.set_msglevel	= ipvlan_ethtool_set_msglevel,
};

static int ipvlan_ethtool_get_link_ksettings(struct net_device *dev,
					     struct ethtool_link_ksettings *cmd)
{
	const struct ipvl_dev *ipvlan = netdev_priv(dev);

	return __ethtool_get_link_ksettings(ipvlan->phy_dev, cmd);
}
```

The allocation of `struct net_device` is handled by `alloc_netdev_mqs()`, and the related function calls are `rtnl_newlink() -> __rtnl_newlink() -> rtnl_newlink_create() -> rtnl_create_link() -> alloc_netdev_mqs()`.

```c
struct net_device *alloc_netdev_mqs(int sizeof_priv, const char *name,
		unsigned char name_assign_type,
		void (*setup)(struct net_device *),
		unsigned int txqs, unsigned int rxqs)
{
	...
	alloc_size = sizeof(struct net_device);
	if (sizeof_priv) {
		/* ensure 32-byte alignment of private area */
		alloc_size = ALIGN(alloc_size, NETDEV_ALIGN);
		alloc_size += sizeof_priv;
	}
	/* ensure 32-byte alignment of whole construct */
	alloc_size += NETDEV_ALIGN - 1;

	p = kvzalloc(alloc_size, GFP_KERNEL_ACCOUNT | __GFP_RETRY_MAYFAIL);
	if (!p)
		return NULL;

	dev = PTR_ALIGN(p, NETDEV_ALIGN);
	...
}
```

When `ipvlan->phy_dev` is specified as a `veth` device, the corresponding slab cache is `kmalloc-cg-4k`. After winning the race and freeing `veth`, heap spraying is performed using `msg_msg` objects to overwrite the `ethtool_ops` pointer in the `veth` netdev to forge `get_link_ksettings()`. After successful heap spraying, function hijacking control flow can be achieved through `...->ethnl_default_doit()->prepare_data()->__ethtool_get_link_ksettings()->ipvlan_ethtool_get_link_ksettings()->dev->ethtool_ops->get_link_ksettings()`.

```c
int __ethtool_get_link_ksettings(struct net_device *dev,
				 struct ethtool_link_ksettings *link_ksettings)
{
	...
	return dev->ethtool_ops->get_link_ksettings(dev, link_ksettings);
}
```

# KASLR Bypass
To bypass KASLR we refer to this [technique](https://github.com/google/security-research/blob/master/pocs/linux/kernelctf/CVE-2023-6817_mitigation/docs/exploit.md#kaslr-bypass).

# ROP Chain

`dev->ethtool_ops` needs to be set to a writable kernel address, and `kernfs_pr_cont_buf` is a good target object. The content of `kernfs_pr_cont_buf` can be controlled by the `kernfs_walk_ns` function. The parameter `const unsigned char *path` can be controlled via `setsockopt(sock, SOL_IP, IPT_SO_SET_REPLACE, &payload_xgcroup, sizeof(struct ipt_replace))`, as referenced in this [technique](https://github.com/YuriiCrimson/ExploitGSM/blob/main/docs/writeup.pdf). The relevant function call chain is: `setsockopt -> ... -> do_ipt_set_ctl -> kernfs_walk_and_get -> kernfs_walk_and_get_ns -> kernfs_walk_ns`.

```c
#define PATH_MAX 	4096	/* # chars in a path name including nul */
static char kernfs_pr_cont_buf[PATH_MAX];
static struct kernfs_node *kernfs_walk_ns(struct kernfs_node *parent,
					  const unsigned char *path,
					  const void *ns)
{
	...
	len = strlcpy(kernfs_pr_cont_buf, path, sizeof(kernfs_pr_cont_buf));
	...
}
```

The `strlcpy` will stop copying at the first null byte, so we employ a reverse segmented population: 
1. Scan the payload backwards to find the last contiguous segment of non-null bytes. 
2. Each time extracts a segment of data that does not contain null bytes into a tmp_buffer, padding the preceding portion with 0x0d. 
3. Trigger a loop repeatedly writes a segment into `kernfs_pr_cont_buf`. Since `strlcpy` will stop at the first null byte it encounters, the data already written to the buffer is preserved.

Through multiple iterations, each call extracts a preceding segment of the payload (progressively including more of the prefix). This process eventually writes the complete payload (the forged `dev->ethtool_ops` and the ROPchain), including its null bytes, into `kernfs_pr_cont_buf`.

We now successfully place the forged `dev->ethtool_ops` and the specific ROPchain in the `kernfs_pr_cont_buf` buffer. And control the `ethtool_ops` of freed `veth` to be the address of `kernfs_pr_cont_buf` via sprayed `msg_msg` objects. However, searching for gadgets that controls the RDI register is not easy. To hijack `dev->ethtool_ops->get_link_ksettings(dev, link_ksettings)`, we overwrite it with the `clk_change_rate()` function. After controlling the content of `core`, `core->ops->set_rate(core->hw, core->new_rate, best_parent_rate)` is a controllable three-parameter function call. `core->ops->recalc_rate(core->hw, parent_rate)` is a controllable two-parameter function call. `clk_change_rate(core->new_child)` can be used to the next loop of control flow hijacking.

```c
static void clk_change_rate(struct clk_core *core)
{
	...
	if (!skip_set_rate && core->ops->set_rate)
		core->ops->set_rate(core->hw, core->new_rate, best_parent_rate);

	trace_clk_set_rate_complete(core, core->new_rate);

	core->rate = clk_recalc(core, best_parent_rate);
	...
	/* handle the new child who might not be in core->children yet */
	if (core->new_child)
		clk_change_rate(core->new_child);
	...
}

static unsigned long clk_recalc(struct clk_core *core,
				unsigned long parent_rate)
{
	...
	if (core->ops->recalc_rate && !clk_pm_runtime_get(core)) {
		rate = core->ops->recalc_rate(core->hw, parent_rate);
		clk_pm_runtime_put(core);
	}
	...
}
```

When `clk_change_rate()` is called for the first time, the content of `core` (the RDI register, i.e., the first parameter `struct clk_core *core`) points to the freed `veth`, which has been overwritten by the sprayed `msg_msg` objects. Note that `struct clk_core`'s `struct clk_ops *ops` member overlaps with `msg_msg`'s `m_list.prev`. We forge
`core->ops` by calling `spray_padding_msg_msg()` to fill the previous `msg_msg`, thereby controlling `core->ops->set_rate`. By setting `core->ops->set_rate` to the gadget `push_rsi_jmp_rsi_0x39` and `core->new_rate` to the ROP chain address on `kernfs_pr_cont_buf`, a stack pivot is performed. This transfers control from RDI to RSI, making RSI point to our previously constructed payload on `kernfs_pr_cont_buf`.

```
0xffffffff81b3351b : push rsi ; jmp QWORD PTR [rsi+0x39]
```

After the stack pivot, the actual ROP payload stored on `kernfs_pr_cont_buf` is executed with multiple `clk_change_rate()` calls to gain root privilege and switch the task namespace.