## Setup

To trigger the TLS encryption we must first configure the socket.
This is done using the setsockopt() with SOL_TLS option:

```
        static struct tls12_crypto_info_aes_ccm_128 crypto_info;
        crypto_info.info.version = TLS_1_2_VERSION;
        crypto_info.info.cipher_type = TLS_CIPHER_AES_CCM_128;

        if (setsockopt(sock, SOL_TLS, TLS_TX, &crypto_info, sizeof(crypto_info)) < 0)
                err(1, "TLS_TX");

```

This syscall triggers allocation of TLS context objects which will be important later on during the exploitation phase.

In KernelCTF config PCRYPT (parallel crypto engine) is disabled, so our only option to trigger async crypto is CRYPTD (software async crypto daemon).

Each crypto operation needed for TLS is usually implemented by multiple drivers. 
For example, AES encryption in CBC mode is available through aesni_intel, aes_generic or cryptd (which is a daemon that runs these basic synchronous crypto operations in parallel using an internal queue).

Available drivers can be examined by looking at /proc/crypto, however those are only the drivers of the currently loaded modules. Crypto API supports loading additional modules on demand.

As seen in the code snippet above we don't have direct control over which crypto drivers are going to be used in our TLS encryption.
Drivers are selected automatically by Crypto API based on the priority field which is calculated internally to try to choose the "best" driver.

By default, cryptd is not selected and is not even loaded, which gives us no chance to exploit vulnerabilities in async operations.

However, we can cause cryptd to be loaded and influence the selection of drivers for TLS operations by using the Crypto User API. This API is used to perform low-level cryptographic operations and allows the user to select an arbitrary driver.

The interesting thing is that requesting a given driver permanently changes the system-wide list of available drivers and their priorities, affecting future TLS operations.

Following code causes AES CCM encryption selected for TLS to be handled by cryptd:

```
        struct sockaddr_alg sa = {
                .salg_family = AF_ALG,
                .salg_type = "skcipher",
                .salg_name = "cryptd(ctr(aes-generic))"
        };
        int c1 = socket(AF_ALG, SOCK_SEQPACKET, 0);

        if (bind(c1, (struct sockaddr *)&sa, sizeof(sa)) < 0)
                err(1, "af_alg bind");

        struct sockaddr_alg sa2 = {
                .salg_family = AF_ALG,
                .salg_type = "aead",
                .salg_name = "ccm_base(cryptd(ctr(aes-generic)),cbcmac(aes-aesni))"
        };

        if (bind(c1, (struct sockaddr *)&sa2, sizeof(sa)) < 0)
                err(1, "af_alg bind");
```

## What we start with and what can we do

If we win the race condition, vulnerability gives us a limited write primitive.
To be exact, it gives us an ability to change a 8 bit integer value of '1' to '0' at an offset 0x9c in the struct tls_sw_context_rx object which is allocated from a general kmalloc-256 cache.

The big problem is finding a victim object in which this limited write gives us the ability to escalate privileges or at least get a better exploitation primitive.

In the case of 5.15 kernel we were able to find an object with a refcount-like field that we were able to modify to turn the vulnerability into a use-after-free, but it proved impossible for 6.1.

Instead, we used a variant of the Dirty PageTable technique. The usual situation with this technique is that we have full write access to the page table entry through a use-after-free and we can arbitrarily modify the PFN to point the PTE to a physical page belonging to e.g. kernel code we want to patch.

Here we have more limitations, but the basic idea is the same.

The x86_64 page table takes 0x1000 bytes and consists of 512 8 byte entries.
It means that if we manage to allocate a page table in place of the tls_sw_context_rx, we will be able to modify a byte at offset 0x4 of the PTE starting at offset 0x90. 
This offset points at the PFN part of the PTE (PFN is encoded in bits 12 to 52 of the PTE). This byte of the PFN has to have a value of 0x01 for us to be able to change to 0x00.

KernelCTF setup has 3.5 GB of physical memory which translates to 0xe0000 of physical pages, so PFNs should go from 0 to 0xe0000, but there are holes for PCI devices etc, so in practice the last PFN is around 0x11b200.

Here's what an example PTE looks for that last PFN:
0x800000011b200867

The 5th byte is 0x01 which is ideal for our write primitive. 
If we change it to 0x00, the entry will now point at PFN 0x1b200 instead of 0x11b200.
We can do the same with any PFN >= 0x100000

This is very promising, but we have to solve several issues to be able to get to privilege escalation:

- we have to remap a page table in place of the tls context. Page tables are allocated straight from the buddy allocator, so the physical page of the kmalloc-256 slab has to be first discarded.
- this page table must map to physical memory with PFN >= 0x100000
- we have to place some useful objects to write to after our PTE modification at PFN-of-modified-pte - 0x100000
- these steps have to be pretty reliable, because the hard to hit race condition gives us a limited chance of success from the start - if the next stage works only e.g. 10% of time, the vulnerability could become hard to exploit in real conditions, especially if a failed attempt crashes the kernel.

## Preparing memory allocations

### "High" memory

We have to ensure that the new page table allocated to replace the tls context will have PFNs >= 0x100000. This value is actually important for Linux memory management - physical memory on x86_64 is split in multiple zones. 
Zone DMA32 contains memory under 4 GB (PFNs < 0x100000) and zone Normal memory above this limit. Physical page allocation is pretty predictable when allocating a lot of memory - first systems tries to use zone Normal, until its empty and then move to DMA32.

To improve the reliability of the exploit, we map a lot of "Normal" memory at the very beginning (g_high_mem in the exploit code) and release chunks of it before allocating our new user memory that will get its PTE modified (g_victim_mem).

### "Low" memory

We also need to make sure that memory that our modified PTE will point to will contain a kernel object that can be used for privilege escalation.
This is pretty simple, we just create a lot of xattr objects until the memory is almost full.
For performance xattrs are split into multiple fds and stored in g_low_xattr_fds array for later use.

## Triggering use-after-free through race condition

```
        spin_lock_bh(&ctx->decrypt_compl_lock);
        if (!atomic_dec_return(&ctx->decrypt_pending))
[1]                complete(&ctx->async_wait.completion);
[2]        spin_unlock_bh(&ctx->decrypt_compl_lock);
}
```

To exploit the race condition we have to hit window between lines [1] and [2] and perform following actions:
1. Close the socket to free tls context (struct tls_sw_context_rx), leading to discard of the slab page
2. Allocate a new page table in place of the tls context.

To hit this small window and extend it enough to fit our allocations we turn to a well-known timerfd technique invented by Jann Horn.
The basic idea is to set hrtimer based timerfd to trigger a timer interrupt during our race window and attach a lot (as many as RLIMIT_NOFILE allows) of epoll watches to this timerfd to make the time needed to handle the interrupt longer.
For more details see the original [blog post](https://googleprojectzero.blogspot.com/2022/03/racing-against-clock-hitting-tiny.html).

Exploitation is done in 2 threads - main process runs on CPU 0, and a new thread (child_recv()) is cloned for each attempt and bound to CPU 1

| CPU 0 | CPU 1 |
| -------- | -------- |
| allocate tls context                                  |  -                                                                    |
| -                                                     |  exploit calls recv() triggering async crypto ops                     |
| -                                                     |  tls_sw_recvmsg() waits on completion                                 |
| -                                                     |  cryptd calls tls_decrypt_done()                                      |
| -                                                     |  tls_decryption_done() finishes complete() call                       |
| -                                                     |  timer interrupts tls_decrypt_done()                                  |
| recv() returns to userspace unlocking the socket      |  timerfd code goes through all epoll notifications                    |
| exploit calls close() to free tls context             |  ...                                                                  |
| exploit allocates a page table in place of tls context|  ...                                                                  |
| -                                                     |  interrupt finishes and returns control to tls_decrypt_done()         |
| -                                                     |  spin_unlock_bh() writes to PTE                                       |


## Ensuring the slab page is discarded

struct tls_sw_context_rx is allocated from kmalloc-256. This cache uses a single page slab storing 16 objects.
To ensure the slab page is discarded we have to meet the same requirements as in a cross-cache attack:

- all objects in the same slab as tls_sw_context_rx must be freed. All neighbouring objects are xattrs from the same kmalloc-256 cache and are freed before starting the race condition, which freezes the slab and puts it on a per cpu partial list
- per cpu partial list must be full to unfreeze the slab after tls context is freed
- per node partial list must also be full for the slab to be discarded instead of moved to the per node list

All these requirements are met before tls context is freed by freeing enough kmalloc-256 xattrs.

## Checking for success

If PTE modification is successful, our user memory will point to the physical memory of simple_xattr objects that have data under our control inside, so we just have to loop through mmapped area to look for a pattern (here 0x4242424242424242) - if this is found, first stage of exploitation is successful and we move to the second stage implemented in stage2() function.
The physical memory is now mapped both under our user virtual address return by mmap (g_victim_mem) and in kernel space for xattr objects.


## Second stage

The xattr structure looks like this on 6.1:

```
struct simple_xattr {
        struct list_head           list;                 /*     0  0x10 */
        char *                     name;                 /*  0x10   0x8 */
        size_t                     size;                 /*  0x18   0x8 */
        char                       value[];              /*  0x20     0 */
```
 
We now have read/write access to every field of it and we have to use it to get a root shell.

For a start, we get a free heap leak from prev/next and name pointers, but unfortunately no function pointers.

There are many possible approaches to get code execution, here we decided to use the name pointer for creating an arbitrary free primitive - when xattr is removed, kfree() is called on the name.

We will use it to create a use-after-free on a struct key_restriction object:
```
struct key_restriction {
        key_restrict_link_func_t   check;                /*     0   0x8 */
        struct key *               key;                  /*   0x8   0x8 */
        struct key_type *          keytype;              /*  0x10   0x8 */
};
```

This object is used after KEYCTL_RESTRING_KEYRING operation and the 'check' function pointer will be called when attempting to add a new key to a restricted keyring.

To do this we have to know two things:
- heap address of the key_restriction object
- kernel text base (we could have leaked it externally with a side-channel, but it's not that hard to leak it from memory and have a self-contained exploit)

### Leaking info

To leak more data we just have to replace some of the xattrs in our memory with different object.
Specifically we would like to have access to struct key - it has a pointer to the key_restriction object and key type pointer to gives us a kernel text leak.

```
struct key {
...
        union {
                struct keyring_index_key index_key;      /*  0x88  0x28 */
                struct {
                        long unsigned int hash;          /*  0x88   0x8 */
                        long unsigned int len_desc;      /*  0x90   0x8 */
                        struct key_type * type;          /*  0x98   0x8 */
                        struct key_tag * domain_tag;     /*  0xa0   0x8 */
                        char *     description;          /*  0xa8   0x8 */
                };                                       /*  0x88  0x28 */
        }; 
...
        struct key_restriction *   restrict_link;        /*  0xd0   0x8 */
};
```

### Allocating key object

Unfortunately, key objects are allocated from a dedicated cache (key_jar, single page slab, 16 objects per slab), so we have to perform a cross-cache attack.
To make matters worse, to allocate "low" memory we used large xattrs from kmalloc-4k which had 8 page slabs (order 3), so we can't expect to simply get the recently freed page from the PCP.
To stay with order 0 pages we would have to limit xattrs to 256 bytes and this would slow down the allocation phase of the exploit too much.

#### Discarding the xattrs page

To discard the slab page we have to perform the same steps as we did with freeing tls context. 
This puts at least one order 3 page in the PCP.


### Moving the page from PCP to buddy allocator

To move pages from PCP we have to free enough pages to exceed the upper limit of the given PCP cache ('high' value). Each time this limit is exceeded on page free PCP will move a batch of existing pages to the buddy allocator.

Fortunately we don't have to guess how many pages need to be freed. Page allocator state is exposed in /proc/zoneinfo (this file is world-readable).

Here's an example of the part describing PCP status of zone DMA32 on CPU 1:
```
    cpu: 1
              count: 185
              high:  9096
              batch: 63
```

We now know that we need to free 8911 pages to move a batch of 63 to the buddy allocator. 
This zoneinfo file is parsed in the exploit (get_pagecount() function) to get real-time info on the page allocator state.

The process of moving pages from PCP to buddy allocator takes place in the free_pcppages_bulk() function and starts with pages of the order that triggered the flush, then moves in a round robin fashion over other orders until. 
Pages in free_pcpages_bulk() are taken from the tail of the list, so pages last freed will be the last to be moved.

In the exploit function prepare_pcp() allocates enough of an order 3 pages to be able trigger the flush, then after discarding the victim page frees them all in flush_pcp().

### Reusing order 3 pages from the buddy allocator for key_jar

The next issue is getting our order 3 page when requesting order 0 page for the key_jar. 
This would be quite simple if we could allocate as many keys as we want - first allocation will use order 0 PCP pages, then buddy allocator pages in increasing order until we eventually get to the page we want.

Unfortunately, we are severely limited in the number of keys we can allocate - the limit is 200 keys with 20000 bytes total, so we would be able to allocate at most 12 pages in the best case scenario, but probably less.

We could try draining the PCP based on the info from /page/zoneinfo and draining order 0, 1 and 2 from buddy allocator, but reliability would probably be less than optimal, and we need this part of the exploit to be very reliable.

So, instead we add another step - allocate an object that also uses order 0 pages, but there are no limits on its creation.

For this we use netlink allocation primitive - when a netlink message with size > 0x1000 is sent, the buffer for the message is allocated through vmalloc, which internally allocates order 0 physical pages.

When allocating through netlink, we mark each page with a different pattern and then read memory to figure out which netlink buffer was allocated in our user victim area.

Then we can free only one netlink buffer, which will free 2 physical pages - that many we can easily allocate within the key limits that we have.

Before the target netlink buffer is freed, we do some additional operations to increase stability.

1. Allocate some keys in get_fresh_slabs() to fill potential holes in existing partial key_jar slabs
2. Ensure we have a fresh empty kmalloc-192 slab. Kmalloc-192 is used when creating a new key and if it needs to allocate a new slab it could take our target slab that we need for the key object itself.

This second operation uses get_pagecount() (parsing of the /proc/zoneinfo) to discover when exactly the new slab is allocated.

The next step is to free the selected netlink buffer. We have to remember that 2 pages are freed. 
If we have detected that the first page was our target, this means it will be the second page allocated from PCP (LIFO).
To improve our chances once more, we try to get rid of this first extra page. 
This is done in drain_pcp_order0() with the help of get_pagecount() to detect when exactly the new page is allocated (we use xattrs for allocation, so it would be impossible to determine current slab state otherwise).

Finally, we alloc new keys of the keyring type (only this type uses the link restriction mechanism).

### Finding keys in memory and leaking kernel base

We know that a key has a pointer to the key type struct located in the kernel text section and the last 12 bits of this address are fixed, so we use it to find the key in our memory and at the same time figure out the base kernel address.


### Leaking location of the key_restriction object

We use keyctl with a KEYCTL_RESTRICT_KEYRING to allocate a key_restriction object and we read it from our key object (which we already know the location of).

### Triggering kfree() on the key_restriction object

Now we replace the name pointer of the simple_xattr under our control and call unlink() on the associated file to trigger removal of all xattrs.
unlink() is used because removal with fremovexattr() requires us to know the name of the xattr and this name consists of the content of the key_restriction object.

Now we can easily allocate our fake key_restriction object using the xattr primitive once more.

### Getting RIP control

All we need to do is try to add a new key to our restricted keyring and the check function will be called giving us RIP control.

### Pivot to ROP

Call to key restriction check function looks like this:
```
        return keyring->restrict_link->check(keyring, key->type, &key->payload,
                                             keyring->restrict_link->key);
```

The fourth argument (RCX) is under our direct control, as we recently performed use-after-free on the key_restriction object.
Now we have to place our ROP under a known address that we will place at restrict_link->key. 
For this, we can use a leak from xattr next pointer. 
All xattrs were freed and we can now reallocate them with our ROP payload. One of them will be placed at the previously leaked pointer.

Then we can pass control to the ROP chain using only 3 gadgets:

```
mov rax, qword ptr [rcx + 8]
test rax, rax
je 0xffffffff812bbdcf
mov rsi, qword ptr [rcx]
lea rdi, [rbp - 0x10]
mov rdx, r14
call    __x86_indirect_thunk_rax
```

```
push   rsi
jmp    QWORD PTR [rsi+0x39]
```

and finally

```
pop rsp
ret
```

## Second pivot

At this point we have full ROP and enough space available, but our standard privilege escalation payload relies on ROP being at a known location, so we choose an unused read/write area in the kernel and use copy_user_generic_string() to copy the second stage ROP from userspace to that area.
Then we use a `pop rsp ; ret` gadget to pivot there.

## Privilege escalation

The execution is happening in the context of a syscall this time, so it's easy to escalate privileges with standard commit_creds(init_cred); switch_task_namespaces(pid, init_nsproxy); sequence and return to the root shell.
