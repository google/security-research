# CVE-2025-37798
## Overview
First, use prefetch sidechannel to bypass KASLR. Then, use the re-entrancy bug to doubly activate a hfsc class. This causes the hfsc qdisc to improperly track the class' reference count, allowing us to obtain a UAF hfsc class. We can then perform a write-what-where to achieve ROP. Other than offsets, the exploit is identical for LTS and COS (105).
The Mitigation instance exploit is explained in the final section.

## Triggering the bug
Let's first understand the drop flow in `codel_dequeue()`. The function first dequeues a single packet using `dequeue_func()` at [1]. Then at [2], `codel_should_drop()` is passed that packet to decide if it should be dropped. After this, the function has two drop modes, the continuous dropping mode ([3]) and the single drop mode ([6]).
```c
	struct sk_buff *skb = dequeue_func(vars, ctx);                           // [1]
	codel_time_t now;
	bool drop;

	if (!skb) {
		vars->dropping = false;
		return skb;
	}
	now = codel_get_time();
	drop = codel_should_drop(skb, ctx, vars, params, stats,                  // [2]
				 skb_len_func, skb_time_func, backlog, now);
	if (vars->dropping) {                                                    // [3]
		if (!drop) {                                                         // [4]
			vars->dropping = false;
		} else if (codel_time_after_eq(now, vars->drop_next)) {
			while (vars->dropping &&                                         // [5]
			       codel_time_after_eq(now, vars->drop_next)) {
                // ...
				drop_func(skb, ctx);
				stats->drop_count++;
				skb = dequeue_func(vars, ctx);
				if (!codel_should_drop(skb, ctx,
						       vars, params, stats,
						       skb_len_func,
						       skb_time_func,
						       backlog, now)) {
					vars->dropping = false;
				} else {
					vars->drop_next =
						codel_control_law(vars->drop_next,
								  params->interval,
								  vars->rec_inv_sqrt);
				}
			}
		}
	} else if (drop) {                                                       // [6]
		u32 delta;

		if (params->ecn && INET_ECN_set_ce(skb)) {
			stats->ecn_mark++;
		} else {
			stats->drop_len += skb_len_func(skb);
			drop_func(skb, ctx);                                             // [7]
			stats->drop_count++;

			skb = dequeue_func(vars, ctx);                                   // [8]
			drop = codel_should_drop(skb, ctx, vars, params,
						 stats, skb_len_func,
						 skb_time_func, backlog, now);
		}
		vars->dropping = true;
        // ...
		vars->drop_next = codel_control_law(now, params->interval,
						    vars->rec_inv_sqrt);
	}
```

Let's look at single drop mode ([6]) first as it is simpler. This mode is activated when the flag for continuous dropping (`vars->dropping`) is not set and when `codel_should_drop()` determines that the packet should be dropped. Then, this packet is dropped at [7]. The next packet in the queue is dequeued at [8]. This then enables the dropping flag for subsequent dequeues. It also updates the `vars->drop_next` time. We control `params->interval` when we create the fq_codel qdisc. By setting it to 0, there is no delay in the next drop time, ensuring that future drops are never too early.

Next, let's look at continuous dropping mode ([3]), which is activated when `vars->dropping` is set. This is essentially a looped version of the single drop mode, terminating when `codel_should_drop()` decides that the packet should not be dropped ([4]). Within the loop, it drops the current packet and dequeues the next packet. Then, it updates `vars->dropping` based on the outcome of `codel_should_drop()`.

Finally, let's examine `codel_should_drop()`, which decides if a packet should be dropped.
```c
	bool ok_to_drop;
	u32 skb_len;

    // ...
    vars->ldelay = now - skb_time_func(skb);

	if (codel_time_before(vars->ldelay, params->target) ||                   // [9]
	    *backlog <= params->mtu) {
		vars->first_above_time = 0;
		return false;
	}
	ok_to_drop = false;
	if (vars->first_above_time == 0) {                                       // [10]
		vars->first_above_time = now + params->interval;
	} else if (codel_time_after(now, vars->first_above_time)) {
		ok_to_drop = true;                                                   // [11]
	}
	return ok_to_drop;
```

At [9], there is first a check whether `vars->ldelay` is before `params->target` and also that `*backlog <= params->mtu`. If either is true, the packet will not be dropped. We can fail the first half of the check by specifying `params->target = 0` when creating the fq_codel qdisc. The second check checks the backlog (total bytes of packets in the qdisc's queue) against the MTU associated with the qdisc. Generally, sending large packets wil defeat this check. We will see how to do so in detail later. At [10], if this is the first time a packet has passed the drop checks, the function will return false. Only packets that pass the drop checks after that first packet will be dropped ([11]).

We will use the following qdisc setup to get the fq_codel to have `qlen = 0` and `drops = 3` (this will be useful for exploitation later):
```
        2:0 (drr)
        /    \
        2:1   2:2
        /       \ 
3:0 (fq_codel)  4:0 (plug)
```
1. Send packet into plug qdisc. This allows us to subsequently accumulate packets in fq_codel.
2. Send 7 packets into fq_codel. Packets 1-6 can have any payload size. Packet 7 must have a payload larger than `params->mtu`.
3. Unplug the plug qdisc. This dequeues the 1 packet in the plug qdisc, then repeatedly dequeues from fq_codel until it is empty.

> This logic is encapsulated in the exploit code `trigger_bug()`.

The first dequeue into the fq_codel qdisc dequeues the packet 1. When `codel_should_drop()` is called, the check at [9] fails because the backlog is larger than `params->mtu` (due to the packet 7). However, check [10] passes and sets `vars->first_above_time` and returns `ok_to_drop = false`. The second dequeue into fq_codel dequeues the packet 2. Again, when `codel_should_drop()` is called, check [9] fails but this time, because `vars->first_above_time` is set, the function returns true. This causes `codel_dequeue()` to enter the single drop mode ([6]). This drops packet 2 and dequeues packet 3 ([8]). Also, `vars->dropping` is set to true. This single drop (of packet 2) is propagated to the parent qdiscs normally (and `drop_count` is reset). The third dequeue into fq_codel dequeues packet 4. Like before, `codel_should_drop()` returns true. The dropping loop at [5] is entered. Packets 4-6 are dropped in the loop, increasing `stats->drop_count` to 3. On the iteration where packet 6 is dropped, packet 7 is dequeued. `codel_should_drop()` returns false for packet 7 as the backlog is now empty, so check [9] fails. The loop ends with packets 4-6 dropped and packet 7 dequeued. Consequently, the queue is empty as all 7 packets have been dropped/dequeued. This triggers the bug when `drop_count > 0` and `qlen == 0` (explained in [vulnerability.md](vulnerability.md)).

One caveat is that a packet's payload usually cannot exceed MTU, so we must find workarounds for step 2 above. Looking at `fq_codel_init()`, we see that `cparams.mtu` is initialized to the network interface's current MTU.
```c
static int fq_codel_init(struct Qdisc *sch, struct nlattr *opt,
			 struct netlink_ext_ack *extack)
{
    // ...
	q->cparams.mtu = psched_mtu(qdisc_dev(sch));
```

In fact, we can then update the network interface's MTU and the `cparams.mtu` value will not change. So, we can first set the network interface MTU to a low value (e.g. 1500), create the fq_codel qdisc, then increase it to a high value (e.g. 65536). The new MTU value allows us to send packets that are larger than the MTU value captured in `fq_codel_init()`.

Another possible method is to use GSO packets.

## Getting UAF
After triggering the bug, we have a fq_codel with `qlen = 0` and `drops = 3`, with a parent with `qlen > 0`. The parent drr qdisc has `qlen = 3` as it was not notified of the drops. First, let's extend the qdisc hierarchy described above.

```
                1:0 (hfsc)
                /   \
                1:1  1:8
               /        \   
        2:0 (drr)       5:0 (plug)
        /    \
        2:1   2:2
        /       \ 
3:0 (fq_codel)  4:0 (plug)
```

Notice that the drr subtree is identical to the previous section. The process of triggering the bug with a hfsc grandparent remains the same. For our UAF, we will be obtaining a UAF on the hfsc class 1:1. To obtain UAF, we leverage a different behaviour of fq_codel qdisc: improper reset.

`qdisc_reset()` is used to reset a qdisc, which sets the qdisc's qlen to 0.
```c
void qdisc_reset(struct Qdisc *qdisc)
{
	const struct Qdisc_ops *ops = qdisc->ops;

	trace_qdisc_reset(qdisc);

	if (ops->reset)
		ops->reset(qdisc);

	__skb_queue_purge(&qdisc->gso_skb);
	__skb_queue_purge(&qdisc->skb_bad_txq);

	qdisc->q.qlen = 0;
	qdisc->qstats.backlog = 0;
}
```

For fq_codel, this calls `fq_codel_reset()`. Interestingly, this function does not zero out the `cstats.drop_count` value.
```c
static void fq_codel_reset(struct Qdisc *sch)
{
	struct fq_codel_sched_data *q = qdisc_priv(sch);
	int i;

	INIT_LIST_HEAD(&q->new_flows);
	INIT_LIST_HEAD(&q->old_flows);
	for (i = 0; i < q->flows_cnt; i++) {
		struct fq_codel_flow *flow = q->flows + i;

		fq_codel_flow_purge(flow);
		INIT_LIST_HEAD(&flow->flowchain);
		codel_vars_init(&flow->cvars);
	}
	memset(q->backlogs, 0, q->flows_cnt * sizeof(u32));
	q->memory_usage = 0;
}
```

After triggering the bug on the above stack, we can reset all qdiscs by bringing the network interface down and then back up. This will reset all qdiscs' qlen to 0, but leave `drop_count = 3` for the fq_codel qdisc.

Next, notice that we can propagate the `drop_count` to parent qdiscs using `fq_codel_change()`.
```c
static int fq_codel_change(struct Qdisc *sch, struct nlattr *opt,
			   struct netlink_ext_ack *extack)
{
    // ...
	while (sch->q.qlen > sch->limit ||
	       q->memory_usage > q->memory_limit) {
		struct sk_buff *skb = fq_codel_dequeue(sch);

		q->cstats.drop_len += qdisc_pkt_len(skb);
		rtnl_kfree_skbs(skb, skb);
		q->cstats.drop_count++;
	}
	qdisc_tree_reduce_backlog(sch, q->cstats.drop_count, q->cstats.drop_len);
	q->cstats.drop_count = 0;
	q->cstats.drop_len = 0;
```

By triggering a call to `fq_codel_change`, `drop_count = 3` is propagated upwards via `qdisc_tree_reduce_backlog()`. If we do so now, both the drr and the hfsc's qlen will underflow to -3 (unsigned).

Instead, let's send packets in this sequence.
1. Send packet to plug 4:0. This allows us to aggregate packets in the fq_codel qdisc. (drr qlen = 1)
2. Send packet to fq_codel 3:0. (drr qlen = 2)
3. Modify fq_codel, propagating `drop_count = 3` to drr and to hfsc qdisc. (drr qlen = -1)
4. Send packet to plug 4:0. (drr qlen = 0)

At the end of this sequence, the drr qlen is zero. Crucially, the hfsc parent is never notified that the drr qdisc is emptied because `qlen = 0` happens only at the end of an enqueue call (step 4), which does not check for empty childs.

> This code is encapsulated in `overflow_qlen()`.

The rest of the exploit is similar to [CVE-2025-37798 exploit](). If we delete class 1:1 now, there will be dangling pointer in the hfsc qdisc's vt tree. Recall that the exploit involves `hfsc_dequeue()` accessing the dangling vt tree pointer to the freed class.

```c
static struct sk_buff *
hfsc_dequeue(struct Qdisc *sch)
{
	struct hfsc_sched *q = qdisc_priv(sch);
	struct hfsc_class *cl;
	struct sk_buff *skb;
	u64 cur_time;
	unsigned int next_len;
	int realtime = 0;

	if (sch->q.qlen == 0)                                                    // [12]
		return NULL;

	cur_time = psched_get_time();

	cl = eltree_get_mindl(q, cur_time);
	if (cl) {
		realtime = 1;
	} else {
		cl = vttree_get_minvt(&q->root, cur_time);                           // [13]
		if (cl == NULL) {
			qdisc_qstats_overlimit(sch);
			hfsc_schedule_watchdog(sch);
			return NULL;
		}
	}
    // ...
```

At this point, the hfsc qlen is zero. In order to pass check [12], we send a single packet into the other hfsc class (1:8), which has plug qdisc (5:0). The order of deleting class 1:1 and sending the packet into 1:8 doesn't matter. The entire process up to this stage is in `setup_uaf()`.

To trigger the UAF, simply send a packet to a hfsc class that doesn't exist. The hfsc qdisc will not enqueue the packet, but `qdisc_run()` will still trigger dequeue from hfsc, calling `hfsc_dequeue()`. This will access the deleted class 1:1, which is a freed `hfsc_class` object, in [13].

## LPE
From this point on, there are many documented strategies to achieve LPE.

For LTS and COS, we use the strategy outlined in [CVE-2023-4623](https://github.com/google/security-research/blob/66053d865bf43b3e8d379f41f353e3b125cf4524/pocs/linux/kernelctf/CVE-2023-4623_lts_cos/docs/exploit.md#write-what-where). There are 2 differences in our exploit: we use `struct user_key_payload` to reclaim instead of `simple_xattr`, and a different ROP chain. First, in `spray_keyring()`, reclaim the UAF class with `struct user_key_payload`, which contents we can [control](https://bsauce.github.io/2021/09/26/kernel-exploit-%E6%9C%89%E7%94%A8%E7%9A%84%E7%BB%93%E6%9E%84%E4%BD%93/#5-add_key). This is elastic size, and allocated with `GFP_KERNEL`, so it is allocated in the same cache as the hfsc_class (in fact, all qdisc classes are allocated with `GFP_KERNEL`).

```c
static int
hfsc_change_class(struct Qdisc *sch, u32 classid, u32 parentid,
		  struct nlattr **tca, unsigned long *arg,
		  struct netlink_ext_ack *extack)
{
	// [...]
	cl = kzalloc(sizeof(struct hfsc_class), GFP_KERNEL);
```

```c
int user_preparse(struct key_preparsed_payload *prep)
{
	struct user_key_payload *upayload;
	size_t datalen = prep->datalen;

	if (datalen <= 0 || datalen > 32767 || !prep->data)
		return -EINVAL;

	upayload = kmalloc(sizeof(*upayload) + datalen, GFP_KERNEL);
```


This method manipulates internal hfsc_class pointers to obtain a 8-byte write-what-where (the pointers are set in `prep_key_desc()`). We use the write-what-where to overwrite the `qfq_qdisc_ops.change()` pointer, and perform ROP. Using available rop gadgets, we overwrite core_pattern with our program path using copy_from_user then simply call msleep. Another thread of our exploit notice the /proc/sys/kernel/core_pattern changes, it will try to crash itself so our exploit will executed as high privilege and gives us root shell to get the flag.

## Mitigation exploit
On LTS/COS, the qlen mismatch primitive can be escalated into a UAF on a classful parent qdisc class, such as hfsc or drr. This is what we did in the previous sections. However, the heap mitigations on the Mitigation instance prevent the exact same exploit from working. Specifically, because of `CONFIG_RANDOM_KMALLOC_CACHES`, we cannot reclaim the `hfsc_class` object.

We will outline a general method for exploiting the Mitigation instance when we have a qlen mismatch primitive. This involves attaching more qdiscs to the vulnerable qdisc hierarchy, and then abusing buddy allocator to bypass `CONFIG_RANDOM_KMALLOC_CACHES`. This method is heavily inspired by [Theori's CVE-2024-36978 exploit](https://www.hexacon.fr/slides/Cho_Lee-Utilizing_Cross-CPU_Allocation_to_Exploit_Preempt-Disabled_Linux_Kernel.pdf).

> Note: This exploit is slower than LTS/COS method.

Firstly, create a drr qdisc (1:0) with two classes 1:1, 1:2. Our goal is to delete class 1:1 while leaving a dangling pointer in the drr qdisc, similar to [CVE-2025-21700](https://github.com/google/security-research/blob/7297a9e6379a298a98850eba026a98144512933c/pocs/linux/kernelctf/CVE-2025-21700_lts_cos_mitigation/docs/exploit.md#triggering-the-vulnerability-into-use-after-free).

Next, attach a multiq qdisc (2:0) to drr class 1:1. The multiq qdisc is useful because it has an elastic object allocation.

```c
static int multiq_init(struct Qdisc *sch, struct nlattr *opt,
		       struct netlink_ext_ack *extack)
{
	struct multiq_sched_data *q = qdisc_priv(sch);
	int i, err;

	q->queues = NULL;

	if (!opt)
		return -EINVAL;

	err = tcf_block_get(&q->block, &q->filter_list, sch, extack);
	if (err)
		return err;

	q->max_bands = qdisc_dev(sch)->num_tx_queues;

	q->queues = kcalloc(q->max_bands, sizeof(struct Qdisc *), GFP_KERNEL);
    // [...]
```

`q->queues` allocation size is based on `q->max_bands`, which is controllable by changing the number of TX queues of the network device. This is possible by creating a dummy interface with the desired number of queues. By allocating a large chunk, the page allocator is used, bypassing kernelCTF heap mitigations.

The exploitation idea is to attach the vulnerable qdisc set-up as one of the multiq qdisc's childs. Triggering the vulnerability on the child set-up will propagate the qlen mismatch up to the drr (1:0), allowing us to delete the class 1:1 while the drr qdisc keeps a reference to the class in `q->active`. Then, we will reclaim the large multiq `q->queues` chunk and forge a qdisc object, obtaining RIP hijack from function pointer calls.

4. Attach UAF setup to multiq band
One caveat is that the multiq qdisc will have a lot of queues because of the large allocation size required (`q->max_bands`). In multiq, packets are directed to different queues based on `netdev_pick_tx()`.

```c
u16 netdev_pick_tx(struct net_device *dev, struct sk_buff *skb,
		     struct net_device *sb_dev)
{
	struct sock *sk = skb->sk;
	int queue_index = sk_tx_queue_get(sk);

	sb_dev = sb_dev ? : dev;

	if (queue_index < 0 || skb->ooo_okay ||
	    queue_index >= dev->real_num_tx_queues) {
		int new_index = get_xps_queue(dev, sb_dev, skb);

		if (new_index < 0)
			new_index = skb_tx_hash(dev, sb_dev, skb);

		if (queue_index != new_index && sk &&
		    sk_fullsock(sk) &&
		    rcu_access_pointer(sk->sk_dst_cache))
			sk_tx_queue_set(sk, new_index);

		queue_index = new_index;
	}

	return queue_index;
}
```

For dummy network devices, the [queue selection](https://www.kerneltravel.net/blog/2020/network_ljr14/) is based on a hash of the packet data - `netdev_pick_tx()` calls `skb_tx_hash()`, which generates a tx number based on a hash of the socket buffer (`skb`). This behaviour means different packets are directed into different queues. Since our vulnerable set-up will be attached to only a single queue, we want all our packets to enter a single queue. So, we will use raw sockets to send crafted payloads to ensure that the socket data is the same between packets (this is encapsulated in the function `send_mq_dev_raw_with_payload()`). The hashed data does not include priority, so we can change the priority to direct packets to different qdiscs, but all other packet data must be the same for a deterministic sorting. This also means that our packet sizes must be the same.

To figure out which queue our packets will go into, we will simply perform a test instead of trying to reproduce the hashing logic. We will attach plug qdiscs to all queues in the multiq qdisc. Then, we will send a sample packet into the qdisc and check which plug qdisc the packet ends up in. We can use this to determine which queue our packets will enter. For ease of reference, we will denote this queue as 2:x. Subsequently, we will attach the vulnerable set-up to that queue. This testing logic is encapsulated in `determine_band()`.

We are almost ready to trigger the qdisc vulnerability. But first, we will attach a plug qdisc (10:0) to the drr class 1:2. As mentioned earlier, we will trigger UAF on class 1:1, to which the multiq qdisc is attached. If class 1:1 is the first element of the drr's active list (`q->active`), any calls to `drr_dequeue()` will access the corrupt heap values in the freed multiq qdisc will be accessed, causing the kernel to panic (such calls may be triggered by network traffic). Instead, we make class 1:2 the first element of the active list before deleting class 1:1. Then, only after reclaiming the UAF multiq qdisc do unplug the plug qdisc (10:0), allowing `drr_dequeue()` to access the multiq forged values.

This is what the qdisc tree looks like at this point:
```
             1:0 (drr)
           /        \
         1:1        1:2
          |           \
     2:0 (multiq)      10:0 (plug)
   / /  |   ... \
2:1 2:2 2:3 ... 2:x
```

Now, we can attach the vulnerable set-up to the multiq class 2:x. For this specific vulnerability, this is the fq_codel set-up. For reference, this was the original set-up for LTS/COS:
```
                1:0 (hfsc)
                /   \
                1:1  1:8
               /        \   
        2:0 (drr)       5:0 (plug)
        /    \
        2:1   2:2
        /       \ 
3:0 (fq_codel)  4:0 (plug)
```

We no longer need the hfsc qdisc (1:0) and the plug qdisc (5:0). Instead, we attach the drr qdisc directly to 2:x. We will also re-number the qdiscs.
```
             1:0 (drr)
           /        \
         1:1        1:2
          |           \
     2:0 (multiq)      10:0 (plug)
   / /  |   ... \
2:1 2:2 2:3 ... 2:x
                 |
                3:0 (drr)
                /    \
                3:1   3:2
                /       \ 
        4:0 (fq_codel)  5:0 (plug)
```

This set-up is done in `setup_vuln_tree()`.

Next, we trigger the vulnerability in the same way as LTS/COS exploit. However, recall that we also want the class 1:2 to be the first element of the drr qdisc (1:0) active list. So, simply send a packet into the plug qdisc (10:0) immediately after the step of resetting the network interface. Triggering the vulnerability is done in `trigger_vuln_tree()`.

After triggering the vulnerability, we delete class 1:1. This will also delete the multiq qdisc attached to the class, freeing the elastic object allocation `q->queues`.

Due to the qlen mismatch vulnerability, the class 1:1 remains in the drr `q->active` list. At this point, class 1:2 is also in the `q->active` list, and is the first element due to our earlier setup. Since the plug qdisc (10:0) is plugged, no packet can be dequeued from it, leaving the `q->active` list unchanged while we perform the next steps.

After the `q->queues` large chunk is freed, we will use `sendmsg` to reclaim it in `do_spray_sendmsg()`, as both allocations are `GFP_KERNEL`.
```c
static int ____sys_sendmsg(struct socket *sock, struct msghdr *msg_sys,
			   unsigned int flags, struct used_address *used_address,
			   unsigned int allowed_msghdr_flags)
...
		BUILD_BUG_ON(sizeof(struct cmsghdr) !=
			     CMSG_ALIGN(sizeof(struct cmsghdr)));
		if (ctl_len > sizeof(ctl)) {
			ctl_buf = sock_kmalloc(sock->sk, ctl_len, GFP_KERNEL);
			if (ctl_buf == NULL)
				goto out;
		}
		err = -EFAULT;
		if (copy_from_user(ctl_buf, msg_sys->msg_control_user, ctl_len))
			goto out_freectl;
```

> Note that the multiq qdisc is freed as well. It is possible for the chunk to be reclaimed by other system allocations, corrupting internal pointers. However, due to cache randomization (`CONFIG_RANDOM_KMALLOC_CACHES`) on the mitigation instance, there is actually very little noise within each cache. In other words, the chances of the freed multiq qdisc being reclaimed is quite low, so we don't have to worry about that in practice. Instead, we just focus on reclaiming `q->queues`.

Since `q->queues` is an array of qdisc (`struct Qdisc **queues`), we will populate the reclaimed chunk with pointers to forged qdiscs. These pointers will point to `kernfs_pr_cont_buf` (explained in [CVE-2025-38083](https://github.com/google/security-research/blob/8f9aa8d7844f307ad31ca3866ee78968248c8bfe/pocs/linux/kernelctf/CVE-2025-38083_cos_mitigation/docs/exploit.md#mitigation-exploit)).

But what do we forge in the qdisc? Recall that we will subsequently unplug the plug qdisc (10:0), triggering `drr_dequeue()`. This will call `ops->peek()` on the multiq qdisc.
```c
static struct sk_buff *drr_dequeue(struct Qdisc *sch)
{
	struct drr_sched *q = qdisc_priv(sch);
	struct drr_class *cl;
	struct sk_buff *skb;
	unsigned int len;

	if (list_empty(&q->active))
		goto out;
	while (1) {
		cl = list_first_entry(&q->active, struct drr_class, alist);
		skb = cl->qdisc->ops->peek(cl->qdisc);
        // [...]
```

This calls the multiq method `multiq_peek()`:
```c
static struct sk_buff *multiq_peek(struct Qdisc *sch)
{
	struct multiq_sched_data *q = qdisc_priv(sch);
	unsigned int curband = q->curband;
	struct Qdisc *qdisc;
	struct sk_buff *skb;
	int band;

	for (band = 0; band < q->bands; band++) {
		curband++;
		if (curband >= q->bands)
			curband = 0;

		if (!netif_xmit_stopped(
		    netdev_get_tx_queue(qdisc_dev(sch), curband))) {
			qdisc = q->queues[curband];                                      // [14]
			skb = qdisc->ops->peek(qdisc);                                   // [15]
			if (skb)
				return skb;
		}
	}
	return NULL;
}
```

This will access the reclaimed `q->queues` at [14] based on the `curband` value. This value is updated in `multiq_peek()` and `multiq_dequeue()` to cycle through the nodes. We can either ignore its value and spray pointers over all qdiscs or empirically determine its value. At [15], a pointer in the forged qdisc is called, giving us RIP hijack. The value in RDI is our forged qdisc pointer. It is possible to ROP chain in such cases using useful functions such as `qdisc_leaf()` ([CVE-2025-21700](https://github.com/liona24/security-research/blob/CVE-2025-21700/pocs/linux/kernelctf/CVE-2025-21700_lts_cos_mitigation/docs/exploit.md#rop-chain)) or `qdisc_reset()` ([CVE-2024-36978](https://www.hexacon.fr/slides/Cho_Lee-Utilizing_Cross-CPU_Allocation_to_Exploit_Preempt-Disabled_Linux_Kernel.pdf)). We opt for `qdisc_reset()`, but the idea is similar. Here is a brief explanation of the gadget:

```c
void qdisc_reset(struct Qdisc *qdisc)
{
	const struct Qdisc_ops *ops = qdisc->ops;

	trace_qdisc_reset(qdisc);

	if (ops->reset)
		ops->reset(qdisc);

	__skb_queue_purge(&qdisc->gso_skb);
	__skb_queue_purge(&qdisc->skb_bad_txq);

	qdisc->q.qlen = 0;
	qdisc->qstats.backlog = 0;
}
```

Since we control `qdisc`, we can get an arbitrary function call at `ops->reset()`. Let's look at the disassembly to see the register values at the call site.

```
Dump of assembler code for function qdisc_reset:
gef> disas qdisc_reset
Dump of assembler code for function qdisc_reset:
   0xffffffff81e83860 <+0>:	call   0xffffffff811170e0 <__fentry__>
   0xffffffff81e83865 <+5>:	push   rbp
   0xffffffff81e83866 <+6>:	push   rbx
   0xffffffff81e83867 <+7>:	mov    eax,DWORD PTR [rip+0x1ede83b]        # 0xffffffff83d620a8 <__tracepoint_qdisc_reset+8>
   0xffffffff81e8386d <+13>:	mov    rbx,rdi
   0xffffffff81e83870 <+16>:	mov    rbp,QWORD PTR [rdi+0x18]
   0xffffffff81e83874 <+20>:	test   eax,eax
   0xffffffff81e83876 <+22>:	jg     0xffffffff81e83961 <qdisc_reset+257>
   0xffffffff81e8387c <+28>:	mov    rax,QWORD PTR [rbp+0x48]
   0xffffffff81e83880 <+32>:	test   rax,rax
   0xffffffff81e83883 <+35>:	je     0xffffffff81e8388d <qdisc_reset+45>
   0xffffffff81e83885 <+37>:	mov    rdi,rbx
   0xffffffff81e83888 <+40>:	call   0xffffffff82604f20 <__x86_indirect_thunk_array>
```

The `ops->reset()` call happens at `qdisc_reset+40`. Prior to that, `rbp` is set to `[rdi+0x18]` at `qdisc_reset+16`. Recall that `rdi` currently points to our forged qdisc, and `[rdi+0x18]` is the value of `qdisc->ops`. So, by calling a `mov rsp, rbp; ret;` instruction, we can successfully pivot stack into the region contains the `->ops` table. Since we forged our qdisc in `kernfs_pr_cont_buf`, we will craft our ROP chain there as well.

In summary,
1. Create qdisc (`setup_drr_multiq()`): drr (1:0), multiq (2:0)
2. Determine band (`determine_band()`)
3. Create qdisc: plug (10:0)
4. Attach vulnerable set-up to 2:x (`setup_vuln_tree()`)
5. Trigger reference count mismatch (`trigger_vuln_tree()`)
6. Delete the drr class 1:1
7. Reclaim `q->queues` (`do_spray_sendmsg()`)
8. Trigger `drr_dequeue()` on UAF multiq (`trigger_uaf_miti()`)

> Once again, this method of bypassing the KernelCTF heap mitigations is generally applicable to qlen mismatch vulnerabilities. Simply replace the fq_codel set-up with a vulnerable set-up that can lead to a qlen mismatch.