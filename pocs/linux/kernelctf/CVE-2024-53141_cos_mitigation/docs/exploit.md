CVE-2024-53141
--------------

Exploit documentation for CVE-2024-53141 for `mitigation-v3b-6.1.55` and
`cos-109-17800.372.31`.
The exploit works for both versions without major changes. Additional hints for
specific versions are added at some places for clarification.

## Triggering the Vulnerability

As pointed out in the `vulnerability.md` document, we are looking at the `bitmap:ip`
set, specifically when adding IP ranges using the `IPSET_ATTR_CIDR`.
Let's have a look at the relevant code pieces below:
```c
struct bitmap_ip {
	unsigned long *members;	/* the set members */
	u32 first_ip;		/* host byte order, included in range */
	u32 last_ip;		/* host byte order, included in range */
	u32 elements;		/* number of max elements in the set */
	u32 hosts;		/* number of hosts in a subnet */
	size_t memsize;		/* members size */
	u8 netmask;		/* subnet netmask */
	struct timer_list gc;	/* garbage collection */
	struct ip_set *set;	/* attached to this ip_set */
	unsigned char extensions[]	/* data extensions */
		__aligned(__alignof__(u64));
};

/* ADT structure for generic function args */
struct bitmap_ip_adt_elem {
	u16 id;
};

static int
bitmap_ip_uadt(struct ip_set *set, struct nlattr *tb[],
	       enum ipset_adt adt, u32 *lineno, u32 flags, bool retried)
{
	struct bitmap_ip *map = set->data;
	ipset_adtfn adtfn = set->variant->adt[adt];
	u32 ip = 0, ip_to = 0;
	struct bitmap_ip_adt_elem e = { .id = 0 };
	struct ip_set_ext ext = IP_SET_INIT_UEXT(set);
	int ret = 0;

	/* .. snip .. */

	ret = ip_set_get_hostipaddr4(tb[IPSET_ATTR_IP], &ip); // [1.1]
	if (ret)
		return ret;

	/* .. snip .. */
	if (tb[IPSET_ATTR_CIDR]) {
		u8 cidr = nla_get_u8(tb[IPSET_ATTR_CIDR]);

		if (!cidr || cidr > HOST_MASK)
			return -IPSET_ERR_INVALID_CIDR;
		ip_set_mask_from_to(ip, ip_to, cidr);              // [1.2]
	} else {
		ip_to = ip;                                        // [1.3]
	}

	for (; !before(ip_to, ip); ip += map->hosts) {       // [1.4]
		e.id = ip_to_id(map, ip);                          // [1.5]
		ret = adtfn(set, &e, &ext, &ext, flags);           // [1.6]

		if (ret && !ip_set_eexist(ret, flags))
			return ret;                                      // [1.7]

		ret = 0;
	}
	return ret;
}
```

Assuming we have constructed a `bitmap:ip` with the range `255.255.255.10 - 255.255.255.255`,
`map->first_ip` would be `255.255.255.10`, `map->last_ip` would be `255.255.255.255`,
`map->members` would be a bitmap with 245 elements and `map->extensions` would be
a flat binary blob of 245 encoded extensions (which vary depending on extensions
chosen).
For a simple case of a single given IP in the valid range, the `bitmap_ip_uadt()`
function will first get the IP ([1.1]), set the range to a 1-element range ([1.3])
and eventually iterate over the range converting each IP in the range to an element
index ([1.5]) and set the bit in the bitmap at the given index (i.e.
`bitmap_set(map->members, index)`) **and copy extensions if needed** ([1.6])
(see also excerpt from `ip_set_bitmap_gen.h` providing the implementation `mtype_add` below).
We will use this copy behaviour of extensions during our out-of-bounds write later.

```c
// ip_set_bitmap_gen.h
static int
mtype_add(struct ip_set *set, void *value, const struct ip_set_ext *ext,
	  struct ip_set_ext *mext, u32 flags)
{
	struct mtype *map = set->data;
	const struct mtype_adt_elem *e = value;
	void *x = get_ext(set, map, e->id);

    /* for our case this is bitmap_ip_do_add, basically a bit test on
       map->members*/
	int ret = mtype_do_add(e, map, flags, set->dsize);

    /* snip [exit early on duplicate] */

	if (SET_WITH_TIMEOUT(set))
#ifdef IP_SET_BITMAP_STORED_TIMEOUT
		mtype_add_timeout(ext_timeout(x, set), e, ext, set, map, ret);
#else
		ip_set_timeout_set(ext_timeout(x, set), ext->timeout);
#endif

	if (SET_WITH_COUNTER(set))
		ip_set_init_counter(ext_counter(x, set), ext);
	if (SET_WITH_COMMENT(set))
		ip_set_init_comment(set, ext_comment(x, set), ext);
	if (SET_WITH_SKBINFO(set))
		ip_set_init_skbinfo(ext_skbinfo(x, set), ext);

	/* Activate element */
	set_bit(e->id, map->members);
	set->elements++;

	return 0;
}
```

The index conversion is performed via `ip_to_id()` which basically maps the first
IP in the range (i.e. `255.255.255.10` in our case) to the first index 0, and the
second IP to 1 etc. We will ignore the possibilities around non-default netmasks
for now:
```c
static u32
ip_to_id(const struct bitmap_ip *m, u32 ip)
{
	return ((ip & ip_set_hostmask(m->netmask)) - m->first_ip) / m->hosts;
}
```

In the case of a given CIDR we might hit the OOB bug because ([1.2]) will clear
the first `ip` according to the CIDR. With a `CIDR = 24` and some valid initial ip
as before we might perform steps ([1.5]) and ([1.6]) for the entire range of
`255.255.255.0/24` (i.e. `ip == 255.255.255.0` and `ip_to = 255.255.255.255`).

Now the `ip_to_id()` function will underflow performing `255.255.255.0 - 255.255.255.10`
for index calulation. At this point note that `e.id` is a `u16` contrary to the
index calculation which is performed on `u32`. This means the negative index calculated
will be truncated before setting the associated bit in the bitmap. This essentially
corresponds to an out-of-bounds write far beyond the end of the bitmap.
To get this index into a saner range we have to underflow the subtraction large
enough so that the truncated result is a somewhat small positive `u16`. This is
easy enough by choosing larger CIDR of f.e. `255.255.0.0/16`. This comes at a cost
though, because the loop at ([1.4]) will iterate the entire range, thus also perform
that many writes which are likely out-of-bounds.
In order to prevent too many uncontrollable writes we will aim to exit the loop
early through ([1.7]) by causing duplicate elements.

Now that we have a good understanding on how the bug can be utilized and controlled
we will look at potential victims to overwrite.

Considering the fact that we are also targeting the mitigation instance with
`CONFIG_RANDOM_KMALLOC_CACHES` enabled we will need to consider which objects
would fall into the same cache in the first place, thus the safest option is to
look at the set object itself.

For this we need a better understanding of the opaque `extensions` member on the
set. As hinted above this is a dynamic array which is sized based on the actual
extensions present for the owning core set object.
For our purposes the following extensions are relevant because we have precise
control over their values:
```c
// include/linux/netfilter/ipset/ip_set.h

struct ip_set_counter {
	atomic64_t bytes;
	atomic64_t packets;
};

struct ip_set_skbinfo {
	u32 skbmark;
	u32 skbmarkmask;
	u32 skbprio;
	u16 skbqueue;
	u16 __pad;
};
```

Given a set with those extensions the `struct bitmap_ip` actually looks like this
in memory:
```c
struct bitmap_ip {
	unsigned long *members;	/* the set members */
	u32 first_ip;		/* host byte order, included in range */
	u32 last_ip;		/* host byte order, included in range */
	u32 elements;		/* number of max elements in the set */
	u32 hosts;		/* number of hosts in a subnet */
	size_t memsize;		/* members size */
	u8 netmask;		/* subnet netmask */
	struct timer_list gc;	/* garbage collection */
	struct ip_set *set;	/* attached to this ip_set */
	struct {
		struct ip_set_counter counters;
		struct ip_set_skbinfo skbinfo;
	} extensions[/*elements*/]	/* data extensions */ __aligned(__alignof__(u64));
};
```

Given our out-of-bounds primitive we can thus write up to 32 bytes of controllable
memory out of bounds by setting these extensions.
Since the `bitmap_ip` types are guaranteed to hit the same cache we will target
another following `struct bitmap_ip` with those overwrites. Specifically we will
attempt to modify the `members`, `first_ip` and `last_ip` so that we can turn our
limited control out-of-bounds-write into a fully controllable arbitrary write
via the standard IP set API provided (see single case add operation above).

We will choose to go with a well known `core_pattern` overwrite as our means of
privilege escalation.

With all the building blocks ready we just need to combine them and implement
the theory presented above. For parameters we will choose `CIDR = 16` in order
to achieve the `u16` overflow. We will set both
`IPSET_FLAG_WITH_COUNTERS | IPSET_FLAG_WITH_SKBINFO` extensions for maximum
control over data written. We will choose sets with `N = 14` IPs so that the
`struct bitmap_ip` will hit `(dyn-)kmalloc-1k-*` cache. This choice of parameters
is a tradeoff between the range of out-of-bounds writes, `members*` bitmap cache
sizes and likelyhood of consecutive `struct bitmap_ip` structures.
With given N we have 32 objects per slab, thus a decent chance of hitting consecutive
objects. Also N is reasonably small tightening the control over the write.
Downside is that the `members*` bitmap will hit `(dyn-)kmalloc-8*` with a lot of
zeros in the initial bitmap. On one side this means we get a guaranteed override
at the desired place in memory, but we also are more likely to corrupt other objects
in this small-object cache.

## KASLR Bypass

The strategy above assumes that we can set the `core_pattern` using the out-of-bounds
write without any prior knowledge about the randomized kernel offset.
Practically this is not a problem, as we can just use side channels as a KASLR bypass.
The reproducer for the mitigation instance will use the solution presented earlier
for the AMD CPU (see [here for a previous example of mine](https://github.com/google/security-research/blob/96319520ae0f09d8c1f3c6ff7530944bb0efb171/pocs/linux/kernelctf/CVE-2023-6817_mitigation/exploit/mitigation-v3-6.1.55/exploit.c#L1097)).
For COS we are lazy and just use the environment provided KASLR leak via the
command line.

## Stability

On a warmed up CPU this exploit had 100% stability on remote (10/10 runs).
We expect it to be somewhere in the range 31/32 ~= 96% because that is what the
object layout imposes for the slab cache we chose (our chance that the corrupted
set is not the last one in the slab).
Interestingly it dropped to about 70% on a cold server (random submissions with
large delays in between after the server presumably went to sleep and CPU resources
were de-allocated). This is likely due to the KASLR bypass being best effort and
not fully optimized.
