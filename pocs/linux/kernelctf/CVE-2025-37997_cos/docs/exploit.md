# Root cause

# Exploit
## Race tondition to Buffer overflow read/write

In the first loop in the function `mtype_gc_do`, when any elements in the bucket are expired, it
will clear used bit of the elements.
When the number of the expired elements is more than two, the shrinkage will be triggered thus,
the second loop will be triggered too.
In the second loop, it copies used elements to smaller region.

```c
static void
mtype_gc_do(struct ip_set *set, struct htype *h, struct htable *t, u32 r)
{
	struct hbucket *n, *tmp;
	struct mtype_elem *data;
	u32 i, j, d;
	size_t dsize = set->dsize;
	u8 htable_bits = t->htable_bits;

	spin_lock_bh(&t->hregion[r].lock);
	for (i = ahash_bucket_start(r, htable_bits);
	     i < ahash_bucket_end(r, htable_bits); i++) {
		n = __ipset_dereference(hbucket(t, i));
		if (!n)
			continue;
        // first loop
		for (j = 0, d = 0; j < n->pos; j++) {
			if (!test_bit(j, n->used)) {
				d++;
				continue;
			}
			data = ahash_data(n, j, dsize);
			if (!ip_set_timeout_expired(ext_timeout(data, set)))
				continue;
			pr_debug("expired %u/%u\n", i, j);
			clear_bit(j, n->used);
			smp_mb__after_atomic();
			t->hregion[r].elements--;
			ip_set_ext_destroy(set, data);
			d++;
		}
		if (d >= AHASH_INIT_SIZE) {
			if (d >= n->size) {
				t->hregion[r].ext_size -=
					ext_size(n->size, dsize);
				rcu_assign_pointer(hbucket(t, i), NULL);
				kfree_rcu(n, rcu);
				continue;
			}
			tmp = kzalloc(sizeof(*tmp) +
				(n->size - AHASH_INIT_SIZE) * dsize,
				GFP_ATOMIC);
			if (!tmp)
				/* Still try to delete expired elements. */
				continue;
			tmp->size = n->size - AHASH_INIT_SIZE;
            // second loop
			for (j = 0, d = 0; j < n->pos; j++) {
				if (!test_bit(j, n->used))
					continue;
				data = ahash_data(n, j, dsize);
				memcpy(tmp->value + d * dsize,
				       data, dsize);
				set_bit(d, tmp->used);
				d++;
			}
			tmp->pos = d;
			t->hregion[r].ext_size -=
				ext_size(AHASH_INIT_SIZE, dsize);
			rcu_assign_pointer(hbucket(t, i), tmp);
			kfree_rcu(n, rcu);
		}
	}
	spin_unlock_bh(&t->hregion[r].lock);
}
```

The problem is, because of the inaccurate locking, this function can concurentlly run with
`mtype_add`. When `mtype_add` is called between the first loop and second loop, setting n->used bits, `dsize` of bytes
buffer overflow write can happen.

In the following content is based on `IPSET_ATTR_TYPENAME` is `hash:ip`.
The value which will overflow is the type of following.

We can control ip `skbinfo` and `timeout`.

```c
struct value {
    __be32 ip;
    /* 4 byte of hole */
    struct ip_set_skbinfo {
        u32 skbmark;
        u32 skbmarkmask;
        u32 skbprio;
        u16 skbqueue;
        u16 __pad;
    } skbinfo;
    unsigned long timeout;
}
```

This is the hbucket header. The value is array of `struct value`. When we can overflow into
hbucket header, we can control the size and pos of the hbucket.

```c
struct hbucket {
	struct rcu_head rcu;	/* for call_rcu */
	/* Which positions are used in the array */
	DECLARE_BITMAP(used, AHASH_MAX_TUNED);
	u8 size;		/* size of the array */
	u8 pos;			/* position of the first free entry */
	unsigned char value[]	/* the array of the values */
		__aligned(__alignof__(u64));
};
```

converted into
```c
struct hbucket {
    struct callback_head rcu;
    unsigned long used[1];
    u8 size;
    u8 pos;
    unsigned char value[];
}
```

## Buffer Overflow to physical AAR/AAW

- create a VICTIM set with infinite timeout
- create a OOB set with 1 second of timeout
- add a hbucket whose size is 2 (kmalloc-96) in VICTIM set
- add a hbucket whose size is 4 (kmalloc-192) in OOB set
- race GC on OOB set to trigger OOB write and overwrite victim header
