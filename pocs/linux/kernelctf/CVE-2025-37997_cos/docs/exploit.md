# Vulnerability

This vulnerability is a race condition caused by differences in how locks are acquired between the `mtype_gc_do` and the `mtype_add` in `net/netfilter/ipset/ip_set_hash_gen.h`.

In `mtype_gc_do`, locks are acquired as follows. This code expects that, by acquiring the lock for `t->hregion[r]`, elements from the `ahash_bucket_start(r, htable_bits)`-th to the `ahash_bucket_end(r, htable_bits)`-th of the `hbucket` are protected.

```c
#define HTABLE_REGION_BITS      10
#define ahash_bucket_start(h,  htable_bits)     \
        ((htable_bits) < HTABLE_REGION_BITS ? 0 \
                : (h) * jhash_size(HTABLE_REGION_BITS))
#define ahash_bucket_end(h,  htable_bits)       \
        ((htable_bits) < HTABLE_REGION_BITS ? jhash_size(htable_bits)   \
                : ((h) + 1) * jhash_size(HTABLE_REGION_BITS))
static void
mtype_gc_do(struct ip_set *set, struct htype *h, struct htable *t, u32 r)
{
...
	spin_lock_bh(&t->hregion[r].lock);
	for (i = ahash_bucket_start(r, htable_bits);
	     i < ahash_bucket_end(r, htable_bits); i++) {
		n = __ipset_dereference(hbucket(t, i));
```

On the other hand, in `mtype_add`, locks are acquired as follows. This code acquires the lock for `t->hregion[key % ahash_numof_locks(t->htable_bits)]` and manipulates the `key`th element of the `hbucket`.

```c
#define HTABLE_REGION_BITS      10
#define ahash_numof_locks(htable_bits)          \
        ((htable_bits) < HTABLE_REGION_BITS ? 1 \
                : jhash_size((htable_bits) - HTABLE_REGION_BITS))
#define ahash_region(n, htable_bits)            \
        ((n) % ahash_numof_locks(htable_bits))
static int
mtype_add(struct ip_set *set, void *value, const struct ip_set_ext *ext,
          struct ip_set_ext *mext, u32 flags)
{
...
        key = HKEY(value, h->initval, t->htable_bits);
        r = ahash_region(key, t->htable_bits);
...
        spin_lock_bh(&t->hregion[r].lock);
        n = rcu_dereference_bh(hbucket(t, key));
...
```

In short, the above two functions has different rules to take locks. For example, when `ahash_numof_locks(t->htable_bits) == 2`, `mtype_gc` recognizes `0~0x3ff` as region0 and `0x400~0x800` as region1, but `mtype_add` recognizes `0,2,4,6,8...` as region0 and `1,3,5,7,9...` as region1.
Therefore, in the following case, a race condition occurs at the element `hbucket[1]`:

- `mtype_gc_do` acuires the lock for `t->hregion[0]` and runs gc on elements from `hbucket[0]` to `hbucket[0x3ff]`
- `mtype_add` acuires the lock for `t->hregion[1]` and adds an element to `hbucket[1]`

# Exploit

## Overview
Exploit proceeds as follows. (You can use any cache.)

1. create a VICTIM set without timeout
2. create a OOB set with 1-second timeout (to trigger GC)
3. add a hbucket whose size is 2 (kmalloc-96) in VICTIM set
4. add a hbucket whose size is 4 (kmalloc-192) in OOB set
5. GC reallocates OOB set into kmalloc-96. At same time, add elements to OOB set to trigger OOB write and overwrite VICTIM header
6. Use the corrupted VICTIM hbucket to overwrite a PTE on an adjacent page

## Race tondition to Buffer overflow read/write

mtype_gc_do processes a bucket in two loops:

First loop: clear used bits for expired entries, counting them in `d`.
If `d >= AHASH_INIT_SIZE (== 2)`, the code decides to shrink and allocates `tmp` with `tmp->size = n->size - AHASH_INIT_SIZE`.

Race window: between the first and second loops, another CPU may run `mtype_add` on the same bucket and set new bits in `n->used`.

Second loop: copy each still-used entry from `n` to `tmp->value[d++]`.
Because `tmp->size` is already fixed to `n->size - 2`, the updated `d` can now exceed `tmp->size`, causing an out-of-bounds write past `tmp->value[]`.

A trimmed view of the shrink path:

```c
static void
mtype_gc_do(struct ip_set *set, struct htype *h, struct htable *t, u32 r)
{
	struct hbucket *n, *tmp;
	struct mtype_elem *data;
	u32 i, j, d;
	size_t dsize = set->dsize;
	u8 htable_bits = t->htable_bits;

	spin_lock_bh(&t->hregion[r].lock);
	for (i = ahash_bucket_start(r, htable_bits);
	     i < ahash_bucket_end(r, htable_bits); i++) {
		n = __ipset_dereference(hbucket(t, i));
		if (!n)
			continue;
        // first loop
		for (j = 0, d = 0; j < n->pos; j++) {
			if (!test_bit(j, n->used)) {
				d++;
				continue;
			}
			data = ahash_data(n, j, dsize);
			if (!ip_set_timeout_expired(ext_timeout(data, set)))
				continue;
			pr_debug("expired %u/%u\n", i, j);
			clear_bit(j, n->used);
			smp_mb__after_atomic();
			t->hregion[r].elements--;
			ip_set_ext_destroy(set, data);
			d++;
		}
		if (d >= AHASH_INIT_SIZE) {
			if (d >= n->size) {
				t->hregion[r].ext_size -=
					ext_size(n->size, dsize);
				rcu_assign_pointer(hbucket(t, i), NULL);
				kfree_rcu(n, rcu);
				continue;
			}
			tmp = kzalloc(sizeof(*tmp) +
				(n->size - AHASH_INIT_SIZE) * dsize,
				GFP_ATOMIC);
			if (!tmp)
				/* Still try to delete expired elements. */
				continue;
			tmp->size = n->size - AHASH_INIT_SIZE;
            // second loop
			for (j = 0, d = 0; j < n->pos; j++) {
				if (!test_bit(j, n->used))
					continue;
				data = ahash_data(n, j, dsize);
				memcpy(tmp->value + d * dsize,
				       data, dsize);
				set_bit(d, tmp->used);
				d++;
			}
			tmp->pos = d;
			t->hregion[r].ext_size -=
				ext_size(AHASH_INIT_SIZE, dsize);
			rcu_assign_pointer(hbucket(t, i), tmp);
			kfree_rcu(n, rcu);
		}
	}
	spin_unlock_bh(&t->hregion[r].lock);
}
```

## Data layout hit by the overflow

In what follows, assume `IPSET_ATTR_TYPENAME` == `hash:ip` and the extensions
`IPSET_ATTR_TIMEOUT`, `IPSET_ATTR_SKBMARK`, `IPSET_ATTR_SKBPRIO`, and `IPSET_ATTR_SKBQUEUE` are present.
Under this assumption, each element copied by the GC has the following layout:

```c
struct value {
    __be32 ip;
    /* 4 byte of hole */
    unsigned long timeout;
    struct ip_set_skbinfo {
        u32 skbmark;
        u32 skbmarkmask;
        u32 skbprio;
        u16 skbqueue;
        u16 __pad;
    } skbinfo;
}
```

Each `hbucket` stores an array of struct value in `value[]`. The header is:

```c
struct hbucket {
	struct rcu_head rcu;	/* for call_rcu */
	/* Which positions are used in the array */
	DECLARE_BITMAP(used, AHASH_MAX_TUNED);
	u8 size;		/* size of the array */
	u8 pos;			/* position of the first free entry */
	unsigned char value[]	/* the array of the values */
		__aligned(__alignof__(u64));
};
```

converted into:

```c
struct hbucket {
    struct callback_head rcu;
    unsigned long used[1];
    u8 size;
    u8 pos;
    unsigned char value[];
}
```

In other words, when the OOB write crosses the end of `tmp`, it lands in the header of the next `hbucket`, allowing the following field corruption:

| hbucket | value               |
| ------- | ------------------- |
| rcu     | ip, timeout         |
| used    | skbmark,skbmarkmask |
| size    | skbprio             |
| pos     | skbprio             |

## Buffer Overflow to AAR/AAW

After overwriting `hbucket->pos` and `hbucket->size` in the VICTIM bucket, subsequent inserts will write beyond the legitimate array bounds and into the next page.

```c
	if (n->pos >= n->size) { // realloc condition
...
    }
	j = n->pos++;
	data = ahash_data(n, j, set->dsize);
...
	memcpy(data, d, sizeof(struct mtype_elem));
...
    if (SET_WITH_COUNTER(set))
		ip_set_init_counter(ext_counter(data, set), ext);
	if (SET_WITH_COMMENT(set))
		ip_set_init_comment(set, ext_comment(data, set), ext);
	if (SET_WITH_SKBINFO(set))
		ip_set_init_skbinfo(ext_skbinfo(data, set), ext);
	/* Must come last for the case when timed out entry is reused */
	if (SET_WITH_TIMEOUT(set))
		ip_set_timeout_set(ext_timeout(data, set), ext->timeout);

```

If the page adjacent to the OOB write happens to be a PTE page (by heap spraying), we can perform a dirty pagetable attack and overwrite `core_pattern` to escalate privileges.
