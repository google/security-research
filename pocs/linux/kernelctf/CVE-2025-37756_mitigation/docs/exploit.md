# Vulnerability

When TLS receive data from TCP side, it will call `tls_strp_read_sock`:

```go
#0  tls_strp_read_sock (strp=0xffff888101f7fb58) at net/tls/tls_strp.c:505
#1  tls_strp_check_rcv (strp=0xffff888101f7fb58) at net/tls/tls_strp.c:543
#2  0xffffffff8257b565 in tls_data_ready (sk=0xffff888020204280) at net/tls/tls_sw.c:2448
#3  0xffffffff824f19d7 in tcp_data_queue (sk=sk@entry=0xffff888020204280, skb=skb@entry=0xffff8880203684e0) at net/ipv4/tcp_input.c:5333
#4  0xffffffff824f2506 in tcp_rcv_established (sk=sk@entry=0xffff888020204280, skb=skb@entry=0xffff8880203684e0) at net/ipv4/tcp_input.c:6287
#5  0xffffffff825014fe in tcp_v4_do_rcv (sk=sk@entry=0xffff888020204280, skb=skb@entry=0xffff8880203684e0) at net/ipv4/tcp_ipv4.c:1916
#6  0xffffffff8250479d in tcp_v4_rcv (skb=0xffff8880203684e0) at net/ipv4/tcp_ipv4.c:2351
#7  0xffffffff824ca1fb in ip_protocol_deliver_rcu (net=net@entry=0xffffffff85293440 <init_net>, skb=0xffff8880203684e0, protocol=<optimized out>) at net/ipv4/ip_input.c:205
#8  0xffffffff824ca3f9 in ip_local_deliver_finish (net=0xffffffff85293440 <init_net>, sk=<optimized out>, skb=<optimized out>) at net/ipv4/ip_input.c:233
#9  0xffffffff823297a9 in __netif_receive_skb_one_core (skb=<optimized out>, pfmemalloc=pfmemalloc@entry=0x0) at net/core/dev.c:5741
#10 0xffffffff82329a49 in __netif_receive_skb (skb=<optimized out>) at net/core/dev.c:5854
#11 process_backlog (napi=0xffff88811c436e08, quota=0x40) at net/core/dev.c:6190
#12 0xffffffff8232a6f8 in __napi_poll (n=0xffff888101f7fb58, n@entry=0xffff88811c436e08, repoll=repoll@entry=0xffffc90000003ebf) at net/core/dev.c:6841
#13 0xffffffff8232acd7 in napi_poll (repoll=0xffffc90000003ed8, n=0xffff88811c436e08) at net/core/dev.c:6910
#14 net_rx_action () at net/core/dev.c:7032
#15 0xffffffff811accd9 in handle_softirqs (ksirqd=<optimized out>) at kernel/softirq.c:579
#16 0xffffffff811ac80b in do_softirq () at kernel/softirq.c:480
```
`tls_strp_read_sock` will call `tls_strp_load_anchor_with_queue` and TLS will set up `strp->anchor` by get the first TCP socket buffer on the TCP receive queue and set it to `skb_shinfo(strp->anchor)->frag_list`:
```C
          tls_strp_load_anchor_with_queue()
            first = tcp_recv_skb(strp->sk, tp->copied_seq, &offset);
            ...
            skb_shinfo(strp->anchor)->frag_list = first;
```
The TCP socket buffer still remains on the tcp receive queue (`sk_receive_queue`), `strp->anchor` just take the skb pointer for the decryption later at `tls_sw_recvmsg`. Problem is, with TCP socket installed with TLS we can call `tcp_disconnect` using connect syscall with AF_UNSPEC like this:
```C
	addr.sin_family = AF_UNSPEC;
	addr.sin_addr.s_addr = htonl(INADDR_ANY);
	addr.sin_port = 0;
	connect(conn, &addr, sizeof(addr));
```

It will call to `tcp_disconnect`:
```C
/*
 *	Connect to a remote host. There is regrettably still a little
 *	TCP 'magic' in here.
 */
int __inet_stream_connect(struct socket *sock, struct sockaddr *uaddr,
			  int addr_len, int flags, int is_sendmsg)
{
	....
	if (uaddr) {
		if (addr_len < sizeof(uaddr->sa_family))
			return -EINVAL;

		if (uaddr->sa_family == AF_UNSPEC) {
			err = sk->sk_prot->disconnect(sk, flags);
			sock->state = err ? SS_DISCONNECTING : SS_UNCONNECTED;
			goto out;
		}
	}

}
int tcp_disconnect(struct sock *sk, int flags)
{
        ...
	__skb_queue_purge(&sk->sk_receive_queue);
        ...
}
```

`__skb_queue_purge` will free the skb while it still hold by `strp->anchor->frag_list`.

# Exploitation
If we call `tls_sw_recvmsg` and finally it will reach `tls_decrypt_sg` there are a few places that it uses our freed skb. For example, `skb_to_sgvec` and `skb_nsg`:
```C
static int tls_decrypt_sg(struct sock *sk, struct iov_iter *out_iov,
			  struct scatterlist *out_sg,
			  struct tls_decrypt_arg *darg)
{

	struct sk_buff *skb = tls_strp_msg(ctx); // strp->anchor
	....
	n_sgin = skb_nsg(skb, rxm->offset + prot->prepend_size,
			 rxm->full_len - prot->prepend_size);
	....
	err = skb_to_sgvec(skb, &sgin[1],
			   rxm->offset + prot->prepend_size,
			   rxm->full_len - prot->prepend_size);
```
But there's no any meaningful operation to use it as exploitation because those function only perform UAF read when it read `skb_shinfo(skb)->frag_list` (our freed skb placed).

Another path that use our freed skb in `skb_shinf(skb)->frag_list` is via this path, we can reach this line when we perform async decryption:

```C
static int tls_decrypt_sg(struct sock *sk, struct iov_iter *out_iov,
			  struct scatterlist *out_sg,
			  struct tls_decrypt_arg *darg)
{
        ....
	if (unlikely(darg->async)) {
		err = tls_strp_msg_hold(&ctx->strp, &ctx->async_hold);
		if (err)
			__skb_queue_tail(&ctx->async_hold, darg->skb);
		return err;
	}
}
```
It will reach `skb_clone` path at `tls_strp_msg_hold`:
```C
int tls_strp_msg_hold(struct tls_strparser *strp, struct sk_buff_head *dst)
{
        struct skb_shared_info *shinfo = skb_shinfo(strp->anchor);
        ....
		struct sk_buff *iter, *clone;
		int chunk, len, offset;

		offset = strp->stm.offset;
		len = strp->stm.full_len;
		iter = shinfo->frag_list; // our freed skb

		while (len > 0) {
			if (iter->len <= offset) {
				offset -= iter->len;
				goto next;
			}

			chunk = iter->len - offset;
			offset = 0;

			clone = skb_clone(iter, strp->sk->sk_allocation);
			if (!clone)
				return -ENOMEM;
			__skb_queue_tail(dst, clone);

			len -= chunk;
next:
			iter = iter->next;
		}
	}
}
```

If we succesfully clone our freed skb, it will queued to the `ctx->async_hold`. After decryption finish, cloned skb that resides at `ctx->async_hold` will free at `__skb_queue_purge`.

```C
int tls_sw_recvmsg(struct sock *sk,
		   struct msghdr *msg,
		   size_t len,
		   int flags,
		   int *addr_len)
{
	...
recv_end:
	if (async) {
		int ret;

		/* Wait for all previously submitted records to be decrypted */
		ret = tls_decrypt_async_wait(ctx);
		__skb_queue_purge(&ctx->async_hold);
}

static inline void __skb_queue_purge_reason(struct sk_buff_head *list,
					    enum skb_drop_reason reason)
{
	struct sk_buff *skb;

	while ((skb = __skb_dequeue(list)) != NULL)
		kfree_skb_reason(skb, reason);
}

static inline void __skb_queue_purge(struct sk_buff_head *list)
{
	__skb_queue_purge_reason(list, SKB_DROP_REASON_QUEUE_PURGE);
}
```

## Mitigation exploit (6.1.x)
In skb, there's a data structure called `skb_shared_info`. Usually it contain non linear skb data, for example such skb that hold spliced page pointer, linked skb, etc.:
```C
/* This data is invariant across clones and lives at
 * the end of the header data, ie. at skb->end.
 */
struct skb_shared_info {
	__u8		flags;
	__u8		meta_len;
	__u8		nr_frags;
	__u8		tx_flags;
	unsigned short	gso_size;
	/* Warning: this field is not always filled in (UFO)! */
	unsigned short	gso_segs;
	struct sk_buff	*frag_list;
	union {
		struct skb_shared_hwtstamps hwtstamps;
		struct xsk_tx_metadata_compl xsk_meta;
	};
	unsigned int	gso_type;
	u32		tskey;

	/*
	 * Warning : all fields before dataref are cleared in __alloc_skb()
	 */
	atomic_t	dataref;
	unsigned int	xdp_frags_size;

	/* Intermediate layers must ensure that destructor_arg
	 * remains valid until skb destructor */
	void *		destructor_arg;

	/* must be last field, see pskb_expand_head() */
	skb_frag_t	frags[MAX_SKB_FRAGS];
};
```
It placed at the end of skb data specifically at `skb->head + skb->end`. If we call `skb_clone`, `__skb_clone` will copy head pointer and increase its `dataref`.
```C
/*
 * You should not add any new code to this function.  Add it to
 * __copy_skb_header above instead.
 */
static struct sk_buff *__skb_clone(struct sk_buff *n, struct sk_buff *skb)
{
#define C(x) n->x = skb->x

	n->next = n->prev = NULL;
	n->sk = NULL;
	__copy_skb_header(n, skb);
	....
	atomic_inc(&(skb_shinfo(skb)->dataref));
	skb->cloned = 1;

	return n;
#undef C
}
```

At `kfree_skb` -> `skb_release_data` it will decrement the `dataref`, if it zero it continue to free the rest.
```C
static void skb_release_data(struct sk_buff *skb)
{
	struct skb_shared_info *shinfo = skb_shinfo(skb);
	int i;

	if (skb->cloned &&
	    atomic_sub_return(skb->nohdr ? (1 << SKB_DATAREF_SHIFT) + 1 : 1,
			      &shinfo->dataref))
		goto exit;
	....
}
```

If we have freed skb, this skb still contain old data so `skb_shinfo(victim_skb)` also contain old data. Old data means, we have `skb_shinfo(victim_skb)->dataref` is 0. If this freed skb is cloned, the dataref will increase to 1 again. Then, this cloned skb is linked to `ctx->async_hold` and will call `__skb_queue_purge` and  reach `skb_release_data` again. Now kernel tries to free our the cloned_skb again based on old `skb_shared_info`.

If we have old spliced page pointer still live in `shinfo->frags`, we can do put page pointer again.
```C
static void skb_release_data(struct sk_buff *skb)
{
	struct skb_shared_info *shinfo = skb_shinfo(skb);
	int i;

	....
	for (i = 0; i < shinfo->nr_frags; i++)
		__skb_frag_unref(&shinfo->frags[i], skb->pp_recycle);
}
```
`__skb_frag_unref` will put page at `shinfo->frags[i]`. So from our freed skb, it already release this `shinfo` and also already put the page from frags, the cloned skb will do release for second time and the page from frags also will put for second time. So we have double put of page.

So these the steps for exploitation in COS and MITIGATION:
1. create two pair of TCP socket, server and client
2. setup another thread that will hang on tcp_recvmsg on client socket with MSG_WAITALL
3. setup TLS on client side
4. splice pipe data page via server socket, it will alloc its victim_skb and victim_skb->head and send to client
5. kernel will got data_ready notification and prepare `strp->anchor`, also set `strp->anchor->frag_list` victim_skb
6. close server socket, it will make tcp_recvmsg return and release the victim_skb, at this time spliced page also released at `skb_release_data`
7. call recvmsg on client socket, it will clone the victim_skb based on `strp->anchor->frag_list` that contain released skb_shared_info from victim_skb.
8. the cloned skb will release will release skb_shared_info from victim_skb again, then our spliced page will put again at `skb_release_data`

After this step we get a page freed that still hold by pipe. At this time we reclaim the page with pagetable and can write the pagetable by writing to the pipe.
We choose to target `core_pattern` to perform privilege escalation, we can deduce `core_pattern` physical address after we spray pagetable and read every memory that we allocate, it will install `empty_zero_page` PTE to the pagetable, then we can calculate `core_pattern` from that PTE.



